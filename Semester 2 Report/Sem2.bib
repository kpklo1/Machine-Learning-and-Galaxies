@INPROCEEDINGS{kernaltrick,
  author={H. {TALABANI} and E. {AVCI}},
  booktitle={2018 International Conference on Artificial Intelligence and Data Processing (IDAP)},
  title={Impact of Various Kernels on Support Vector Machine Classification Performance for Treating Wart Disease},
  year={2018},
  volume={},
  number={},
  pages={1-6},}


@article{UniversalApproximationTheorem,
title = "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function",
journal = "Neural Networks",
volume = "6",
number = "6",
pages = "861 - 867",
year = "1993",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(05)80131-5",
url = "http://www.sciencedirect.com/science/article/pii/S0893608005801315",
author = "Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken",
keywords = "Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (ÎŒ) approximation",
abstract = "Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold."
}

@InProceedings{ReLu,
  title = 	 {Deep Sparse Rectifier Neural Networks},
  author = 	 {Xavier Glorot and Antoine Bordes and Yoshua Bengio},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {315--323},
  year = 	 {2011},
  editor = 	 {Geoffrey Gordon and David Dunson and Miroslav Dudík},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  url = 	 {http://proceedings.mlr.press/v15/glorot11a.html},
  abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]}
}

@book{NeuralNetworksandDeepLearning,
 author={Nielsen, Michael}
 title={Neural Networks and Deep Learning},
 url={http://neuralnetworksanddeeplearning.com},
}

@ARTICLE{Backpropagation,
       author = {{Rumelhart}, David E. and {Hinton}, Geoffrey E. and
         {Williams}, Ronald J.},
        title = "{Learning representations by back-propagating errors}",
      journal = {\nat},
         year = 1986,
        month = oct,
       volume = {323},
       number = {6088},
        pages = {533-536},
          doi = {10.1038/323533a0},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1986Natur.323..533R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Abraham2000,
       author = {{Abraham}, Roberto G. and {Merrifield}, Michael R.},
        title = "{Explorations in Hubble Space: A Quantitative Tuning Fork}",
      journal = {\aj},
     keywords = {Galaxies: Evolution, Galaxies: Fundamental Parameters, Astrophysics},
         year = 2000,
        month = dec,
       volume = {120},
       number = {6},
        pages = {2835-2842},
          doi = {10.1086/316877},
archivePrefix = {arXiv},
       eprint = {astro-ph/0008415},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2000AJ....120.2835A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{CNNSparse,
author = {Ide, Hidenori and Kurita, Takio},
year = {2017},
month = {05},
pages = {2684-2691},
title = {Improvement of learning for CNN with ReLU activation by sparse regularization},
doi = {10.1109/IJCNN.2017.7966185}
}

@article{SGD,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2236690},
 abstract = {Let M(x) be a regression function which has a maximum at the unknown point θ. M(x) is itself unknown to the statistician who, however, can take observations at any level x. This paper gives a scheme whereby, starting from an arbitrary point x1, one obtains successively x2, x3, ⋯ such that xn converges to θ in probability as n → ∞.},
 author = {J. Kiefer and J. Wolfowitz},
 journal = {The Annals of Mathematical Statistics},
 number = {3},
 pages = {462--466},
 publisher = {Institute of Mathematical Statistics},
 title = {Stochastic Estimation of the Maximum of a Regression Function},
 volume = {23},
 year = {1952}
}

@article{Momentum,
title = "On the momentum term in gradient descent learning algorithms",
journal = "Neural Networks",
volume = "12",
number = "1",
pages = "145 - 151",
year = "1999",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(98)00116-6",
url = "http://www.sciencedirect.com/science/article/pii/S0893608098001166",
author = "Ning Qian",
keywords = "Momentum, Gradient descent learning algorithm, Damped harmonic oscillator, Critical damping, Learning rate, Speed of convergence",
abstract = "A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed."
}

@article{AdaDelta,
author = {Zeiler, Matthew},
year = {2012},
month = {12},
pages = {},
title = {ADADELTA: An adaptive learning rate method},
volume = {1212}
}

@article{AdaGrad,
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
year = {2011},
month = {07},
pages = {2121-2159},
title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
volume = {12},
journal = {Journal of Machine Learning Research}
}

@article{Adam,
author = {Kingma, Diederik and Ba, Jimmy},
year = {2014},
month = {12},
pages = {},
title = {Adam: A Method for Stochastic Optimization},
journal = {International Conference on Learning Representations}
}

@inproceedings{ObjectAMSGrad,
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian},
year = {2017},
month = {07},
pages = {},
title = {Densely Connected Convolutional Networks},
doi = {10.1109/CVPR.2017.243}
}

@article{translateAMSGrad,
author = {Johnson, Melvin and Schuster, Mike and Le, Quoc and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Viégas, Fernanda and Wattenberg, Martin and Corrado, G.s and Hughes, Macduff and Dean, Jeffrey},
year = {2016},
month = {11},
pages = {},
title = {Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
volume = {5},
journal = {Transactions of the Association for Computational Linguistics},
doi = {10.1162/tacl_a_00065}
}

@ARTICLE{AMSGrad,
       author = {{Reddi}, Sashank J. and {Kale}, Satyen and {Kumar}, Sanjiv},
        title = "{On the Convergence of Adam and Beyond}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
         year = 2019,
        month = apr,
          eid = {arXiv:1904.09237},
        pages = {arXiv:1904.09237},
archivePrefix = {arXiv},
       eprint = {1904.09237},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190409237R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{HarvardClassification,
       author = {{Cannon}, Annie Jump and {Pickering}, Edward Charles},
        title = "{Classification of 1,477 stars by means of their photographic spectra}",
      journal = {Annals of Harvard College Observatory},
     keywords = {STARS: SPECTRA, STARS: CLASSIFICATION},
         year = 1912,
        month = jan,
       volume = {56},
       number = {4},
        pages = {65-114},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1912AnHar..56...65C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Nebulae,
       author = {{Herschel}, William},
        title = "{Catalogue of 500 New Nebulae, Nebulous Stars, Planetary Nebulae, and Clusters of Stars; With Remarks on the Construction of the Heavens}",
      journal = {Philosophical Transactions of the Royal Society of London Series I},
         year = 1802,
        month = jan,
       volume = {92},
        pages = {477-528},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1802RSPT...92..477H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Hubble,
       author = {{Hubble}, E.~P.},
        title = "{The classification of spiral nebulae}",
      journal = {The Observatory},
         year = 1927,
        month = sep,
       volume = {50},
        pages = {276-281},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1927Obs....50..276H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{SVM,
  title={A class of algorithms for pattern recognition learning},
  author={Vapnik, V and Chervonenkis, A Ya},
  journal={Avtomat. i Telemekh},
  volume={25},
  number={6},
  pages={937--945},
  year={1964}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{adorf_1988_supervised,
  author = {Adorf, H. -M. and Meurs, E. J. A.},
  pages = {315-322},
  title = {Supervised and unsupervised classification — The case of IRAS point sources},
  url = {http://adsabs.harvard.edu/abs/1988LNP...310..315A},
  urldate = {2019-12-05},
  year = {1988},
  journal = {Large-Scale Structures in the Universe Observational and Analytical Methods}
}

@article{MCC,
title = "Comparison of the predicted and observed secondary structure of T4 phage lysozyme",
journal = "Biochimica et Biophysica Acta (BBA) - Protein Structure",
volume = "405",
number = "2",
pages = "442 - 451",
year = "1975",
issn = "0005-2795",
doi = "https://doi.org/10.1016/0005-2795(75)90109-9",
url = "http://www.sciencedirect.com/science/article/pii/0005279575901099",
author = "B.W. Matthews",
abstract = "Predictions of the secondary structure of T4 phage lysozyme, made by a number of investigators on the basis of the amino acid sequence, are compared with the structure of the protein determined experimentally by X-ray crystallography. Within the amino terminal half of the molecule the locations of helices predicted by a number of methods agree moderately well with the observed structure, however within the carboxyl half of the molecule the overall agreement is poor. For eleven different helix predictions, the coefficients giving the correlation between prediction and observation range from 0.14 to 0.42. The accuracy of the predictions for both Î²-sheet regions and for turns are generally lower than for the helices, and in a number of instances the agreement between prediction and observation is no better than would be expected for a random selection of residues. The structural predictions for T4 phage lysozyme are much less successful than was the case for adenylate kinase (Schulz et al. (1974) Nature 250, 140â142). No one method of prediction is clearly superior to all others, and although empirical predictions based on larger numbers of known protein structure tend to be more accurate than those based on a limited sample, the improvement in accuracy is not dramatic, suggesting that the accuracy of current empirical predictive methods will not be substantially increased simply by the inclusion of more data from additional protein structure determinations."
}

@article{FNN,
author = {Sazli, Murat},
year = {2006},
month = {01},
pages = {11-17},
title = {A brief review of feed-forward neural networks},
volume = {50},
journal = {Communications, Faculty Of Science, University of Ankara},
doi = {10.1501/0003168}
}

@article{GradientDescent,
  title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es},
  author={Cauchy, Augustin},
  journal={Comp. Rend. Sci. Paris},
  volume={25},
  number={1847},
  pages={536--538},
  year={1847}
}

@inproceedings{AutoKeras,
  title={Auto-keras: An efficient neural architecture search system},
  author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1946--1956},
  year={2019}
}

@inproceedings{AutoML,
  title={AM-LFS: AutoML for Loss Function Search},
  author={Li, Chuming and Yuan, Xin and Lin, Chen and Guo, Minghao and Wu, Wei and Yan, Junjie and Ouyang, Wanli},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={8410--8419},
  year={2019}
}
