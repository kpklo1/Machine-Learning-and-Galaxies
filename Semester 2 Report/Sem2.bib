@INPROCEEDINGS{kernaltrick,
  author={H. {TALABANI} and E. {AVCI}},
  booktitle={2018 International Conference on Artificial Intelligence and Data Processing (IDAP)},
  title={Impact of Various Kernels on Support Vector Machine Classification Performance for Treating Wart Disease},
  year={2018},
  volume={},
  number={},
  pages={1-6},}


@article{UniversalApproximationTheorem,
title = "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function",
journal = "Neural Networks",
volume = "6",
number = "6",
pages = "861 - 867",
year = "1993",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(05)80131-5",
url = "http://www.sciencedirect.com/science/article/pii/S0893608005801315",
author = "Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken",
keywords = "Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (ÎŒ) approximation",
abstract = "Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold."
}

@InProceedings{ReLu,
  title = 	 {Deep Sparse Rectifier Neural Networks},
  author = 	 {Xavier Glorot and Antoine Bordes and Yoshua Bengio},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {315--323},
  year = 	 {2011},
  editor = 	 {Geoffrey Gordon and David Dunson and Miroslav Dudík},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  url = 	 {http://proceedings.mlr.press/v15/glorot11a.html},
  abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]}
}

@book{NeuralNetworksandDeepLearning,
 author={Nielsen, Michael}
 title={Neural Networks and Deep Learning},
 url={http://neuralnetworksanddeeplearning.com},
}

@ARTICLE{Backpropagation,
       author = {{Rumelhart}, David E. and {Hinton}, Geoffrey E. and
         {Williams}, Ronald J.},
        title = "{Learning representations by back-propagating errors}",
      journal = {\nat},
         year = 1986,
        month = oct,
       volume = {323},
       number = {6088},
        pages = {533-536},
          doi = {10.1038/323533a0},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1986Natur.323..533R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Abraham2000,
       author = {{Abraham}, Roberto G. and {Merrifield}, Michael R.},
        title = "{Explorations in Hubble Space: A Quantitative Tuning Fork}",
      journal = {\aj},
     keywords = {Galaxies: Evolution, Galaxies: Fundamental Parameters, Astrophysics},
         year = 2000,
        month = dec,
       volume = {120},
       number = {6},
        pages = {2835-2842},
          doi = {10.1086/316877},
archivePrefix = {arXiv},
       eprint = {astro-ph/0008415},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2000AJ....120.2835A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{CNNSparse,
author = {Ide, Hidenori and Kurita, Takio},
year = {2017},
month = {05},
pages = {2684-2691},
title = {Improvement of learning for CNN with ReLU activation by sparse regularization},
doi = {10.1109/IJCNN.2017.7966185}
}

@article{SGD,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2236690},
 abstract = {Let M(x) be a regression function which has a maximum at the unknown point θ. M(x) is itself unknown to the statistician who, however, can take observations at any level x. This paper gives a scheme whereby, starting from an arbitrary point x1, one obtains successively x2, x3, ⋯ such that xn converges to θ in probability as n → ∞.},
 author = {J. Kiefer and J. Wolfowitz},
 journal = {The Annals of Mathematical Statistics},
 number = {3},
 pages = {462--466},
 publisher = {Institute of Mathematical Statistics},
 title = {Stochastic Estimation of the Maximum of a Regression Function},
 volume = {23},
 year = {1952}
}

@article{Momentum,
title = "On the momentum term in gradient descent learning algorithms",
journal = "Neural Networks",
volume = "12",
number = "1",
pages = "145 - 151",
year = "1999",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(98)00116-6",
url = "http://www.sciencedirect.com/science/article/pii/S0893608098001166",
author = "Ning Qian",
keywords = "Momentum, Gradient descent learning algorithm, Damped harmonic oscillator, Critical damping, Learning rate, Speed of convergence",
abstract = "A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed."
}

@article{AdaDelta,
author = {Zeiler, Matthew},
year = {2012},
month = {12},
pages = {},
title = {ADADELTA: An adaptive learning rate method},
volume = {1212}
}

@article{AdaGrad,
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
year = {2011},
month = {07},
pages = {2121-2159},
title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
volume = {12},
journal = {Journal of Machine Learning Research}
}

@article{Adam,
author = {Kingma, Diederik and Ba, Jimmy},
year = {2014},
month = {12},
pages = {},
title = {Adam: A Method for Stochastic Optimization},
journal = {International Conference on Learning Representations}
}

@inproceedings{ObjectAMSGrad,
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian},
year = {2017},
month = {07},
pages = {},
title = {Densely Connected Convolutional Networks},
doi = {10.1109/CVPR.2017.243}
}

@article{translateAMSGrad,
author = {Johnson, Melvin and Schuster, Mike and Le, Quoc and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Viégas, Fernanda and Wattenberg, Martin and Corrado, G.s and Hughes, Macduff and Dean, Jeffrey},
year = {2016},
month = {11},
pages = {},
title = {Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
volume = {5},
journal = {Transactions of the Association for Computational Linguistics},
doi = {10.1162/tacl_a_00065}
}

@ARTICLE{AMSGrad,
       author = {{Reddi}, Sashank J. and {Kale}, Satyen and {Kumar}, Sanjiv},
        title = "{On the Convergence of Adam and Beyond}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
         year = 2019,
        month = apr,
          eid = {arXiv:1904.09237},
        pages = {arXiv:1904.09237},
archivePrefix = {arXiv},
       eprint = {1904.09237},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190409237R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{HarvardClassification,
       author = {{Cannon}, Annie Jump and {Pickering}, Edward Charles},
        title = "{Classification of 1,477 stars by means of their photographic spectra}",
      journal = {Annals of Harvard College Observatory},
     keywords = {STARS: SPECTRA, STARS: CLASSIFICATION},
         year = 1912,
        month = jan,
       volume = {56},
       number = {4},
        pages = {65-114},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1912AnHar..56...65C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Nebulae,
       author = {{Herschel}, William},
        title = "{Catalogue of 500 New Nebulae, Nebulous Stars, Planetary Nebulae, and Clusters of Stars; With Remarks on the Construction of the Heavens}",
      journal = {Philosophical Transactions of the Royal Society of London Series I},
         year = 1802,
        month = jan,
       volume = {92},
        pages = {477-528},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1802RSPT...92..477H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Hubble,
       author = {{Hubble}, E.~P.},
        title = "{The classification of spiral nebulae}",
      journal = {The Observatory},
         year = 1927,
        month = sep,
       volume = {50},
        pages = {276-281},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1927Obs....50..276H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{SVM,
  title={A class of algorithms for pattern recognition learning},
  author={Vapnik, V and Chervonenkis, A Ya},
  journal={Avtomat. i Telemekh},
  volume={25},
  number={6},
  pages={937--945},
  year={1964}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{adorf_1988_supervised,
  author = {Adorf, H. -M. and Meurs, E. J. A.},
  pages = {315-322},
  title = {Supervised and unsupervised classification — The case of IRAS point sources},
  url = {http://adsabs.harvard.edu/abs/1988LNP...310..315A},
  urldate = {2019-12-05},
  year = {1988},
  journal = {Large-Scale Structures in the Universe Observational and Analytical Methods}
}

@article{MCC,
title = "Comparison of the predicted and observed secondary structure of T4 phage lysozyme",
journal = "Biochimica et Biophysica Acta (BBA) - Protein Structure",
volume = "405",
number = "2",
pages = "442 - 451",
year = "1975",
issn = "0005-2795",
doi = "https://doi.org/10.1016/0005-2795(75)90109-9",
url = "http://www.sciencedirect.com/science/article/pii/0005279575901099",
author = "B.W. Matthews",
abstract = "Predictions of the secondary structure of T4 phage lysozyme, made by a number of investigators on the basis of the amino acid sequence, are compared with the structure of the protein determined experimentally by X-ray crystallography. Within the amino terminal half of the molecule the locations of helices predicted by a number of methods agree moderately well with the observed structure, however within the carboxyl half of the molecule the overall agreement is poor. For eleven different helix predictions, the coefficients giving the correlation between prediction and observation range from 0.14 to 0.42. The accuracy of the predictions for both Î²-sheet regions and for turns are generally lower than for the helices, and in a number of instances the agreement between prediction and observation is no better than would be expected for a random selection of residues. The structural predictions for T4 phage lysozyme are much less successful than was the case for adenylate kinase (Schulz et al. (1974) Nature 250, 140â142). No one method of prediction is clearly superior to all others, and although empirical predictions based on larger numbers of known protein structure tend to be more accurate than those based on a limited sample, the improvement in accuracy is not dramatic, suggesting that the accuracy of current empirical predictive methods will not be substantially increased simply by the inclusion of more data from additional protein structure determinations."
}

@article{FNN,
author = {Sazli, Murat},
year = {2006},
month = {01},
pages = {11-17},
title = {A brief review of feed-forward neural networks},
volume = {50},
journal = {Communications, Faculty Of Science, University of Ankara},
doi = {10.1501/0003168}
}

@article{GradientDescent,
  title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es},
  author={Cauchy, Augustin},
  journal={Comp. Rend. Sci. Paris},
  volume={25},
  number={1847},
  pages={536--538},
  year={1847}
}

@inproceedings{AutoKeras,
  title={Auto-keras: An efficient neural architecture search system},
  author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1946--1956},
  year={2019}
}

@inproceedings{AutoML,
  title={AM-LFS: AutoML for Loss Function Search},
  author={Li, Chuming and Yuan, Xin and Lin, Chen and Guo, Minghao and Wu, Wei and Yan, Junjie and Ouyang, Wanli},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={8410--8419},
  year={2019}
}

@article{History,
author = {Wang, Haohan and Raj, Bhiksha},
year = {2017},
month = {02},
pages = {},
title = {On the Origin of Deep Learning}
}

@article{PCA,
author = {Jolliffe, Ian and Cadima, Jorge},
year = {2016},
month = {04},
pages = {20150202},
title = {Principal component analysis: A review and recent developments},
volume = {374},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
doi = {10.1098/rsta.2015.0202}
}

@inproceedings{SVM,
  title={A Review on Support Vector Machine for Data Classification},
  author={Himani P. Bhavsar and Mahesh H. Panchal},
  year={2012}
}

@article{SVMPic,
author = {Baghaee, Hamid and Mlakić, Dragan and Nikolovski, Srete and Dragicevic, Tomislav},
year = {2019},
month = {05},
pages = {1-19},
title = {Support Vector Machine-based Islanding and Grid Fault Detection in Active Distribution Networks},
volume = {PP},
journal = {IEEE Journal of Emerging and Selected Topics in Power Electronics},
doi = {10.1109/JESTPE.2019.2916621}
}

@article{GOTO,
author = {Gompertz, BP and Cutter, R and Steeghs, D and Galloway, Duncan and Lyman, J and Ulaczyk, K and Dyer, MJ and Ackley, K and Dhillon, VS and O'Brien, PT and Ramsay, G and Poshyachinda, S and Kotak, Raj and Nuttall, L and Breton, Rene and Pallé, E and Pollacco, D and Thrane, E and Aukkaravittayapun, S and Wiersema, K},
year = {2020},
month = {03},
pages = {},
title = {Searching for electromagnetic counterparts to gravitational-wave merger events with the prototype Gravitational-wave Optical Transient Observer (GOTO-4)}
}

@article{CNNdiag,
author = {Kamencay, Patrik and Benco, Miroslav and Mizdos, Tomas and Radil, Roman},
year = {2017},
month = {11},
pages = {},
title = {A New Method for Face Recognition Using Convolutional Neural Network},
volume = {15},
journal = {Advances in Electrical and Electronic Engineering},
doi = {10.15598/aeee.v15i4.2389}
}

@phdthesis{convolution,
author = {S. Mohamed, Ihab},
year = {2017},
month = {09},
pages = {},
title = {Detection and Tracking of Pallets using a Laser Rangefinder and Machine Learning Techniques},
doi = {10.13140/RG.2.2.30795.69926}
}

@article{pooling,
author = {Yani, Muhamad and Irawan, S, and S.T., M.T.},
year = {2019},
month = {05},
pages = {012052},
title = {Application of Transfer Learning Using Convolutional Neural Network Method for Early Detection of Terry’s Nail},
volume = {1201},
journal = {Journal of Physics: Conference Series},
doi = {10.1088/1742-6596/1201/1/012052}
}

@article{MNIST,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

@article{LogisticRegression,
author = {Peng, Joanne and Lee, Kuk and Ingersoll, Gary},
year = {2002},
month = {09},
pages = {3-14},
title = {An Introduction to Logistic Regression Analysis and Reporting},
volume = {96},
journal = {Journal of Educational Research - J EDUC RES},
doi = {10.1080/00220670209598786}
}

@unknown{Dataloader,
author = {Aizman, Alex and Maltby, Gavin and Breuel, Thomas},
year = {2020},
month = {01},
pages = {},
title = {High Performance I/O For Large Scale Deep Learning}
}

@article{Khalifa2017,
author = {Khalifa, Nour Eldeen and Taha, Mohamed and Hassanien, Aboul Ella and Selim, Ibrahim},
year = {2017},
month = {09},
pages = {},
title = {Deep Galaxy: Classification of Galaxies based on Deep Convolutional Neural Networks}
}

@article{Aniyan2017,
	doi = {10.3847/1538-4365/aa7333},
	url = {https://doi.org/10.3847%2F1538-4365%2Faa7333},
	year = 2017,
	month = {jun},
	publisher = {American Astronomical Society},
	volume = {230},
	number = {2},
	pages = {20},
	author = {A. K. Aniyan and K. Thorat},
	title = {Classifying Radio Galaxies with the Convolutional Neural Network},
	journal = {The Astrophysical Journal Supplement Series},
	abstract = {We present the application of aÂ deep machine learning technique to classify radio images of extended sources on a morphological basis using convolutional neural networks (CNN). In this study, we have taken the case of theÂ FanaroffâRiley (FR) class of radio galaxies as well as radio galaxies with bent-tailed morphology. We have used archival data from the Very Large Array (VLA)âFaint Images of the Radio Sky at Twenty Centimeters survey and existing visually classified samples available in the literature to train a neural network for morphological classification of these categories of radio sources. Our training sample size for each of these categories is âŒ200 sources, which has been augmented by rotated versions of the same. Our study shows that CNNs can classify images of the FRI and FRII and bent-tailed radio galaxies with high accuracy (maximum precision at 95%) using well-defined samples and aÂ âfusion classifier,â which combines the results of binary classifications, while allowing for a mechanism to find sources with unusual morphologies. The individual precision is highest for bent-tailed radio galaxies at 95% and is 91% and 75% for the FRI and FRII classes, respectively, whereas the recall is highest for FRI and FRIIs at 91% each, while the bent-tailed class has a recall of 79%. These results show that our results are comparable to that of manual classification, while being much faster. Finally, we discuss the computational and data-related challenges associated with theÂ morphological classification of radio galaxies with CNNs.}
}

@article{Kim2016,
    author = {Kim, Edward J. and Brunner, Robert J.},
    title = "{Star–galaxy classification using deep convolutional neural networks}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {464},
    number = {4},
    pages = {4463-4475},
    year = {2016},
    month = {10},
    abstract = "{Most existing star–galaxy classifiers use the reduced summary information from catalogues, requiring careful feature extraction and selection. The latest advances in machine learning that use deep convolutional neural networks (ConvNets) allow a machine to automatically learn the features directly from the data, minimizing the need for input from human experts. We present a star–galaxy classification framework that uses deep ConvNets directly on the reduced, calibrated pixel values. Using data from the Sloan Digital Sky Survey and the Canada–France–Hawaii Telescope Lensing Survey, we demonstrate that ConvNets are able to produce accurate and well-calibrated probabilistic classifications that are competitive with conventional machine learning techniques. Future advances in deep learning may bring more success with current and forthcoming photometric surveys, such as the Dark Energy Survey and the Large Synoptic Survey Telescope, because deep neural networks require very little, manual feature engineering.}",
    issn = {0035-8711},
    doi = {10.1093/mnras/stw2672},
    url = {https://doi.org/10.1093/mnras/stw2672},
    eprint = {https://academic.oup.com/mnras/article-pdf/464/4/4463/8313746/stw2672.pdf},
}

@ARTICLE{Hocking2018,
       author = {{Hocking}, Alex and {Geach}, James E. and {Sun}, Yi and {Davey}, Neil},
        title = "{An automatic taxonomy of galaxy morphology using unsupervised machine learning}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: observational, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = 2018,
        month = jan,
       volume = {473},
       number = {1},
        pages = {1108-1129},
          doi = {10.1093/mnras/stx2351},
archivePrefix = {arXiv},
       eprint = {1709.05834},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.473.1108H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@unpublished{GOTOLSST
author = "J. R. Mullaney, L. Makrygianni, and friends",
title = "Processing GOTO data with the LSST stack i : Production of coadded frames",
note = "submitted",
}

@article{Flatten,
author = {Dumoulin, Vincent and Visin, Francesco},
year = {2016},
month = {03},
pages = {},
title = {A guide to convolution arithmetic for deep learning}
}

@inproceedings{F1,
author = {Sokolova, Marina and Japkowicz, Nathalie and Szpakowicz, Stan},
year = {2006},
month = {01},
pages = {1015-1021},
title = {Beyond Accuracy, F-Score and ROC: A Family of Discriminant Measures for Performance Evaluation},
volume = {Vol. 4304},
journal = {AI 2006: Advances in Artificial Intelligence, Lecture Notes in Computer Science},
doi = {10.1007/11941439_114}
}

@ARTICLE{LSST,
       author = {{Bosch}, James and {Armstrong}, Robert and {Bickerton}, Steven and
         {Furusawa}, Hisanori and {Ikeda}, Hiroyuki and {Koike}, Michitaro and
         {Lupton}, Robert and {Mineo}, Sogo and {Price}, Paul and
         {Takata}, Tadafumi and {Tanaka}, Masayuki and {Yasuda}, Naoki and
         {AlSayyad}, Yusra and {Becker}, Andrew C. and {Coulton}, William and
         {Coupon}, Jean and {Garmilla}, Jose and {Huang}, Song and
         {Krughoff}, K. Simon and {Lang}, Dustin and {Leauthaud}, Alexie and
         {Lim}, Kian-Tat and {Lust}, Nate B. and {MacArthur}, Lauren A. and {Mand
        elbaum}, Rachel and {Miyatake}, Hironao and {Miyazaki}, Satoshi and
         {Murata}, Ryoma and {More}, Surhud and {Okura}, Yuki and
         {Owen}, Russell and {Swinbank}, John D. and {Strauss}, Michael A. and
         {Yamada}, Yoshihiko and {Yamanoi}, Hitomi},
        title = "{The Hyper Suprime-Cam software pipeline}",
      journal = {\pasj},
     keywords = {methods: data analysis, surveys, techniques: image processing, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = 2018,
        month = jan,
       volume = {70},
          eid = {S5},
        pages = {S5},
          doi = {10.1093/pasj/psx080},
archivePrefix = {arXiv},
       eprint = {1705.06766},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PASJ...70S...5B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@misc{sourceproperties - photutils v0.7.2, title={SourceProperties¶}, url=\url{https://photutils.readthedocs.io/en/stable/api/photutils.segmentation.SourceProperties.html#photutils.segmentation.SourceProperties}, journal={SourceProperties - photutils v0.7.2}}

@misc{photutils - photutils v0.7.2, title={MedianBackground¶}, url=\url{https://photutils.readthedocs.io/en/stable/api/photutils.background.MedianBackground.html}, journal={MedianBackground - photutils v0.7.2}}

@misc{KaggleMNIST, title={How to choose CNN Architecture MNIST}, url=\url{https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist}, journal={Kaggle}, publisher={Kaggle}, author={Deotte, Chris}, year={2018}, month={Sep}}
