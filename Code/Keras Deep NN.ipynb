{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3123, 1600)\n"
     ]
    }
   ],
   "source": [
    "galaxy = np.zeros(1600)\n",
    "for filepath in glob.iglob('../Data/cutouts/galaxyfits/*fits', recursive=True):\n",
    "    fp = Path(filepath)\n",
    "    hdulist = fits.open(fp)\n",
    "    scidata = hdulist[0].data\n",
    "    scidata = scidata.flatten()\n",
    "    galaxy = np.vstack((galaxy,scidata.transpose()))\n",
    "galaxy = galaxy[1:,]\n",
    "print(galaxy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3123"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(galaxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26.60893512  60.82042313  30.41021156 ...  60.82042313 -19.00638223\n",
      "    1.        ]\n",
      " [ 64.62169957  22.80765867 -11.40382934 ...  -7.60255289 -38.01276445\n",
      "    1.        ]\n",
      " [ 48.78079796 115.3000679  -88.69235992 ... -53.21541595 -31.04232597\n",
      "    1.        ]\n",
      " ...\n",
      " [  0.          69.7310257    8.71637821 ... -17.43275642  47.94008017\n",
      "    1.        ]\n",
      " [  4.35818911  52.29826927  39.22370195 ... -30.50732374 -43.58189106\n",
      "    1.        ]\n",
      " [ 21.79094553  47.94008017  78.44740391 ...  21.79094553 -61.01464748\n",
      "    1.        ]]\n"
     ]
    }
   ],
   "source": [
    "ones = np.ones((len(galaxy),1))\n",
    "galaxy = np.hstack((galaxy,ones))\n",
    "print(galaxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3123, 1600)\n"
     ]
    }
   ],
   "source": [
    "star = np.zeros(1600)\n",
    "count = 0\n",
    "for filepath in glob.iglob('../Data/cutouts/starfits/*fits', recursive=True):\n",
    "    fp = Path(filepath)\n",
    "    hdulist = fits.open(fp)\n",
    "    scidata = hdulist[0].data\n",
    "    scidata = scidata.flatten()\n",
    "    star = np.vstack((star,scidata.transpose()))\n",
    "    count += 1\n",
    "    if count == len(galaxy):\n",
    "        break\n",
    "star = star[1:,]\n",
    "print(star.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.90063822e+01  3.80127645e+01  1.52051058e+01 ... -1.14038293e+02\n",
      "   4.18140409e+01  0.00000000e+00]\n",
      " [ 4.99866815e-12  5.70191467e+01  1.14038293e+01 ... -1.10237017e+02\n",
      "   4.99866815e-12  0.00000000e+00]\n",
      " [-3.80127645e+00  7.22242525e+01  7.60255289e+00 ... -7.60255289e+00\n",
      "   4.94165938e+01  0.00000000e+00]\n",
      " ...\n",
      " [ 3.44787161e+01 -3.76131449e+01 -1.25377150e+01 ... -3.76131449e+01\n",
      "  -3.13442874e+00  0.00000000e+00]\n",
      " [-3.13442874e+01  4.07475736e+01  2.19410012e+01 ... -3.13442874e+01\n",
      "   3.13442874e+01  0.00000000e+00]\n",
      " [ 1.56721437e+01 -3.13442874e+01 -3.76131449e+01 ... -4.38820024e+01\n",
      "   9.40328622e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "zeros = np.zeros((len(star),1))\n",
    "star = np.hstack((star,zeros))\n",
    "print(star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.vstack((galaxy,star)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1591</th>\n",
       "      <th>1592</th>\n",
       "      <th>1593</th>\n",
       "      <th>1594</th>\n",
       "      <th>1595</th>\n",
       "      <th>1596</th>\n",
       "      <th>1597</th>\n",
       "      <th>1598</th>\n",
       "      <th>1599</th>\n",
       "      <th>1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.498241</td>\n",
       "      <td>16.166994</td>\n",
       "      <td>17.030917</td>\n",
       "      <td>18.865960</td>\n",
       "      <td>19.642405</td>\n",
       "      <td>15.751126</td>\n",
       "      <td>12.082307</td>\n",
       "      <td>10.947788</td>\n",
       "      <td>10.674772</td>\n",
       "      <td>11.464448</td>\n",
       "      <td>...</td>\n",
       "      <td>11.416840</td>\n",
       "      <td>10.848935</td>\n",
       "      <td>10.765682</td>\n",
       "      <td>11.982885</td>\n",
       "      <td>12.248577</td>\n",
       "      <td>16.667663</td>\n",
       "      <td>21.197518</td>\n",
       "      <td>22.188929</td>\n",
       "      <td>21.904926</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>789.364095</td>\n",
       "      <td>792.656985</td>\n",
       "      <td>788.375624</td>\n",
       "      <td>798.481783</td>\n",
       "      <td>664.580959</td>\n",
       "      <td>415.159451</td>\n",
       "      <td>221.475164</td>\n",
       "      <td>146.449387</td>\n",
       "      <td>230.554243</td>\n",
       "      <td>249.099017</td>\n",
       "      <td>...</td>\n",
       "      <td>117.041259</td>\n",
       "      <td>147.647160</td>\n",
       "      <td>217.965113</td>\n",
       "      <td>278.130590</td>\n",
       "      <td>293.304200</td>\n",
       "      <td>569.699828</td>\n",
       "      <td>996.746178</td>\n",
       "      <td>1005.059266</td>\n",
       "      <td>1010.097000</td>\n",
       "      <td>0.50004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-139.462051</td>\n",
       "      <td>-755.397326</td>\n",
       "      <td>-126.387484</td>\n",
       "      <td>-149.747497</td>\n",
       "      <td>-132.017389</td>\n",
       "      <td>-128.976402</td>\n",
       "      <td>-139.420083</td>\n",
       "      <td>-134.256376</td>\n",
       "      <td>-5601.224158</td>\n",
       "      <td>-2385.300271</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.238349</td>\n",
       "      <td>-478.960832</td>\n",
       "      <td>-156.815577</td>\n",
       "      <td>-186.262546</td>\n",
       "      <td>-132.017389</td>\n",
       "      <td>-141.907776</td>\n",
       "      <td>-131.775187</td>\n",
       "      <td>-142.172573</td>\n",
       "      <td>-2543.053942</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.612554</td>\n",
       "      <td>-18.612554</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.612554</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.432756</td>\n",
       "      <td>-17.906025</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.612554</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>-18.806572</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.134429</td>\n",
       "      <td>2.923796</td>\n",
       "      <td>2.963288</td>\n",
       "      <td>3.008710</td>\n",
       "      <td>3.134429</td>\n",
       "      <td>3.134429</td>\n",
       "      <td>3.003340</td>\n",
       "      <td>3.134429</td>\n",
       "      <td>3.102092</td>\n",
       "      <td>3.134429</td>\n",
       "      <td>...</td>\n",
       "      <td>3.134429</td>\n",
       "      <td>3.134429</td>\n",
       "      <td>3.014081</td>\n",
       "      <td>3.134429</td>\n",
       "      <td>3.102092</td>\n",
       "      <td>3.134429</td>\n",
       "      <td>2.949377</td>\n",
       "      <td>2.956272</td>\n",
       "      <td>2.951470</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25.075430</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>26.598812</td>\n",
       "      <td>...</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>24.112644</td>\n",
       "      <td>25.075430</td>\n",
       "      <td>24.483574</td>\n",
       "      <td>24.812742</td>\n",
       "      <td>23.611759</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61713.767453</td>\n",
       "      <td>62193.335050</td>\n",
       "      <td>61792.128171</td>\n",
       "      <td>62171.394049</td>\n",
       "      <td>49762.190669</td>\n",
       "      <td>29805.282884</td>\n",
       "      <td>15007.644805</td>\n",
       "      <td>6672.651478</td>\n",
       "      <td>15791.548860</td>\n",
       "      <td>18441.157375</td>\n",
       "      <td>...</td>\n",
       "      <td>5162.133413</td>\n",
       "      <td>9434.768137</td>\n",
       "      <td>15820.912566</td>\n",
       "      <td>20549.700464</td>\n",
       "      <td>19314.285619</td>\n",
       "      <td>41971.308360</td>\n",
       "      <td>77651.573605</td>\n",
       "      <td>78368.156918</td>\n",
       "      <td>78808.654020</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1             2             3             4     \\\n",
       "count   6246.000000   6246.000000   6246.000000   6246.000000   6246.000000   \n",
       "mean      17.498241     16.166994     17.030917     18.865960     19.642405   \n",
       "std      789.364095    792.656985    788.375624    798.481783    664.580959   \n",
       "min     -139.462051   -755.397326   -126.387484   -149.747497   -132.017389   \n",
       "25%      -18.806572    -18.806572    -18.806572    -18.806572    -18.806572   \n",
       "50%        3.134429      2.923796      2.963288      3.008710      3.134429   \n",
       "75%       25.075430     25.075430     25.075430     25.075430     25.075430   \n",
       "max    61713.767453  62193.335050  61792.128171  62171.394049  49762.190669   \n",
       "\n",
       "               5             6            7             8             9     \\\n",
       "count   6246.000000   6246.000000  6246.000000   6246.000000   6246.000000   \n",
       "mean      15.751126     12.082307    10.947788     10.674772     11.464448   \n",
       "std      415.159451    221.475164   146.449387    230.554243    249.099017   \n",
       "min     -128.976402   -139.420083  -134.256376  -5601.224158  -2385.300271   \n",
       "25%      -18.612554    -18.612554   -18.806572    -18.806572    -18.612554   \n",
       "50%        3.134429      3.003340     3.134429      3.102092      3.134429   \n",
       "75%       25.075430     25.075430    25.075430     25.075430     26.598812   \n",
       "max    29805.282884  15007.644805  6672.651478  15791.548860  18441.157375   \n",
       "\n",
       "       ...         1591         1592          1593          1594  \\\n",
       "count  ...  6246.000000  6246.000000   6246.000000   6246.000000   \n",
       "mean   ...    11.416840    10.848935     10.765682     11.982885   \n",
       "std    ...   117.041259   147.647160    217.965113    278.130590   \n",
       "min    ...  -100.238349  -478.960832   -156.815577   -186.262546   \n",
       "25%    ...   -17.432756   -17.906025    -18.806572    -18.806572   \n",
       "50%    ...     3.134429     3.134429      3.014081      3.134429   \n",
       "75%    ...    25.075430    25.075430     25.075430     25.075430   \n",
       "max    ...  5162.133413  9434.768137  15820.912566  20549.700464   \n",
       "\n",
       "               1595          1596          1597          1598          1599  \\\n",
       "count   6246.000000   6246.000000   6246.000000   6246.000000   6246.000000   \n",
       "mean      12.248577     16.667663     21.197518     22.188929     21.904926   \n",
       "std      293.304200    569.699828    996.746178   1005.059266   1010.097000   \n",
       "min     -132.017389   -141.907776   -131.775187   -142.172573  -2543.053942   \n",
       "25%      -18.612554    -18.806572    -18.806572    -18.806572    -18.806572   \n",
       "50%        3.102092      3.134429      2.949377      2.956272      2.951470   \n",
       "75%       24.112644     25.075430     24.483574     24.812742     23.611759   \n",
       "max    19314.285619  41971.308360  77651.573605  78368.156918  78808.654020   \n",
       "\n",
       "             1600  \n",
       "count  6246.00000  \n",
       "mean      0.50000  \n",
       "std       0.50004  \n",
       "min       0.00000  \n",
       "25%       0.00000  \n",
       "50%       0.50000  \n",
       "75%       1.00000  \n",
       "max       1.00000  \n",
       "\n",
       "[8 rows x 1601 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0           1          2          3             4           5     \\\n",
      "0     26.608935   60.820423  30.410212 -19.006382 -4.561532e+01   11.403829   \n",
      "1     64.621700   22.807659 -11.403829  -3.801276  4.998668e-12  125.442123   \n",
      "2     48.780798  115.300068 -88.692360 -35.476944 -3.552714e-15  -53.215416   \n",
      "3      8.716378  -39.223702  13.074567  -4.358189 -3.486551e+01   17.432756   \n",
      "4     -8.716378   17.432756  56.656458 -13.074567  6.537284e+01  -26.149135   \n",
      "...         ...         ...        ...        ...           ...         ...   \n",
      "6241   6.268857   -6.268857  25.075430 -12.537715  9.403286e+00   18.806572   \n",
      "6242 -12.537715   75.226290   3.134429   9.403286 -2.820986e+01   -3.134429   \n",
      "6243  34.478716  -37.613145 -12.537715  12.537715  6.268857e+00    6.268857   \n",
      "6244 -31.344287   40.747574  21.941001  -6.268857  2.194100e+01   15.672144   \n",
      "6245  15.672144  -31.344287 -37.613145 -34.478716  2.194100e+01   34.478716   \n",
      "\n",
      "           6             7          8             9     ...       1590  \\\n",
      "0    -22.807659  4.998668e-12  41.814041  7.602553e+01  ...   7.602553   \n",
      "1     45.615317  3.421149e+01  72.224252  1.900638e+01  ...  41.814041   \n",
      "2    -13.303854  4.434618e+00  44.346180 -3.552714e-15  ... -39.911562   \n",
      "3    -43.581891 -2.179095e+01 -30.507324  3.050732e+01  ...  13.074567   \n",
      "4      0.000000  4.794008e+01 -21.790946  3.922370e+01  ... -26.149135   \n",
      "...         ...           ...        ...           ...  ...        ...   \n",
      "6241   9.403286  2.194100e+01  31.344287 -6.895743e+01  ...  28.209859   \n",
      "6242 -28.209859 -1.567214e+01   9.403286  1.253771e+01  ...   6.268857   \n",
      "6243 -31.344287  2.507543e+01  18.806572  1.253771e+01  ... -37.613145   \n",
      "6244  21.941001 -1.880657e+01 -12.537715  3.761314e+01  ...  28.209859   \n",
      "6245   6.268857 -2.820986e+01 -18.806572 -4.074757e+01  ...  37.613145   \n",
      "\n",
      "           1591       1592          1593       1594       1595        1596  \\\n",
      "0    -26.608935  30.410212  3.801276e+00   3.801276 -15.205106  -11.403829   \n",
      "1     68.422976  -7.602553  4.998668e-12  53.217870  57.019147  -76.025529   \n",
      "2     62.084652  44.346180 -2.660771e+01  66.519270  -8.869236  137.473158   \n",
      "3    -82.805593   4.358189  2.179095e+01   8.716378  47.940080  -21.790946   \n",
      "4     74.089215  -4.358189 -4.358189e+01 -69.731026 -43.581891   30.507324   \n",
      "...         ...        ...           ...        ...        ...         ...   \n",
      "6241 -47.016431  37.613145 -5.955415e+01 -56.419717   3.134429   25.075430   \n",
      "6242 -56.419717  15.672144 -4.701643e+01  -6.268857 -25.075430    0.000000   \n",
      "6243 -15.672144  34.478716 -9.403286e+00 -56.419717  47.016431  -43.882002   \n",
      "6244   9.403286  -6.268857  3.134429e+01 -31.344287  -6.268857   -9.403286   \n",
      "6245 -28.209859 -50.150860  1.880657e+01  21.941001  21.941001    6.268857   \n",
      "\n",
      "           1597        1598       1599  \n",
      "0     72.224252   60.820423 -19.006382  \n",
      "1     26.608935   -7.602553 -38.012764  \n",
      "2    -48.780798  -53.215416 -31.042326  \n",
      "3      0.000000 -100.238349  56.656458  \n",
      "4     -4.358189    8.716378 -21.790946  \n",
      "...         ...         ...        ...  \n",
      "6241 -37.613145   -6.268857  43.882002  \n",
      "6242  -3.134429    0.000000   3.134429  \n",
      "6243   0.000000  -37.613145  -3.134429  \n",
      "6244  47.016431  -31.344287  31.344287  \n",
      "6245   0.000000  -43.882002   9.403286  \n",
      "\n",
      "[6246 rows x 1600 columns] 0       1.0\n",
      "1       1.0\n",
      "2       1.0\n",
      "3       1.0\n",
      "4       1.0\n",
      "       ... \n",
      "6241    0.0\n",
      "6242    0.0\n",
      "6243    0.0\n",
      "6244    0.0\n",
      "6245    0.0\n",
      "Name: 1600, Length: 6246, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df.drop([1600],axis=1)\n",
    "y = df[1600]\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.93621959e-05,  1.35685019e-04,  6.78425096e-05, ...,\n",
       "         1.61125960e-04,  1.35685019e-04, -4.24015685e-05],\n",
       "       [ 1.67091678e-04,  5.89735333e-05, -2.94867667e-05, ...,\n",
       "         6.88024556e-05, -1.96578444e-05, -9.82892222e-05],\n",
       "       [ 2.28522769e-04,  5.40144726e-04, -4.15495943e-04, ...,\n",
       "        -2.28522769e-04, -2.49297566e-04, -1.45423580e-04],\n",
       "       ...,\n",
       "       [ 1.58349807e-03, -1.72745244e-03, -5.75817480e-04, ...,\n",
       "         0.00000000e+00, -1.72745244e-03, -1.43954370e-04],\n",
       "       [-2.35911285e-03,  3.06684671e-03,  1.65137900e-03, ...,\n",
       "         3.53866928e-03, -2.35911285e-03,  2.35911285e-03],\n",
       "       [ 6.55707920e-04, -1.31141584e-03, -1.57369901e-03, ...,\n",
       "         0.00000000e+00, -1.83598218e-03,  3.93424752e-04]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "transformer = Normalizer().fit(X)\n",
    "transformer\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1590</th>\n",
       "      <th>1591</th>\n",
       "      <th>1592</th>\n",
       "      <th>1593</th>\n",
       "      <th>1594</th>\n",
       "      <th>1595</th>\n",
       "      <th>1596</th>\n",
       "      <th>1597</th>\n",
       "      <th>1598</th>\n",
       "      <th>1599</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "      <td>6246.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.010730</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>0.010037</td>\n",
       "      <td>0.012365</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.012327</td>\n",
       "      <td>0.010209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010240</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.009812</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.009817</td>\n",
       "      <td>0.010328</td>\n",
       "      <td>0.010026</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.012201</td>\n",
       "      <td>0.014302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.046769</td>\n",
       "      <td>-0.048820</td>\n",
       "      <td>-0.046747</td>\n",
       "      <td>-0.052804</td>\n",
       "      <td>-0.043172</td>\n",
       "      <td>-0.044975</td>\n",
       "      <td>-0.045145</td>\n",
       "      <td>-0.039824</td>\n",
       "      <td>-0.226332</td>\n",
       "      <td>-0.109372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043176</td>\n",
       "      <td>-0.039813</td>\n",
       "      <td>-0.086491</td>\n",
       "      <td>-0.056570</td>\n",
       "      <td>-0.057899</td>\n",
       "      <td>-0.046037</td>\n",
       "      <td>-0.057203</td>\n",
       "      <td>-0.045459</td>\n",
       "      <td>-0.045208</td>\n",
       "      <td>-0.163675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.001335</td>\n",
       "      <td>-0.001454</td>\n",
       "      <td>-0.001463</td>\n",
       "      <td>-0.001422</td>\n",
       "      <td>-0.001363</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>-0.001452</td>\n",
       "      <td>-0.001504</td>\n",
       "      <td>-0.001526</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001276</td>\n",
       "      <td>-0.001348</td>\n",
       "      <td>-0.001394</td>\n",
       "      <td>-0.001491</td>\n",
       "      <td>-0.001396</td>\n",
       "      <td>-0.001397</td>\n",
       "      <td>-0.001305</td>\n",
       "      <td>-0.001434</td>\n",
       "      <td>-0.001432</td>\n",
       "      <td>-0.001381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.001564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.749077</td>\n",
       "      <td>0.477454</td>\n",
       "      <td>0.233318</td>\n",
       "      <td>0.194644</td>\n",
       "      <td>0.156820</td>\n",
       "      <td>0.315806</td>\n",
       "      <td>0.563615</td>\n",
       "      <td>0.499648</td>\n",
       "      <td>0.524072</td>\n",
       "      <td>0.381915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337170</td>\n",
       "      <td>0.348117</td>\n",
       "      <td>0.256195</td>\n",
       "      <td>0.249393</td>\n",
       "      <td>0.359902</td>\n",
       "      <td>0.464029</td>\n",
       "      <td>0.309904</td>\n",
       "      <td>0.416777</td>\n",
       "      <td>0.448118</td>\n",
       "      <td>0.496360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0            1            2            3            4     \\\n",
       "count  6246.000000  6246.000000  6246.000000  6246.000000  6246.000000   \n",
       "mean      0.000532     0.000486     0.000338     0.000296     0.000464   \n",
       "std       0.013333     0.010730     0.009127     0.008808     0.009077   \n",
       "min      -0.046769    -0.048820    -0.046747    -0.052804    -0.043172   \n",
       "25%      -0.001335    -0.001454    -0.001463    -0.001422    -0.001363   \n",
       "50%       0.000028     0.000009     0.000014     0.000019     0.000027   \n",
       "75%       0.001716     0.001667     0.001618     0.001653     0.001638   \n",
       "max       0.749077     0.477454     0.233318     0.194644     0.156820   \n",
       "\n",
       "              5            6            7            8            9     ...  \\\n",
       "count  6246.000000  6246.000000  6246.000000  6246.000000  6246.000000  ...   \n",
       "mean      0.000487     0.000361     0.000509     0.000427     0.000288  ...   \n",
       "std       0.010037     0.012365     0.012497     0.012327     0.010209  ...   \n",
       "min      -0.044975    -0.045145    -0.039824    -0.226332    -0.109372  ...   \n",
       "25%      -0.001419    -0.001452    -0.001504    -0.001526    -0.001472  ...   \n",
       "50%       0.000030     0.000019     0.000035     0.000026     0.000045  ...   \n",
       "75%       0.001633     0.001526     0.001597     0.001476     0.001614  ...   \n",
       "max       0.315806     0.563615     0.499648     0.524072     0.381915  ...   \n",
       "\n",
       "              1590         1591         1592         1593         1594  \\\n",
       "count  6246.000000  6246.000000  6246.000000  6246.000000  6246.000000   \n",
       "mean      0.000649     0.000550     0.000410     0.000222     0.000313   \n",
       "std       0.010240     0.009897     0.009812     0.009531     0.009817   \n",
       "min      -0.043176    -0.039813    -0.086491    -0.056570    -0.057899   \n",
       "25%      -0.001276    -0.001348    -0.001394    -0.001491    -0.001396   \n",
       "50%       0.000048     0.000047     0.000042     0.000019     0.000031   \n",
       "75%       0.001796     0.001596     0.001631     0.001401     0.001592   \n",
       "max       0.337170     0.348117     0.256195     0.249393     0.359902   \n",
       "\n",
       "              1595         1596         1597         1598         1599  \n",
       "count  6246.000000  6246.000000  6246.000000  6246.000000  6246.000000  \n",
       "mean      0.000460     0.000475     0.000501     0.000525     0.000699  \n",
       "std       0.010328     0.010026     0.010827     0.012201     0.014302  \n",
       "min      -0.046037    -0.057203    -0.045459    -0.045208    -0.163675  \n",
       "25%      -0.001397    -0.001305    -0.001434    -0.001432    -0.001381  \n",
       "50%       0.000024     0.000024     0.000012     0.000016     0.000011  \n",
       "75%       0.001641     0.001656     0.001641     0.001589     0.001564  \n",
       "max       0.464029     0.309904     0.416777     0.448118     0.496360  \n",
       "\n",
       "[8 rows x 1600 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(transformer.transform(X))\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "4372/4372 [==============================] - 1s 314us/step - loss: 0.5228 - accuracy: 0.7466\n",
      "Epoch 2/250\n",
      "4372/4372 [==============================] - 1s 213us/step - loss: 0.3907 - accuracy: 0.8500\n",
      "Epoch 3/250\n",
      "4372/4372 [==============================] - 1s 215us/step - loss: 0.3753 - accuracy: 0.8584\n",
      "Epoch 4/250\n",
      "4372/4372 [==============================] - 1s 226us/step - loss: 0.3596 - accuracy: 0.8676\n",
      "Epoch 5/250\n",
      "4372/4372 [==============================] - 1s 197us/step - loss: 0.3482 - accuracy: 0.8705\n",
      "Epoch 6/250\n",
      "4372/4372 [==============================] - 1s 202us/step - loss: 0.3316 - accuracy: 0.8829\n",
      "Epoch 7/250\n",
      "4372/4372 [==============================] - 1s 200us/step - loss: 0.3157 - accuracy: 0.8859\n",
      "Epoch 8/250\n",
      "4372/4372 [==============================] - 1s 201us/step - loss: 0.2988 - accuracy: 0.8849\n",
      "Epoch 9/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.2944 - accuracy: 0.8907\n",
      "Epoch 10/250\n",
      "4372/4372 [==============================] - 1s 215us/step - loss: 0.2810 - accuracy: 0.8964\n",
      "Epoch 11/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.2692 - accuracy: 0.9026\n",
      "Epoch 12/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.2600 - accuracy: 0.9044\n",
      "Epoch 13/250\n",
      "4372/4372 [==============================] - 1s 202us/step - loss: 0.2576 - accuracy: 0.9039\n",
      "Epoch 14/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.2343 - accuracy: 0.9129\n",
      "Epoch 15/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.2483 - accuracy: 0.9081\n",
      "Epoch 16/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.2218 - accuracy: 0.9147\n",
      "Epoch 17/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.2273 - accuracy: 0.9147\n",
      "Epoch 18/250\n",
      "4372/4372 [==============================] - 1s 210us/step - loss: 0.2087 - accuracy: 0.9218\n",
      "Epoch 19/250\n",
      "4372/4372 [==============================] - 1s 220us/step - loss: 0.2144 - accuracy: 0.9206\n",
      "Epoch 20/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.2045 - accuracy: 0.9225\n",
      "Epoch 21/250\n",
      "4372/4372 [==============================] - 1s 201us/step - loss: 0.2089 - accuracy: 0.9199\n",
      "Epoch 22/250\n",
      "4372/4372 [==============================] - 1s 203us/step - loss: 0.2063 - accuracy: 0.9215\n",
      "Epoch 23/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1906 - accuracy: 0.9282\n",
      "Epoch 24/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1996 - accuracy: 0.9167\n",
      "Epoch 25/250\n",
      "4372/4372 [==============================] - 1s 217us/step - loss: 0.1922 - accuracy: 0.9291\n",
      "Epoch 26/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1897 - accuracy: 0.9323\n",
      "Epoch 27/250\n",
      "4372/4372 [==============================] - 1s 239us/step - loss: 0.1823 - accuracy: 0.9273\n",
      "Epoch 28/250\n",
      "4372/4372 [==============================] - 1s 221us/step - loss: 0.1877 - accuracy: 0.9266\n",
      "Epoch 29/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1769 - accuracy: 0.9321\n",
      "Epoch 30/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1693 - accuracy: 0.9325\n",
      "Epoch 31/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1789 - accuracy: 0.9296\n",
      "Epoch 32/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1777 - accuracy: 0.9293\n",
      "Epoch 33/250\n",
      "4372/4372 [==============================] - 1s 221us/step - loss: 0.1715 - accuracy: 0.9355\n",
      "Epoch 34/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1706 - accuracy: 0.9325\n",
      "Epoch 35/250\n",
      "4372/4372 [==============================] - 1s 200us/step - loss: 0.1831 - accuracy: 0.9280\n",
      "Epoch 36/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.1584 - accuracy: 0.9392\n",
      "Epoch 37/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1626 - accuracy: 0.9398\n",
      "Epoch 38/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1581 - accuracy: 0.9373\n",
      "Epoch 39/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1591 - accuracy: 0.9341\n",
      "Epoch 40/250\n",
      "4372/4372 [==============================] - 1s 203us/step - loss: 0.1613 - accuracy: 0.9350\n",
      "Epoch 41/250\n",
      "4372/4372 [==============================] - 1s 218us/step - loss: 0.1624 - accuracy: 0.9323\n",
      "Epoch 42/250\n",
      "4372/4372 [==============================] - 1s 203us/step - loss: 0.1509 - accuracy: 0.9389\n",
      "Epoch 43/250\n",
      "4372/4372 [==============================] - 1s 202us/step - loss: 0.1569 - accuracy: 0.9353\n",
      "Epoch 44/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.1548 - accuracy: 0.9355\n",
      "Epoch 45/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1491 - accuracy: 0.9403\n",
      "Epoch 46/250\n",
      "4372/4372 [==============================] - 1s 223us/step - loss: 0.1407 - accuracy: 0.9472\n",
      "Epoch 47/250\n",
      "4372/4372 [==============================] - 1s 230us/step - loss: 0.1501 - accuracy: 0.9437\n",
      "Epoch 48/250\n",
      "4372/4372 [==============================] - 1s 219us/step - loss: 0.1516 - accuracy: 0.9405\n",
      "Epoch 49/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1512 - accuracy: 0.9403\n",
      "Epoch 50/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1528 - accuracy: 0.9337\n",
      "Epoch 51/250\n",
      "4372/4372 [==============================] - 1s 226us/step - loss: 0.1374 - accuracy: 0.9449\n",
      "Epoch 52/250\n",
      "4372/4372 [==============================] - 1s 203us/step - loss: 0.1509 - accuracy: 0.9419\n",
      "Epoch 53/250\n",
      "4372/4372 [==============================] - 1s 202us/step - loss: 0.1571 - accuracy: 0.9428\n",
      "Epoch 54/250\n",
      "4372/4372 [==============================] - 1s 200us/step - loss: 0.1482 - accuracy: 0.9456\n",
      "Epoch 55/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1449 - accuracy: 0.9465\n",
      "Epoch 56/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1456 - accuracy: 0.9437\n",
      "Epoch 57/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1535 - accuracy: 0.9376\n",
      "Epoch 58/250\n",
      "4372/4372 [==============================] - 1s 220us/step - loss: 0.1400 - accuracy: 0.9483\n",
      "Epoch 59/250\n",
      "4372/4372 [==============================] - 1s 217us/step - loss: 0.1556 - accuracy: 0.9357\n",
      "Epoch 60/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1441 - accuracy: 0.9428\n",
      "Epoch 61/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1419 - accuracy: 0.9444\n",
      "Epoch 62/250\n",
      "4372/4372 [==============================] - 1s 214us/step - loss: 0.1290 - accuracy: 0.9499\n",
      "Epoch 63/250\n",
      "4372/4372 [==============================] - 1s 202us/step - loss: 0.1400 - accuracy: 0.9444\n",
      "Epoch 64/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1395 - accuracy: 0.9449\n",
      "Epoch 65/250\n",
      "4372/4372 [==============================] - 1s 201us/step - loss: 0.1315 - accuracy: 0.9458\n",
      "Epoch 66/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.1305 - accuracy: 0.9453\n",
      "Epoch 67/250\n",
      "4372/4372 [==============================] - 1s 210us/step - loss: 0.1398 - accuracy: 0.9446\n",
      "Epoch 68/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.1429 - accuracy: 0.9442\n",
      "Epoch 69/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1394 - accuracy: 0.9495\n",
      "Epoch 70/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1336 - accuracy: 0.9449\n",
      "Epoch 71/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1345 - accuracy: 0.9435\n",
      "Epoch 72/250\n",
      "4372/4372 [==============================] - 1s 202us/step - loss: 0.1361 - accuracy: 0.9474\n",
      "Epoch 73/250\n",
      "4372/4372 [==============================] - 1s 202us/step - loss: 0.1364 - accuracy: 0.9469\n",
      "Epoch 74/250\n",
      "4372/4372 [==============================] - 1s 228us/step - loss: 0.1312 - accuracy: 0.9430\n",
      "Epoch 75/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1300 - accuracy: 0.9490\n",
      "Epoch 76/250\n",
      "4372/4372 [==============================] - 1s 199us/step - loss: 0.1397 - accuracy: 0.9462\n",
      "Epoch 77/250\n",
      "4372/4372 [==============================] - 1s 203us/step - loss: 0.1319 - accuracy: 0.9499\n",
      "Epoch 78/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 1s 202us/step - loss: 0.1229 - accuracy: 0.9517\n",
      "Epoch 79/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1329 - accuracy: 0.9456\n",
      "Epoch 80/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1277 - accuracy: 0.9501\n",
      "Epoch 81/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1262 - accuracy: 0.9469\n",
      "Epoch 82/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1327 - accuracy: 0.9485\n",
      "Epoch 83/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1314 - accuracy: 0.9476\n",
      "Epoch 84/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1261 - accuracy: 0.9543\n",
      "Epoch 85/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1267 - accuracy: 0.9485\n",
      "Epoch 86/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1295 - accuracy: 0.9460\n",
      "Epoch 87/250\n",
      "4372/4372 [==============================] - 1s 224us/step - loss: 0.1309 - accuracy: 0.9488\n",
      "Epoch 88/250\n",
      "4372/4372 [==============================] - 1s 202us/step - loss: 0.1196 - accuracy: 0.9492\n",
      "Epoch 89/250\n",
      "4372/4372 [==============================] - 1s 201us/step - loss: 0.1169 - accuracy: 0.9506\n",
      "Epoch 90/250\n",
      "4372/4372 [==============================] - 1s 201us/step - loss: 0.1335 - accuracy: 0.9469\n",
      "Epoch 91/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1239 - accuracy: 0.9481\n",
      "Epoch 92/250\n",
      "4372/4372 [==============================] - 1s 212us/step - loss: 0.1188 - accuracy: 0.9490\n",
      "Epoch 93/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.1203 - accuracy: 0.9501\n",
      "Epoch 94/250\n",
      "4372/4372 [==============================] - 1s 219us/step - loss: 0.1305 - accuracy: 0.9488\n",
      "Epoch 95/250\n",
      "4372/4372 [==============================] - 1s 210us/step - loss: 0.1219 - accuracy: 0.9483\n",
      "Epoch 96/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1225 - accuracy: 0.9456\n",
      "Epoch 97/250\n",
      "4372/4372 [==============================] - 1s 233us/step - loss: 0.1218 - accuracy: 0.9508\n",
      "Epoch 98/250\n",
      "4372/4372 [==============================] - 1s 202us/step - loss: 0.1226 - accuracy: 0.9515\n",
      "Epoch 99/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1152 - accuracy: 0.9520\n",
      "Epoch 100/250\n",
      "4372/4372 [==============================] - 1s 214us/step - loss: 0.1205 - accuracy: 0.9488\n",
      "Epoch 101/250\n",
      "4372/4372 [==============================] - 1s 215us/step - loss: 0.1236 - accuracy: 0.9506\n",
      "Epoch 102/250\n",
      "4372/4372 [==============================] - 1s 214us/step - loss: 0.1147 - accuracy: 0.9501\n",
      "Epoch 103/250\n",
      "4372/4372 [==============================] - 1s 212us/step - loss: 0.1218 - accuracy: 0.9531\n",
      "Epoch 104/250\n",
      "4372/4372 [==============================] - 1s 214us/step - loss: 0.1189 - accuracy: 0.9533\n",
      "Epoch 105/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1281 - accuracy: 0.9490\n",
      "Epoch 106/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1068 - accuracy: 0.9556\n",
      "Epoch 107/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1282 - accuracy: 0.9469\n",
      "Epoch 108/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1229 - accuracy: 0.9504\n",
      "Epoch 109/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1253 - accuracy: 0.9492\n",
      "Epoch 110/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1070 - accuracy: 0.9549\n",
      "Epoch 111/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1260 - accuracy: 0.9506\n",
      "Epoch 112/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1156 - accuracy: 0.9531\n",
      "Epoch 113/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1171 - accuracy: 0.9515\n",
      "Epoch 114/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1126 - accuracy: 0.9563\n",
      "Epoch 115/250\n",
      "4372/4372 [==============================] - 1s 212us/step - loss: 0.1212 - accuracy: 0.9522\n",
      "Epoch 116/250\n",
      "4372/4372 [==============================] - 1s 214us/step - loss: 0.1189 - accuracy: 0.9543\n",
      "Epoch 117/250\n",
      "4372/4372 [==============================] - 1s 212us/step - loss: 0.1307 - accuracy: 0.9527\n",
      "Epoch 118/250\n",
      "4372/4372 [==============================] - 1s 218us/step - loss: 0.1103 - accuracy: 0.9547\n",
      "Epoch 119/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1128 - accuracy: 0.9527\n",
      "Epoch 120/250\n",
      "4372/4372 [==============================] - 1s 201us/step - loss: 0.1100 - accuracy: 0.9517\n",
      "Epoch 121/250\n",
      "4372/4372 [==============================] - 1s 223us/step - loss: 0.1186 - accuracy: 0.9490\n",
      "Epoch 122/250\n",
      "4372/4372 [==============================] - 1s 220us/step - loss: 0.1148 - accuracy: 0.9577\n",
      "Epoch 123/250\n",
      "4372/4372 [==============================] - 1s 201us/step - loss: 0.1143 - accuracy: 0.9508\n",
      "Epoch 124/250\n",
      "4372/4372 [==============================] - 1s 201us/step - loss: 0.1116 - accuracy: 0.9545\n",
      "Epoch 125/250\n",
      "4372/4372 [==============================] - 1s 202us/step - loss: 0.1149 - accuracy: 0.9540\n",
      "Epoch 126/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.1118 - accuracy: 0.9520\n",
      "Epoch 127/250\n",
      "4372/4372 [==============================] - 1s 203us/step - loss: 0.1282 - accuracy: 0.9499\n",
      "Epoch 128/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.1192 - accuracy: 0.9524\n",
      "Epoch 129/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1136 - accuracy: 0.9549\n",
      "Epoch 130/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1118 - accuracy: 0.9527\n",
      "Epoch 131/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1053 - accuracy: 0.9524\n",
      "Epoch 132/250\n",
      "4372/4372 [==============================] - 1s 210us/step - loss: 0.1143 - accuracy: 0.9570\n",
      "Epoch 133/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1091 - accuracy: 0.9547\n",
      "Epoch 134/250\n",
      "4372/4372 [==============================] - 1s 210us/step - loss: 0.1135 - accuracy: 0.9531\n",
      "Epoch 135/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1247 - accuracy: 0.9554\n",
      "Epoch 136/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1051 - accuracy: 0.9517\n",
      "Epoch 137/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1126 - accuracy: 0.9501\n",
      "Epoch 138/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1165 - accuracy: 0.9559\n",
      "Epoch 139/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1126 - accuracy: 0.9511\n",
      "Epoch 140/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1161 - accuracy: 0.9554\n",
      "Epoch 141/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1151 - accuracy: 0.9488\n",
      "Epoch 142/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.1057 - accuracy: 0.9540\n",
      "Epoch 143/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1045 - accuracy: 0.9577\n",
      "Epoch 144/250\n",
      "4372/4372 [==============================] - 1s 210us/step - loss: 0.1134 - accuracy: 0.9561\n",
      "Epoch 145/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1151 - accuracy: 0.9524\n",
      "Epoch 146/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1055 - accuracy: 0.9540\n",
      "Epoch 147/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1027 - accuracy: 0.9570\n",
      "Epoch 148/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1175 - accuracy: 0.9497\n",
      "Epoch 149/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1021 - accuracy: 0.9538\n",
      "Epoch 150/250\n",
      "4372/4372 [==============================] - 1s 210us/step - loss: 0.1066 - accuracy: 0.9549\n",
      "Epoch 151/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1001 - accuracy: 0.9588\n",
      "Epoch 152/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.0988 - accuracy: 0.9575\n",
      "Epoch 153/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1068 - accuracy: 0.9556\n",
      "Epoch 154/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1121 - accuracy: 0.9531\n",
      "Epoch 155/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1155 - accuracy: 0.9529\n",
      "Epoch 156/250\n",
      "4372/4372 [==============================] - 1s 210us/step - loss: 0.1105 - accuracy: 0.9536\n",
      "Epoch 157/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1080 - accuracy: 0.9565\n",
      "Epoch 158/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1022 - accuracy: 0.9604\n",
      "Epoch 159/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1167 - accuracy: 0.9540\n",
      "Epoch 160/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1082 - accuracy: 0.9556\n",
      "Epoch 161/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1057 - accuracy: 0.9547\n",
      "Epoch 162/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1118 - accuracy: 0.9529\n",
      "Epoch 163/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.0992 - accuracy: 0.9579\n",
      "Epoch 164/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1025 - accuracy: 0.9540\n",
      "Epoch 165/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1079 - accuracy: 0.9533\n",
      "Epoch 166/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1111 - accuracy: 0.9536\n",
      "Epoch 167/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1171 - accuracy: 0.9513\n",
      "Epoch 168/250\n",
      "4372/4372 [==============================] - 1s 210us/step - loss: 0.1065 - accuracy: 0.9545\n",
      "Epoch 169/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1047 - accuracy: 0.9565\n",
      "Epoch 170/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1199 - accuracy: 0.9524\n",
      "Epoch 171/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1098 - accuracy: 0.9524\n",
      "Epoch 172/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1088 - accuracy: 0.9531\n",
      "Epoch 173/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.0993 - accuracy: 0.9549\n",
      "Epoch 174/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1007 - accuracy: 0.9561\n",
      "Epoch 175/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1037 - accuracy: 0.9565\n",
      "Epoch 176/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1050 - accuracy: 0.9563\n",
      "Epoch 177/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1056 - accuracy: 0.9554\n",
      "Epoch 178/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1093 - accuracy: 0.9563\n",
      "Epoch 179/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1038 - accuracy: 0.9600\n",
      "Epoch 180/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1064 - accuracy: 0.9524\n",
      "Epoch 181/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1021 - accuracy: 0.9586\n",
      "Epoch 182/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.0994 - accuracy: 0.9554\n",
      "Epoch 183/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1032 - accuracy: 0.9533\n",
      "Epoch 184/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.0975 - accuracy: 0.9620\n",
      "Epoch 185/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1089 - accuracy: 0.9513\n",
      "Epoch 186/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1098 - accuracy: 0.9568\n",
      "Epoch 187/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1035 - accuracy: 0.9556\n",
      "Epoch 188/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.0979 - accuracy: 0.9593\n",
      "Epoch 189/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.1037 - accuracy: 0.9593\n",
      "Epoch 190/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1051 - accuracy: 0.9533\n",
      "Epoch 191/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1082 - accuracy: 0.9556\n",
      "Epoch 192/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.0977 - accuracy: 0.9609\n",
      "Epoch 193/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.0975 - accuracy: 0.9629\n",
      "Epoch 194/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.0991 - accuracy: 0.9565\n",
      "Epoch 195/250\n",
      "4372/4372 [==============================] - 1s 212us/step - loss: 0.1215 - accuracy: 0.9524\n",
      "Epoch 196/250\n",
      "4372/4372 [==============================] - 1s 220us/step - loss: 0.1108 - accuracy: 0.9515\n",
      "Epoch 197/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.1238 - accuracy: 0.9552\n",
      "Epoch 198/250\n",
      "4372/4372 [==============================] - 1s 199us/step - loss: 0.0977 - accuracy: 0.9568\n",
      "Epoch 199/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.0941 - accuracy: 0.9616\n",
      "Epoch 200/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.0946 - accuracy: 0.9616\n",
      "Epoch 201/250\n",
      "4372/4372 [==============================] - 1s 218us/step - loss: 0.1126 - accuracy: 0.9568\n",
      "Epoch 202/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.0923 - accuracy: 0.9613\n",
      "Epoch 203/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1051 - accuracy: 0.9533\n",
      "Epoch 204/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1007 - accuracy: 0.9563\n",
      "Epoch 205/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.1100 - accuracy: 0.9563\n",
      "Epoch 206/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.0980 - accuracy: 0.9561\n",
      "Epoch 207/250\n",
      "4372/4372 [==============================] - 1s 220us/step - loss: 0.0985 - accuracy: 0.9597\n",
      "Epoch 208/250\n",
      "4372/4372 [==============================] - 1s 212us/step - loss: 0.1067 - accuracy: 0.9570\n",
      "Epoch 209/250\n",
      "4372/4372 [==============================] - 1s 203us/step - loss: 0.0965 - accuracy: 0.9595\n",
      "Epoch 210/250\n",
      "4372/4372 [==============================] - 1s 203us/step - loss: 0.1044 - accuracy: 0.9559\n",
      "Epoch 211/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.0975 - accuracy: 0.9559\n",
      "Epoch 212/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1198 - accuracy: 0.9559\n",
      "Epoch 213/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1049 - accuracy: 0.9572\n",
      "Epoch 214/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1059 - accuracy: 0.9538\n",
      "Epoch 215/250\n",
      "4372/4372 [==============================] - 1s 215us/step - loss: 0.0947 - accuracy: 0.9588\n",
      "Epoch 216/250\n",
      "4372/4372 [==============================] - 1s 206us/step - loss: 0.1005 - accuracy: 0.9552\n",
      "Epoch 217/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.0982 - accuracy: 0.9570\n",
      "Epoch 218/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.0942 - accuracy: 0.9547\n",
      "Epoch 219/250\n",
      "4372/4372 [==============================] - 1s 205us/step - loss: 0.0950 - accuracy: 0.9586\n",
      "Epoch 220/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.1011 - accuracy: 0.9556\n",
      "Epoch 221/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1165 - accuracy: 0.9506\n",
      "Epoch 222/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.0950 - accuracy: 0.9575\n",
      "Epoch 223/250\n",
      "4372/4372 [==============================] - 1s 216us/step - loss: 0.0947 - accuracy: 0.9607\n",
      "Epoch 224/250\n",
      "4372/4372 [==============================] - 1s 203us/step - loss: 0.1118 - accuracy: 0.9511\n",
      "Epoch 225/250\n",
      "4372/4372 [==============================] - 1s 204us/step - loss: 0.0961 - accuracy: 0.9556\n",
      "Epoch 226/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.0954 - accuracy: 0.9604\n",
      "Epoch 227/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.0935 - accuracy: 0.9611\n",
      "Epoch 228/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.0966 - accuracy: 0.9600\n",
      "Epoch 229/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1152 - accuracy: 0.9508\n",
      "Epoch 230/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.0855 - accuracy: 0.9611\n",
      "Epoch 231/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.0958 - accuracy: 0.95680s - l\n",
      "Epoch 232/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.0983 - accuracy: 0.9597\n",
      "Epoch 233/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.0966 - accuracy: 0.9595\n",
      "Epoch 234/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.0960 - accuracy: 0.9563\n",
      "Epoch 235/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.0945 - accuracy: 0.9575\n",
      "Epoch 236/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.0876 - accuracy: 0.9602\n",
      "Epoch 237/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.1051 - accuracy: 0.9579\n",
      "Epoch 238/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.0997 - accuracy: 0.9563\n",
      "Epoch 239/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.0962 - accuracy: 0.9577\n",
      "Epoch 240/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.0951 - accuracy: 0.9597\n",
      "Epoch 241/250\n",
      "4372/4372 [==============================] - 1s 207us/step - loss: 0.0929 - accuracy: 0.9593\n",
      "Epoch 242/250\n",
      "4372/4372 [==============================] - 1s 209us/step - loss: 0.0855 - accuracy: 0.9636\n",
      "Epoch 243/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.0978 - accuracy: 0.9554\n",
      "Epoch 244/250\n",
      "4372/4372 [==============================] - 1s 210us/step - loss: 0.0959 - accuracy: 0.9554\n",
      "Epoch 245/250\n",
      "4372/4372 [==============================] - 1s 208us/step - loss: 0.1011 - accuracy: 0.9568\n",
      "Epoch 246/250\n",
      "4372/4372 [==============================] - 1s 214us/step - loss: 0.0906 - accuracy: 0.9600\n",
      "Epoch 247/250\n",
      "4372/4372 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.95 - 1s 219us/step - loss: 0.0929 - accuracy: 0.9579\n",
      "Epoch 248/250\n",
      "4372/4372 [==============================] - 1s 215us/step - loss: 0.0964 - accuracy: 0.9579\n",
      "Epoch 249/250\n",
      "4372/4372 [==============================] - 1s 211us/step - loss: 0.1003 - accuracy: 0.9540\n",
      "Epoch 250/250\n",
      "4372/4372 [==============================] - 1s 213us/step - loss: 0.0895 - accuracy: 0.9611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2575127b648>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout\n",
    "classifier = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier.add(Dense(200, activation='relu', kernel_initializer='random_normal', input_dim=1600, ))\n",
    "classifier.add(Dropout(0.5))\n",
    "#Second  Hidden Layer\n",
    "classifier.add(Dense(40, activation='relu', kernel_initializer='random_normal'))\n",
    "classifier.add(Dropout(0.5))\n",
    "#Third  Hidden Layer\n",
    "classifier.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n",
    "classifier.add(Dropout(0.5))\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "#Compiling the neural network\n",
    "classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "#Fitting the data to the training dataset\n",
    "classifier.fit(X_train,y_train, batch_size=10, epochs=250, shuffle=True, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               320200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                8040      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 328       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 328,577\n",
      "Trainable params: 328,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 0s 36us/step\n"
     ]
    }
   ],
   "source": [
    "eval_model=classifier.evaluate(X_train, y_train)\n",
    "eval_model\n",
    "y_pred=classifier.predict(X_test)\n",
    "y_pred =(y_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[827 147]\n",
      " [153 747]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3123\n"
     ]
    }
   ],
   "source": [
    "galaxy = np.zeros((40,40))\n",
    "for filepath in glob.iglob('../Data/cutouts/galaxyfits/*fits', recursive=True):\n",
    "    fp = Path(filepath)\n",
    "    hdulist = fits.open(fp)\n",
    "    scidata = hdulist[0].data\n",
    "    galaxy = np.dstack((galaxy,scidata))\n",
    "galaxy = galaxy[:,:,1:]\n",
    "print(galaxy.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 40, 3123)\n"
     ]
    }
   ],
   "source": [
    "star = np.zeros((40,40))\n",
    "count = 0\n",
    "for filepath in glob.iglob('../Data/cutouts/starfits/*fits', recursive=True):\n",
    "    fp = Path(filepath)\n",
    "    hdulist = fits.open(fp)\n",
    "    scidata = hdulist[0].data\n",
    "    scidata = scidata\n",
    "    star = np.dstack((star,scidata))\n",
    "    count += 1\n",
    "    if count == galaxy.shape[2]:\n",
    "        break\n",
    "star = star[:,:,1:]\n",
    "print(star.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 40, 6246)\n"
     ]
    }
   ],
   "source": [
    "X = np.dstack((galaxy,star))\n",
    "y = np.dstack((np.ones((1,1,galaxy.shape[2])),np.zeros((1,1,galaxy.shape[2]))))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.moveaxis(X, -1, 0)\n",
    "y = np.moveaxis(y, -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6246, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(len(X_train),40,40,1)\n",
    "X_test = X_test.reshape(len(X_test),40,40,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "4372/4372 [==============================] - 3s 577us/step - loss: 19.9325 - accuracy: 0.7113\n",
      "Epoch 2/250\n",
      "4372/4372 [==============================] - 1s 279us/step - loss: 0.7231 - accuracy: 0.7953\n",
      "Epoch 3/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.5890 - accuracy: 0.8131\n",
      "Epoch 4/250\n",
      "4372/4372 [==============================] - 1s 279us/step - loss: 0.4503 - accuracy: 0.8367\n",
      "Epoch 5/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.4925 - accuracy: 0.8431\n",
      "Epoch 6/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.4903 - accuracy: 0.8184\n",
      "Epoch 7/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.4615 - accuracy: 0.8422\n",
      "Epoch 8/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.3579 - accuracy: 0.8662\n",
      "Epoch 9/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.3696 - accuracy: 0.8733\n",
      "Epoch 10/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.3813 - accuracy: 0.8742\n",
      "Epoch 11/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.3312 - accuracy: 0.8795\n",
      "Epoch 12/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.3345 - accuracy: 0.8769\n",
      "Epoch 13/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.3327 - accuracy: 0.8925\n",
      "Epoch 14/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.3023 - accuracy: 0.8975\n",
      "Epoch 15/250\n",
      "4372/4372 [==============================] - 1s 281us/step - loss: 0.3586 - accuracy: 0.8664\n",
      "Epoch 16/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.3318 - accuracy: 0.8948\n",
      "Epoch 17/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2555 - accuracy: 0.9062\n",
      "Epoch 18/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.2844 - accuracy: 0.9065\n",
      "Epoch 19/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.2904 - accuracy: 0.8939\n",
      "Epoch 20/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.2458 - accuracy: 0.9097\n",
      "Epoch 21/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.3797 - accuracy: 0.8575\n",
      "Epoch 22/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.2690 - accuracy: 0.9074\n",
      "Epoch 23/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.2551 - accuracy: 0.9133\n",
      "Epoch 24/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.2755 - accuracy: 0.8996\n",
      "Epoch 25/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2117 - accuracy: 0.9181\n",
      "Epoch 26/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2646 - accuracy: 0.9138\n",
      "Epoch 27/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.2952 - accuracy: 0.9106\n",
      "Epoch 28/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.2824 - accuracy: 0.9000\n",
      "Epoch 29/250\n",
      "4372/4372 [==============================] - 1s 279us/step - loss: 0.2237 - accuracy: 0.9174\n",
      "Epoch 30/250\n",
      "4372/4372 [==============================] - 1s 267us/step - loss: 0.3275 - accuracy: 0.8900\n",
      "Epoch 31/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.3242 - accuracy: 0.8687\n",
      "Epoch 32/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.2597 - accuracy: 0.8930\n",
      "Epoch 33/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.2138 - accuracy: 0.9119\n",
      "Epoch 34/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2226 - accuracy: 0.9243\n",
      "Epoch 35/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.1971 - accuracy: 0.9193\n",
      "Epoch 36/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.1792 - accuracy: 0.9312\n",
      "Epoch 37/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1633 - accuracy: 0.9293\n",
      "Epoch 38/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.2328 - accuracy: 0.9252\n",
      "Epoch 39/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.2119 - accuracy: 0.9135\n",
      "Epoch 40/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.4501 - accuracy: 0.8564\n",
      "Epoch 41/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.3067 - accuracy: 0.8891\n",
      "Epoch 42/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.2289 - accuracy: 0.9147\n",
      "Epoch 43/250\n",
      "4372/4372 [==============================] - 1s 284us/step - loss: 0.2657 - accuracy: 0.9149\n",
      "Epoch 44/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2018 - accuracy: 0.9193\n",
      "Epoch 45/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2007 - accuracy: 0.9225\n",
      "Epoch 46/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.2183 - accuracy: 0.9140\n",
      "Epoch 47/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.1566 - accuracy: 0.9440\n",
      "Epoch 48/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1520 - accuracy: 0.9401\n",
      "Epoch 49/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1820 - accuracy: 0.9332\n",
      "Epoch 50/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.2413 - accuracy: 0.9252\n",
      "Epoch 51/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.1958 - accuracy: 0.9277\n",
      "Epoch 52/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2660 - accuracy: 0.9051\n",
      "Epoch 53/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.3979 - accuracy: 0.9007\n",
      "Epoch 54/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2316 - accuracy: 0.9261\n",
      "Epoch 55/250\n",
      "4372/4372 [==============================] - 1s 279us/step - loss: 0.2000 - accuracy: 0.9321\n",
      "Epoch 56/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.1562 - accuracy: 0.9405\n",
      "Epoch 57/250\n",
      "4372/4372 [==============================] - 1s 269us/step - loss: 0.1355 - accuracy: 0.9481\n",
      "Epoch 58/250\n",
      "4372/4372 [==============================] - 1s 269us/step - loss: 0.1595 - accuracy: 0.9440\n",
      "Epoch 59/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.1903 - accuracy: 0.9366\n",
      "Epoch 60/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.1719 - accuracy: 0.9371\n",
      "Epoch 61/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1565 - accuracy: 0.9444\n",
      "Epoch 62/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1317 - accuracy: 0.9499\n",
      "Epoch 63/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1617 - accuracy: 0.9488\n",
      "Epoch 64/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.1881 - accuracy: 0.9355\n",
      "Epoch 65/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.1432 - accuracy: 0.9474\n",
      "Epoch 66/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.1400 - accuracy: 0.9495\n",
      "Epoch 67/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.3410 - accuracy: 0.9044\n",
      "Epoch 68/250\n",
      "4372/4372 [==============================] - 1s 280us/step - loss: 0.2674 - accuracy: 0.9087\n",
      "Epoch 69/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.1811 - accuracy: 0.9428\n",
      "Epoch 70/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.6171 - accuracy: 0.9220\n",
      "Epoch 71/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.3559 - accuracy: 0.9044\n",
      "Epoch 72/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2772 - accuracy: 0.9209\n",
      "Epoch 73/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2297 - accuracy: 0.9353\n",
      "Epoch 74/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.1708 - accuracy: 0.9376\n",
      "Epoch 75/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1599 - accuracy: 0.9396\n",
      "Epoch 76/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1475 - accuracy: 0.9460\n",
      "Epoch 77/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.1781 - accuracy: 0.9419\n",
      "Epoch 78/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1793 - accuracy: 0.9433\n",
      "Epoch 79/250\n",
      "4372/4372 [==============================] - 1s 271us/step - loss: 0.1181 - accuracy: 0.9545\n",
      "Epoch 80/250\n",
      "4372/4372 [==============================] - 1s 288us/step - loss: 0.1045 - accuracy: 0.9563\n",
      "Epoch 81/250\n",
      "4372/4372 [==============================] - 1s 292us/step - loss: 0.1057 - accuracy: 0.9536\n",
      "Epoch 82/250\n",
      "4372/4372 [==============================] - 1s 291us/step - loss: 0.1135 - accuracy: 0.9492\n",
      "Epoch 83/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.1711 - accuracy: 0.9245\n",
      "Epoch 84/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1809 - accuracy: 0.9360\n",
      "Epoch 85/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.1200 - accuracy: 0.9547\n",
      "Epoch 86/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1695 - accuracy: 0.9330\n",
      "Epoch 87/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1188 - accuracy: 0.9536\n",
      "Epoch 88/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1683 - accuracy: 0.9572\n",
      "Epoch 89/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.3729 - accuracy: 0.9058\n",
      "Epoch 90/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.2037 - accuracy: 0.9309\n",
      "Epoch 91/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.3310 - accuracy: 0.9177\n",
      "Epoch 92/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.2486 - accuracy: 0.9282\n",
      "Epoch 93/250\n",
      "4372/4372 [==============================] - 1s 269us/step - loss: 0.1358 - accuracy: 0.9442\n",
      "Epoch 94/250\n",
      "4372/4372 [==============================] - 1s 279us/step - loss: 0.1163 - accuracy: 0.9492\n",
      "Epoch 95/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1278 - accuracy: 0.9517\n",
      "Epoch 96/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.1056 - accuracy: 0.9536\n",
      "Epoch 97/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1247 - accuracy: 0.9561\n",
      "Epoch 98/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.1321 - accuracy: 0.9506\n",
      "Epoch 99/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.9018 - accuracy: 0.8909\n",
      "Epoch 100/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.7065 - accuracy: 0.8712\n",
      "Epoch 101/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.3532 - accuracy: 0.8952\n",
      "Epoch 102/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.2020 - accuracy: 0.9177\n",
      "Epoch 103/250\n",
      "4372/4372 [==============================] - 1s 280us/step - loss: 0.2311 - accuracy: 0.9257\n",
      "Epoch 104/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.1596 - accuracy: 0.9314\n",
      "Epoch 105/250\n",
      "4372/4372 [==============================] - 1s 271us/step - loss: 0.1626 - accuracy: 0.9401\n",
      "Epoch 106/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.1653 - accuracy: 0.9469\n",
      "Epoch 107/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.1584 - accuracy: 0.9458\n",
      "Epoch 108/250\n",
      "4372/4372 [==============================] - 1s 283us/step - loss: 0.1734 - accuracy: 0.9403\n",
      "Epoch 109/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.3773 - accuracy: 0.9065\n",
      "Epoch 110/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.2254 - accuracy: 0.9186\n",
      "Epoch 111/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2894 - accuracy: 0.9140\n",
      "Epoch 112/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.1656 - accuracy: 0.9245\n",
      "Epoch 113/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1345 - accuracy: 0.9430\n",
      "Epoch 114/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1197 - accuracy: 0.9543\n",
      "Epoch 115/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.1237 - accuracy: 0.9515\n",
      "Epoch 116/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1813 - accuracy: 0.9362\n",
      "Epoch 117/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.2174 - accuracy: 0.9328\n",
      "Epoch 118/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1564 - accuracy: 0.9421\n",
      "Epoch 119/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1902 - accuracy: 0.9392\n",
      "Epoch 120/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1806 - accuracy: 0.9339\n",
      "Epoch 121/250\n",
      "4372/4372 [==============================] - 1s 281us/step - loss: 0.1423 - accuracy: 0.9467\n",
      "Epoch 122/250\n",
      "4372/4372 [==============================] - 1s 271us/step - loss: 0.1670 - accuracy: 0.9353\n",
      "Epoch 123/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.1410 - accuracy: 0.9536\n",
      "Epoch 124/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1596 - accuracy: 0.9376\n",
      "Epoch 125/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.5405 - accuracy: 0.8989\n",
      "Epoch 126/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.7612 - accuracy: 0.8216\n",
      "Epoch 127/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.3538 - accuracy: 0.8856\n",
      "Epoch 128/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.2526 - accuracy: 0.9213\n",
      "Epoch 129/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1750 - accuracy: 0.9309\n",
      "Epoch 130/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2403 - accuracy: 0.9302\n",
      "Epoch 131/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.1251 - accuracy: 0.9460\n",
      "Epoch 132/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.1136 - accuracy: 0.9527\n",
      "Epoch 133/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1279 - accuracy: 0.9478\n",
      "Epoch 134/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1167 - accuracy: 0.9501\n",
      "Epoch 135/250\n",
      "4372/4372 [==============================] - 1s 279us/step - loss: 0.1376 - accuracy: 0.9529\n",
      "Epoch 136/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2479 - accuracy: 0.9211\n",
      "Epoch 137/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.1609 - accuracy: 0.9435\n",
      "Epoch 138/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1273 - accuracy: 0.9499\n",
      "Epoch 139/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1015 - accuracy: 0.9602\n",
      "Epoch 140/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.4017 - accuracy: 0.9428\n",
      "Epoch 141/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.2304 - accuracy: 0.9147\n",
      "Epoch 142/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.1996 - accuracy: 0.9508\n",
      "Epoch 143/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1740 - accuracy: 0.9403\n",
      "Epoch 144/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1735 - accuracy: 0.9446\n",
      "Epoch 145/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1428 - accuracy: 0.9382\n",
      "Epoch 146/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1456 - accuracy: 0.9444\n",
      "Epoch 147/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1085 - accuracy: 0.9559\n",
      "Epoch 148/250\n",
      "4372/4372 [==============================] - 1s 279us/step - loss: 0.3470 - accuracy: 0.9007\n",
      "Epoch 149/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.3416 - accuracy: 0.9081\n",
      "Epoch 150/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.1562 - accuracy: 0.9446\n",
      "Epoch 151/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.1294 - accuracy: 0.9458\n",
      "Epoch 152/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.1158 - accuracy: 0.9547\n",
      "Epoch 153/250\n",
      "4372/4372 [==============================] - 1s 269us/step - loss: 0.0995 - accuracy: 0.9549\n",
      "Epoch 154/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.4307 - accuracy: 0.9149\n",
      "Epoch 155/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2061 - accuracy: 0.9298\n",
      "Epoch 156/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.1402 - accuracy: 0.9456\n",
      "Epoch 157/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.1150 - accuracy: 0.9602\n",
      "Epoch 158/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1183 - accuracy: 0.9485\n",
      "Epoch 159/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1656 - accuracy: 0.9325\n",
      "Epoch 160/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.1032 - accuracy: 0.9540\n",
      "Epoch 161/250\n",
      "4372/4372 [==============================] - 1s 279us/step - loss: 0.3766 - accuracy: 0.9147\n",
      "Epoch 162/250\n",
      "4372/4372 [==============================] - 1s 271us/step - loss: 0.1660 - accuracy: 0.9419\n",
      "Epoch 163/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1097 - accuracy: 0.9552\n",
      "Epoch 164/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.1450 - accuracy: 0.9435\n",
      "Epoch 165/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1040 - accuracy: 0.9545\n",
      "Epoch 166/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.0838 - accuracy: 0.9632\n",
      "Epoch 167/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1104 - accuracy: 0.9478\n",
      "Epoch 168/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1710 - accuracy: 0.9371\n",
      "Epoch 169/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1438 - accuracy: 0.9330\n",
      "Epoch 170/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.1018 - accuracy: 0.9531\n",
      "Epoch 171/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.1771 - accuracy: 0.9426\n",
      "Epoch 172/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.1899 - accuracy: 0.9252\n",
      "Epoch 173/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 1.0346 - accuracy: 0.8817\n",
      "Epoch 174/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.1969 - accuracy: 0.9238\n",
      "Epoch 175/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1428 - accuracy: 0.9398\n",
      "Epoch 176/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.8190 - accuracy: 0.8920\n",
      "Epoch 177/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.3929 - accuracy: 0.8973\n",
      "Epoch 178/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.2245 - accuracy: 0.9202\n",
      "Epoch 179/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.1718 - accuracy: 0.9339\n",
      "Epoch 180/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.1994 - accuracy: 0.9337\n",
      "Epoch 181/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.2354 - accuracy: 0.9437\n",
      "Epoch 182/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.1486 - accuracy: 0.9478\n",
      "Epoch 183/250\n",
      "4372/4372 [==============================] - 1s 269us/step - loss: 0.2014 - accuracy: 0.9300\n",
      "Epoch 184/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.1214 - accuracy: 0.9419\n",
      "Epoch 185/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.4387 - accuracy: 0.9083\n",
      "Epoch 186/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.2999 - accuracy: 0.9131\n",
      "Epoch 187/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1946 - accuracy: 0.9247\n",
      "Epoch 188/250\n",
      "4372/4372 [==============================] - 1s 279us/step - loss: 0.1411 - accuracy: 0.9401\n",
      "Epoch 189/250\n",
      "4372/4372 [==============================] - 1s 268us/step - loss: 0.1438 - accuracy: 0.9428\n",
      "Epoch 190/250\n",
      "4372/4372 [==============================] - 1s 271us/step - loss: 0.2528 - accuracy: 0.9302\n",
      "Epoch 191/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.2085 - accuracy: 0.9286\n",
      "Epoch 192/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.1325 - accuracy: 0.9465\n",
      "Epoch 193/250\n",
      "4372/4372 [==============================] - 1s 266us/step - loss: 0.1245 - accuracy: 0.9497\n",
      "Epoch 194/250\n",
      "4372/4372 [==============================] - 1s 266us/step - loss: 0.1227 - accuracy: 0.9529\n",
      "Epoch 195/250\n",
      "4372/4372 [==============================] - 1s 266us/step - loss: 0.1144 - accuracy: 0.9508\n",
      "Epoch 196/250\n",
      "4372/4372 [==============================] - 1s 268us/step - loss: 0.1829 - accuracy: 0.9360\n",
      "Epoch 197/250\n",
      "4372/4372 [==============================] - 1s 268us/step - loss: 0.1550 - accuracy: 0.9410\n",
      "Epoch 198/250\n",
      "4372/4372 [==============================] - 1s 268us/step - loss: 0.1153 - accuracy: 0.9490\n",
      "Epoch 199/250\n",
      "4372/4372 [==============================] - 1s 268us/step - loss: 0.1356 - accuracy: 0.9456\n",
      "Epoch 200/250\n",
      "4372/4372 [==============================] - 1s 271us/step - loss: 0.1082 - accuracy: 0.9559\n",
      "Epoch 201/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.3731 - accuracy: 0.9085\n",
      "Epoch 202/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.7126 - accuracy: 0.8339\n",
      "Epoch 203/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.4341 - accuracy: 0.8955\n",
      "Epoch 204/250\n",
      "4372/4372 [==============================] - 1s 279us/step - loss: 0.2345 - accuracy: 0.9172\n",
      "Epoch 205/250\n",
      "4372/4372 [==============================] - 1s 292us/step - loss: 0.2127 - accuracy: 0.9316\n",
      "Epoch 206/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.1641 - accuracy: 0.9396\n",
      "Epoch 207/250\n",
      "4372/4372 [==============================] - 1s 271us/step - loss: 0.1607 - accuracy: 0.9396\n",
      "Epoch 208/250\n",
      "4372/4372 [==============================] - 1s 269us/step - loss: 0.1688 - accuracy: 0.9357\n",
      "Epoch 209/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.6291 - accuracy: 0.8991\n",
      "Epoch 210/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.4816 - accuracy: 0.9282\n",
      "Epoch 211/250\n",
      "4372/4372 [==============================] - 1s 275us/step - loss: 0.1879 - accuracy: 0.9419\n",
      "Epoch 212/250\n",
      "4372/4372 [==============================] - 1s 284us/step - loss: 0.1563 - accuracy: 0.9405\n",
      "Epoch 213/250\n",
      "4372/4372 [==============================] - 1s 285us/step - loss: 0.1408 - accuracy: 0.9529\n",
      "Epoch 214/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.1278 - accuracy: 0.9570\n",
      "Epoch 215/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.1110 - accuracy: 0.9495\n",
      "Epoch 216/250\n",
      "4372/4372 [==============================] - 1s 268us/step - loss: 0.3229 - accuracy: 0.9161\n",
      "Epoch 217/250\n",
      "4372/4372 [==============================] - 1s 271us/step - loss: 0.2139 - accuracy: 0.9236\n",
      "Epoch 218/250\n",
      "4372/4372 [==============================] - 1s 281us/step - loss: 0.1206 - accuracy: 0.9506\n",
      "Epoch 219/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1842 - accuracy: 0.9412\n",
      "Epoch 220/250\n",
      "4372/4372 [==============================] - 1s 261us/step - loss: 0.1443 - accuracy: 0.9437\n",
      "Epoch 221/250\n",
      "4372/4372 [==============================] - 1s 266us/step - loss: 0.1030 - accuracy: 0.9556\n",
      "Epoch 222/250\n",
      "4372/4372 [==============================] - 1s 271us/step - loss: 0.1241 - accuracy: 0.9501\n",
      "Epoch 223/250\n",
      "4372/4372 [==============================] - 1s 269us/step - loss: 0.1231 - accuracy: 0.9497\n",
      "Epoch 224/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.0914 - accuracy: 0.9623\n",
      "Epoch 225/250\n",
      "4372/4372 [==============================] - 1s 274us/step - loss: 0.1678 - accuracy: 0.9442\n",
      "Epoch 226/250\n",
      "4372/4372 [==============================] - 1s 277us/step - loss: 0.5185 - accuracy: 0.8740\n",
      "Epoch 227/250\n",
      "4372/4372 [==============================] - 1s 284us/step - loss: 0.3846 - accuracy: 0.8870\n",
      "Epoch 228/250\n",
      "4372/4372 [==============================] - 1s 299us/step - loss: 0.1793 - accuracy: 0.9209\n",
      "Epoch 229/250\n",
      "4372/4372 [==============================] - 1s 284us/step - loss: 0.1892 - accuracy: 0.9293\n",
      "Epoch 230/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 1s 287us/step - loss: 0.1868 - accuracy: 0.9355\n",
      "Epoch 231/250\n",
      "4372/4372 [==============================] - 1s 279us/step - loss: 0.2963 - accuracy: 0.9316\n",
      "Epoch 232/250\n",
      "4372/4372 [==============================] - 1s 262us/step - loss: 0.2930 - accuracy: 0.9353\n",
      "Epoch 233/250\n",
      "4372/4372 [==============================] - 1s 273us/step - loss: 0.3660 - accuracy: 0.9010\n",
      "Epoch 234/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.1672 - accuracy: 0.9314\n",
      "Epoch 235/250\n",
      "4372/4372 [==============================] - 1s 262us/step - loss: 0.1492 - accuracy: 0.9401\n",
      "Epoch 236/250\n",
      "4372/4372 [==============================] - 1s 260us/step - loss: 0.1308 - accuracy: 0.9497\n",
      "Epoch 237/250\n",
      "4372/4372 [==============================] - 1s 270us/step - loss: 0.1646 - accuracy: 0.9360\n",
      "Epoch 238/250\n",
      "4372/4372 [==============================] - 1s 263us/step - loss: 0.9954 - accuracy: 0.8975\n",
      "Epoch 239/250\n",
      "4372/4372 [==============================] - 1s 262us/step - loss: 0.2158 - accuracy: 0.9275\n",
      "Epoch 240/250\n",
      "4372/4372 [==============================] - 1s 271us/step - loss: 0.1434 - accuracy: 0.9378\n",
      "Epoch 241/250\n",
      "4372/4372 [==============================] - 1s 272us/step - loss: 0.1744 - accuracy: 0.9401\n",
      "Epoch 242/250\n",
      "4372/4372 [==============================] - 1s 278us/step - loss: 0.2494 - accuracy: 0.9252\n",
      "Epoch 243/250\n",
      "4372/4372 [==============================] - 1s 287us/step - loss: 0.1370 - accuracy: 0.9456\n",
      "Epoch 244/250\n",
      "4372/4372 [==============================] - 1s 263us/step - loss: 0.2009 - accuracy: 0.9122\n",
      "Epoch 245/250\n",
      "4372/4372 [==============================] - 1s 264us/step - loss: 0.1891 - accuracy: 0.9300\n",
      "Epoch 246/250\n",
      "4372/4372 [==============================] - 1s 269us/step - loss: 0.1583 - accuracy: 0.93460s - loss: 0.1670 - accura\n",
      "Epoch 247/250\n",
      "4372/4372 [==============================] - 1s 268us/step - loss: 0.3024 - accuracy: 0.9181\n",
      "Epoch 248/250\n",
      "4372/4372 [==============================] - 1s 266us/step - loss: 0.1912 - accuracy: 0.9215\n",
      "Epoch 249/250\n",
      "4372/4372 [==============================] - 1s 276us/step - loss: 0.2144 - accuracy: 0.9362\n",
      "Epoch 250/250\n",
      "4372/4372 [==============================] - 1s 284us/step - loss: 0.3337 - accuracy: 0.9302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25764752888>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(16, kernel_size=5, activation='relu', input_shape=(40,40,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='random_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#Compiling the neural network\n",
    "model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "#Fitting the data to the training dataset\n",
    "model.fit(X_train,y_train, batch_size=32, epochs=256, shuffle=True, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 36, 36, 16)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 18, 18, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 97,537\n",
      "Trainable params: 97,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4372/4372 [==============================] - 0s 70us/step\n"
     ]
    }
   ],
   "source": [
    "eval_model=model.evaluate(X_train, y_train)\n",
    "eval_model\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred =(y_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[762 192]\n",
      " [ 80 840]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Train for 110 steps, validate for 28 steps\n",
      "Epoch 1/1000\n",
      "110/110 [==============================] - ETA: 40s - loss: 0.6344 - accuracy: 0.593 - ETA: 2s - loss: 0.6272 - accuracy: 0.602 - ETA: 1s - loss: 0.5930 - accuracy: 0.65 - ETA: 0s - loss: 0.5756 - accuracy: 0.67 - ETA: 0s - loss: 0.5818 - accuracy: 0.67 - ETA: 0s - loss: 0.5773 - accuracy: 0.67 - ETA: 0s - loss: 0.5739 - accuracy: 0.68 - ETA: 0s - loss: 0.5703 - accuracy: 0.68 - 1s 9ms/step - loss: 0.5689 - accuracy: 0.6852 - val_loss: 0.5590 - val_accuracy: 0.6808\n",
      "Epoch 2/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.5041 - accuracy: 0.68 - ETA: 0s - loss: 0.5524 - accuracy: 0.67 - ETA: 0s - loss: 0.5313 - accuracy: 0.70 - ETA: 0s - loss: 0.5304 - accuracy: 0.71 - ETA: 0s - loss: 0.5320 - accuracy: 0.71 - ETA: 0s - loss: 0.5258 - accuracy: 0.72 - ETA: 0s - loss: 0.5251 - accuracy: 0.72 - ETA: 0s - loss: 0.5206 - accuracy: 0.73 - 1s 5ms/step - loss: 0.5197 - accuracy: 0.7341 - val_loss: 0.5156 - val_accuracy: 0.7265\n",
      "Epoch 3/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.4683 - accuracy: 0.68 - ETA: 0s - loss: 0.4969 - accuracy: 0.75 - ETA: 0s - loss: 0.4616 - accuracy: 0.77 - ETA: 0s - loss: 0.4583 - accuracy: 0.77 - ETA: 0s - loss: 0.4584 - accuracy: 0.77 - ETA: 0s - loss: 0.4592 - accuracy: 0.78 - ETA: 0s - loss: 0.4570 - accuracy: 0.78 - ETA: 0s - loss: 0.4545 - accuracy: 0.78 - 1s 5ms/step - loss: 0.4539 - accuracy: 0.7870 - val_loss: 0.4696 - val_accuracy: 0.7643\n",
      "Epoch 4/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.4066 - accuracy: 0.78 - ETA: 0s - loss: 0.4283 - accuracy: 0.81 - ETA: 0s - loss: 0.4066 - accuracy: 0.82 - ETA: 0s - loss: 0.4057 - accuracy: 0.82 - ETA: 0s - loss: 0.4120 - accuracy: 0.82 - ETA: 0s - loss: 0.4076 - accuracy: 0.82 - ETA: 0s - loss: 0.4088 - accuracy: 0.82 - ETA: 0s - loss: 0.4066 - accuracy: 0.82 - 1s 5ms/step - loss: 0.4068 - accuracy: 0.8293 - val_loss: 0.4260 - val_accuracy: 0.8181\n",
      "Epoch 5/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.4728 - accuracy: 0.81 - ETA: 0s - loss: 0.4141 - accuracy: 0.83 - ETA: 0s - loss: 0.3788 - accuracy: 0.85 - ETA: 0s - loss: 0.3757 - accuracy: 0.85 - ETA: 0s - loss: 0.3801 - accuracy: 0.84 - ETA: 0s - loss: 0.3833 - accuracy: 0.84 - ETA: 0s - loss: 0.3909 - accuracy: 0.84 - ETA: 0s - loss: 0.3868 - accuracy: 0.84 - 1s 5ms/step - loss: 0.3888 - accuracy: 0.8448 - val_loss: 0.3842 - val_accuracy: 0.8444\n",
      "Epoch 6/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3512 - accuracy: 0.90 - ETA: 0s - loss: 0.3637 - accuracy: 0.86 - ETA: 0s - loss: 0.3491 - accuracy: 0.86 - ETA: 0s - loss: 0.3503 - accuracy: 0.86 - ETA: 0s - loss: 0.3573 - accuracy: 0.86 - ETA: 0s - loss: 0.3631 - accuracy: 0.85 - ETA: 0s - loss: 0.3641 - accuracy: 0.85 - ETA: 0s - loss: 0.3640 - accuracy: 0.85 - 1s 5ms/step - loss: 0.3654 - accuracy: 0.8573 - val_loss: 0.3750 - val_accuracy: 0.8478\n",
      "Epoch 7/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3078 - accuracy: 0.93 - ETA: 0s - loss: 0.3373 - accuracy: 0.88 - ETA: 0s - loss: 0.3231 - accuracy: 0.88 - ETA: 0s - loss: 0.3260 - accuracy: 0.88 - ETA: 0s - loss: 0.3336 - accuracy: 0.87 - ETA: 0s - loss: 0.3310 - accuracy: 0.87 - ETA: 0s - loss: 0.3304 - accuracy: 0.87 - ETA: 0s - loss: 0.3303 - accuracy: 0.87 - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8728 - val_loss: 0.3476 - val_accuracy: 0.8764\n",
      "Epoch 8/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3563 - accuracy: 0.87 - ETA: 0s - loss: 0.3514 - accuracy: 0.88 - ETA: 0s - loss: 0.3217 - accuracy: 0.88 - ETA: 0s - loss: 0.3250 - accuracy: 0.88 - ETA: 0s - loss: 0.3309 - accuracy: 0.87 - ETA: 0s - loss: 0.3257 - accuracy: 0.87 - ETA: 0s - loss: 0.3272 - accuracy: 0.87 - ETA: 0s - loss: 0.3245 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8739 - val_loss: 0.3455 - val_accuracy: 0.8707\n",
      "Epoch 9/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3308 - accuracy: 0.87 - ETA: 0s - loss: 0.3342 - accuracy: 0.88 - ETA: 0s - loss: 0.3062 - accuracy: 0.89 - ETA: 0s - loss: 0.3063 - accuracy: 0.89 - ETA: 0s - loss: 0.3077 - accuracy: 0.89 - ETA: 0s - loss: 0.3074 - accuracy: 0.88 - ETA: 0s - loss: 0.3076 - accuracy: 0.88 - ETA: 0s - loss: 0.3097 - accuracy: 0.88 - 1s 5ms/step - loss: 0.3127 - accuracy: 0.8828 - val_loss: 0.3259 - val_accuracy: 0.8879\n",
      "Epoch 10/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2367 - accuracy: 0.93 - ETA: 0s - loss: 0.3154 - accuracy: 0.88 - ETA: 0s - loss: 0.3071 - accuracy: 0.88 - ETA: 0s - loss: 0.3060 - accuracy: 0.89 - ETA: 0s - loss: 0.3042 - accuracy: 0.89 - ETA: 0s - loss: 0.3047 - accuracy: 0.88 - ETA: 0s - loss: 0.3064 - accuracy: 0.88 - ETA: 0s - loss: 0.3077 - accuracy: 0.88 - 1s 5ms/step - loss: 0.3097 - accuracy: 0.8842 - val_loss: 0.3236 - val_accuracy: 0.8947\n",
      "Epoch 11/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3152 - accuracy: 0.90 - ETA: 0s - loss: 0.3167 - accuracy: 0.88 - ETA: 0s - loss: 0.2984 - accuracy: 0.89 - ETA: 0s - loss: 0.3018 - accuracy: 0.88 - ETA: 0s - loss: 0.3065 - accuracy: 0.88 - ETA: 0s - loss: 0.3009 - accuracy: 0.88 - ETA: 0s - loss: 0.3006 - accuracy: 0.88 - ETA: 0s - loss: 0.3000 - accuracy: 0.88 - 1s 5ms/step - loss: 0.3025 - accuracy: 0.8862 - val_loss: 0.3100 - val_accuracy: 0.8993\n",
      "Epoch 12/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2634 - accuracy: 0.93 - ETA: 0s - loss: 0.2903 - accuracy: 0.90 - ETA: 0s - loss: 0.2878 - accuracy: 0.90 - ETA: 0s - loss: 0.2901 - accuracy: 0.90 - ETA: 0s - loss: 0.2891 - accuracy: 0.90 - ETA: 0s - loss: 0.2886 - accuracy: 0.90 - ETA: 0s - loss: 0.2911 - accuracy: 0.90 - ETA: 0s - loss: 0.2952 - accuracy: 0.89 - 1s 5ms/step - loss: 0.2968 - accuracy: 0.8971 - val_loss: 0.3037 - val_accuracy: 0.8993\n",
      "Epoch 13/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2145 - accuracy: 0.90 - ETA: 0s - loss: 0.2891 - accuracy: 0.89 - ETA: 0s - loss: 0.2816 - accuracy: 0.89 - ETA: 0s - loss: 0.2757 - accuracy: 0.89 - ETA: 0s - loss: 0.2815 - accuracy: 0.89 - ETA: 0s - loss: 0.2819 - accuracy: 0.89 - ETA: 0s - loss: 0.2792 - accuracy: 0.90 - ETA: 0s - loss: 0.2793 - accuracy: 0.90 - 0s 5ms/step - loss: 0.2823 - accuracy: 0.8982 - val_loss: 0.3030 - val_accuracy: 0.8993\n",
      "Epoch 14/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 2s - loss: 0.3059 - accuracy: 0.87 - ETA: 0s - loss: 0.3067 - accuracy: 0.88 - ETA: 0s - loss: 0.2815 - accuracy: 0.90 - ETA: 0s - loss: 0.2710 - accuracy: 0.90 - ETA: 0s - loss: 0.2749 - accuracy: 0.89 - ETA: 0s - loss: 0.2746 - accuracy: 0.89 - ETA: 0s - loss: 0.2745 - accuracy: 0.89 - ETA: 0s - loss: 0.2767 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2794 - accuracy: 0.8962 - val_loss: 0.3174 - val_accuracy: 0.8867\n",
      "Epoch 15/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3565 - accuracy: 0.90 - ETA: 0s - loss: 0.2889 - accuracy: 0.90 - ETA: 0s - loss: 0.2625 - accuracy: 0.91 - ETA: 0s - loss: 0.2667 - accuracy: 0.91 - ETA: 0s - loss: 0.2676 - accuracy: 0.90 - ETA: 0s - loss: 0.2686 - accuracy: 0.90 - ETA: 0s - loss: 0.2688 - accuracy: 0.90 - ETA: 0s - loss: 0.2669 - accuracy: 0.90 - 1s 5ms/step - loss: 0.2726 - accuracy: 0.9057 - val_loss: 0.2972 - val_accuracy: 0.9016\n",
      "Epoch 16/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3492 - accuracy: 0.90 - ETA: 0s - loss: 0.2976 - accuracy: 0.90 - ETA: 0s - loss: 0.2757 - accuracy: 0.90 - ETA: 0s - loss: 0.2721 - accuracy: 0.90 - ETA: 0s - loss: 0.2708 - accuracy: 0.90 - ETA: 0s - loss: 0.2723 - accuracy: 0.90 - ETA: 0s - loss: 0.2674 - accuracy: 0.90 - ETA: 0s - loss: 0.2667 - accuracy: 0.90 - 1s 5ms/step - loss: 0.2685 - accuracy: 0.9054 - val_loss: 0.2877 - val_accuracy: 0.8993\n",
      "Epoch 17/1000\n",
      "110/110 [==============================] - ETA: 3s - loss: 0.2975 - accuracy: 0.93 - ETA: 0s - loss: 0.2687 - accuracy: 0.90 - ETA: 0s - loss: 0.2503 - accuracy: 0.91 - ETA: 0s - loss: 0.2484 - accuracy: 0.91 - ETA: 0s - loss: 0.2520 - accuracy: 0.90 - ETA: 0s - loss: 0.2524 - accuracy: 0.90 - ETA: 0s - loss: 0.2528 - accuracy: 0.90 - ETA: 0s - loss: 0.2559 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2589 - accuracy: 0.9057 - val_loss: 0.2887 - val_accuracy: 0.9108\n",
      "Epoch 18/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2514 - accuracy: 0.90 - ETA: 0s - loss: 0.2678 - accuracy: 0.91 - ETA: 0s - loss: 0.2464 - accuracy: 0.92 - ETA: 0s - loss: 0.2489 - accuracy: 0.91 - ETA: 0s - loss: 0.2493 - accuracy: 0.91 - ETA: 0s - loss: 0.2453 - accuracy: 0.91 - ETA: 0s - loss: 0.2473 - accuracy: 0.91 - ETA: 0s - loss: 0.2469 - accuracy: 0.91 - 1s 5ms/step - loss: 0.2501 - accuracy: 0.9137 - val_loss: 0.2863 - val_accuracy: 0.9096\n",
      "Epoch 19/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2818 - accuracy: 0.90 - ETA: 0s - loss: 0.2368 - accuracy: 0.91 - ETA: 0s - loss: 0.2337 - accuracy: 0.91 - ETA: 0s - loss: 0.2409 - accuracy: 0.91 - ETA: 0s - loss: 0.2462 - accuracy: 0.91 - ETA: 0s - loss: 0.2427 - accuracy: 0.91 - ETA: 0s - loss: 0.2423 - accuracy: 0.91 - ETA: 0s - loss: 0.2436 - accuracy: 0.91 - 0s 4ms/step - loss: 0.2459 - accuracy: 0.9128 - val_loss: 0.2914 - val_accuracy: 0.8947\n",
      "Epoch 20/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2755 - accuracy: 0.93 - ETA: 0s - loss: 0.2495 - accuracy: 0.91 - ETA: 0s - loss: 0.2402 - accuracy: 0.92 - ETA: 0s - loss: 0.2416 - accuracy: 0.91 - ETA: 0s - loss: 0.2402 - accuracy: 0.91 - ETA: 0s - loss: 0.2419 - accuracy: 0.91 - ETA: 0s - loss: 0.2472 - accuracy: 0.91 - ETA: 0s - loss: 0.2498 - accuracy: 0.91 - 1s 5ms/step - loss: 0.2504 - accuracy: 0.9114 - val_loss: 0.2745 - val_accuracy: 0.9130\n",
      "Epoch 21/1000\n",
      "110/110 [==============================] - ETA: 3s - loss: 0.3137 - accuracy: 0.87 - ETA: 0s - loss: 0.2575 - accuracy: 0.90 - ETA: 0s - loss: 0.2534 - accuracy: 0.91 - ETA: 0s - loss: 0.2448 - accuracy: 0.91 - ETA: 0s - loss: 0.2428 - accuracy: 0.91 - ETA: 0s - loss: 0.2415 - accuracy: 0.91 - ETA: 0s - loss: 0.2400 - accuracy: 0.91 - ETA: 0s - loss: 0.2396 - accuracy: 0.91 - 1s 5ms/step - loss: 0.2418 - accuracy: 0.9140 - val_loss: 0.2742 - val_accuracy: 0.9108\n",
      "Epoch 22/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2978 - accuracy: 0.93 - ETA: 0s - loss: 0.2557 - accuracy: 0.91 - ETA: 0s - loss: 0.2370 - accuracy: 0.91 - ETA: 0s - loss: 0.2339 - accuracy: 0.92 - ETA: 0s - loss: 0.2320 - accuracy: 0.92 - ETA: 0s - loss: 0.2340 - accuracy: 0.92 - ETA: 0s - loss: 0.2338 - accuracy: 0.92 - ETA: 0s - loss: 0.2320 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2347 - accuracy: 0.9197 - val_loss: 0.2819 - val_accuracy: 0.9130\n",
      "Epoch 23/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2532 - accuracy: 0.93 - ETA: 0s - loss: 0.2461 - accuracy: 0.91 - ETA: 0s - loss: 0.2302 - accuracy: 0.92 - ETA: 0s - loss: 0.2283 - accuracy: 0.92 - ETA: 0s - loss: 0.2320 - accuracy: 0.92 - ETA: 0s - loss: 0.2300 - accuracy: 0.91 - ETA: 0s - loss: 0.2318 - accuracy: 0.91 - ETA: 0s - loss: 0.2307 - accuracy: 0.91 - 0s 5ms/step - loss: 0.2334 - accuracy: 0.9177 - val_loss: 0.2664 - val_accuracy: 0.9130\n",
      "Epoch 24/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3204 - accuracy: 0.84 - ETA: 0s - loss: 0.2327 - accuracy: 0.92 - ETA: 0s - loss: 0.2184 - accuracy: 0.93 - ETA: 0s - loss: 0.2187 - accuracy: 0.93 - ETA: 0s - loss: 0.2243 - accuracy: 0.92 - ETA: 0s - loss: 0.2248 - accuracy: 0.92 - ETA: 0s - loss: 0.2296 - accuracy: 0.92 - ETA: 0s - loss: 0.2323 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2365 - accuracy: 0.9214 - val_loss: 0.2686 - val_accuracy: 0.9119\n",
      "Epoch 25/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2779 - accuracy: 0.93 - ETA: 0s - loss: 0.2439 - accuracy: 0.91 - ETA: 0s - loss: 0.2267 - accuracy: 0.92 - ETA: 0s - loss: 0.2214 - accuracy: 0.92 - ETA: 0s - loss: 0.2195 - accuracy: 0.91 - ETA: 0s - loss: 0.2227 - accuracy: 0.91 - ETA: 0s - loss: 0.2220 - accuracy: 0.91 - ETA: 0s - loss: 0.2271 - accuracy: 0.91 - 1s 5ms/step - loss: 0.2284 - accuracy: 0.9165 - val_loss: 0.2631 - val_accuracy: 0.9142\n",
      "Epoch 26/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2218 - accuracy: 0.93 - ETA: 0s - loss: 0.2323 - accuracy: 0.92 - ETA: 0s - loss: 0.2202 - accuracy: 0.92 - ETA: 0s - loss: 0.2100 - accuracy: 0.93 - ETA: 0s - loss: 0.2144 - accuracy: 0.93 - ETA: 0s - loss: 0.2164 - accuracy: 0.92 - ETA: 0s - loss: 0.2133 - accuracy: 0.92 - ETA: 0s - loss: 0.2233 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2240 - accuracy: 0.9240 - val_loss: 0.2635 - val_accuracy: 0.9188\n",
      "Epoch 27/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2757 - accuracy: 0.90 - ETA: 0s - loss: 0.2378 - accuracy: 0.91 - ETA: 0s - loss: 0.2140 - accuracy: 0.92 - ETA: 0s - loss: 0.2145 - accuracy: 0.93 - ETA: 0s - loss: 0.2138 - accuracy: 0.92 - ETA: 0s - loss: 0.2116 - accuracy: 0.92 - ETA: 0s - loss: 0.2180 - accuracy: 0.92 - ETA: 0s - loss: 0.2190 - accuracy: 0.92 - 1s 5ms/step - loss: 0.2239 - accuracy: 0.9260 - val_loss: 0.2602 - val_accuracy: 0.9165\n",
      "Epoch 28/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2479 - accuracy: 0.90 - ETA: 0s - loss: 0.2157 - accuracy: 0.92 - ETA: 0s - loss: 0.2084 - accuracy: 0.93 - ETA: 0s - loss: 0.1990 - accuracy: 0.93 - ETA: 0s - loss: 0.2063 - accuracy: 0.92 - ETA: 0s - loss: 0.2091 - accuracy: 0.92 - ETA: 0s - loss: 0.2038 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2090 - accuracy: 0.9268 - val_loss: 0.2596 - val_accuracy: 0.9188\n",
      "Epoch 29/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2944 - accuracy: 0.93 - ETA: 0s - loss: 0.2211 - accuracy: 0.92 - ETA: 0s - loss: 0.2062 - accuracy: 0.93 - ETA: 0s - loss: 0.1968 - accuracy: 0.93 - ETA: 0s - loss: 0.2004 - accuracy: 0.93 - ETA: 0s - loss: 0.2078 - accuracy: 0.93 - ETA: 0s - loss: 0.2109 - accuracy: 0.93 - ETA: 0s - loss: 0.2103 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2120 - accuracy: 0.9288 - val_loss: 0.2734 - val_accuracy: 0.9130\n",
      "Epoch 30/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2146 - accuracy: 0.93 - ETA: 0s - loss: 0.2216 - accuracy: 0.93 - ETA: 0s - loss: 0.1983 - accuracy: 0.93 - ETA: 0s - loss: 0.1951 - accuracy: 0.93 - ETA: 0s - loss: 0.1979 - accuracy: 0.93 - ETA: 0s - loss: 0.1999 - accuracy: 0.93 - ETA: 0s - loss: 0.2018 - accuracy: 0.93 - ETA: 0s - loss: 0.2033 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2057 - accuracy: 0.9288 - val_loss: 0.2541 - val_accuracy: 0.9211\n",
      "Epoch 31/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 2s - loss: 0.1675 - accuracy: 0.93 - ETA: 0s - loss: 0.2253 - accuracy: 0.91 - ETA: 0s - loss: 0.2113 - accuracy: 0.92 - ETA: 0s - loss: 0.2074 - accuracy: 0.92 - ETA: 0s - loss: 0.2092 - accuracy: 0.93 - ETA: 0s - loss: 0.2095 - accuracy: 0.92 - ETA: 0s - loss: 0.2062 - accuracy: 0.93 - ETA: 0s - loss: 0.2092 - accuracy: 0.92 - 1s 5ms/step - loss: 0.2111 - accuracy: 0.9262 - val_loss: 0.2520 - val_accuracy: 0.9153\n",
      "Epoch 32/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1858 - accuracy: 0.90 - ETA: 0s - loss: 0.1986 - accuracy: 0.93 - ETA: 0s - loss: 0.1988 - accuracy: 0.93 - ETA: 0s - loss: 0.1907 - accuracy: 0.93 - ETA: 0s - loss: 0.1930 - accuracy: 0.93 - ETA: 0s - loss: 0.2007 - accuracy: 0.93 - ETA: 0s - loss: 0.1974 - accuracy: 0.93 - ETA: 0s - loss: 0.2028 - accuracy: 0.93 - 0s 4ms/step - loss: 0.2023 - accuracy: 0.9308 - val_loss: 0.2610 - val_accuracy: 0.9199\n",
      "Epoch 33/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2632 - accuracy: 0.84 - ETA: 0s - loss: 0.2167 - accuracy: 0.91 - ETA: 0s - loss: 0.2000 - accuracy: 0.92 - ETA: 0s - loss: 0.1940 - accuracy: 0.93 - ETA: 0s - loss: 0.1940 - accuracy: 0.93 - ETA: 0s - loss: 0.1924 - accuracy: 0.93 - ETA: 0s - loss: 0.1971 - accuracy: 0.93 - ETA: 0s - loss: 0.1983 - accuracy: 0.93 - 1s 5ms/step - loss: 0.2013 - accuracy: 0.9297 - val_loss: 0.2501 - val_accuracy: 0.9188\n",
      "Epoch 34/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1483 - accuracy: 0.93 - ETA: 0s - loss: 0.2043 - accuracy: 0.93 - ETA: 0s - loss: 0.1915 - accuracy: 0.93 - ETA: 0s - loss: 0.1942 - accuracy: 0.93 - ETA: 0s - loss: 0.1956 - accuracy: 0.93 - ETA: 0s - loss: 0.1978 - accuracy: 0.92 - ETA: 0s - loss: 0.1986 - accuracy: 0.93 - ETA: 0s - loss: 0.1975 - accuracy: 0.93 - 0s 4ms/step - loss: 0.2001 - accuracy: 0.9294 - val_loss: 0.2618 - val_accuracy: 0.9222\n",
      "Epoch 35/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1622 - accuracy: 0.93 - ETA: 0s - loss: 0.2127 - accuracy: 0.93 - ETA: 0s - loss: 0.1940 - accuracy: 0.93 - ETA: 0s - loss: 0.1882 - accuracy: 0.93 - ETA: 0s - loss: 0.1908 - accuracy: 0.93 - ETA: 0s - loss: 0.1922 - accuracy: 0.92 - ETA: 0s - loss: 0.1925 - accuracy: 0.92 - ETA: 0s - loss: 0.1922 - accuracy: 0.92 - 1s 5ms/step - loss: 0.1954 - accuracy: 0.9280 - val_loss: 0.2437 - val_accuracy: 0.9245\n",
      "Epoch 36/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1921 - accuracy: 0.93 - ETA: 0s - loss: 0.1918 - accuracy: 0.93 - ETA: 0s - loss: 0.1749 - accuracy: 0.94 - ETA: 0s - loss: 0.1768 - accuracy: 0.94 - ETA: 0s - loss: 0.1840 - accuracy: 0.93 - ETA: 0s - loss: 0.1862 - accuracy: 0.93 - ETA: 0s - loss: 0.1889 - accuracy: 0.93 - ETA: 0s - loss: 0.1880 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1910 - accuracy: 0.9322 - val_loss: 0.2467 - val_accuracy: 0.9165\n",
      "Epoch 37/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1510 - accuracy: 0.96 - ETA: 0s - loss: 0.2037 - accuracy: 0.92 - ETA: 0s - loss: 0.1816 - accuracy: 0.93 - ETA: 0s - loss: 0.1816 - accuracy: 0.93 - ETA: 0s - loss: 0.1880 - accuracy: 0.93 - ETA: 0s - loss: 0.1906 - accuracy: 0.93 - ETA: 0s - loss: 0.1910 - accuracy: 0.93 - ETA: 0s - loss: 0.1921 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1937 - accuracy: 0.9308 - val_loss: 0.2579 - val_accuracy: 0.9199\n",
      "Epoch 38/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2460 - accuracy: 0.90 - ETA: 0s - loss: 0.1977 - accuracy: 0.93 - ETA: 0s - loss: 0.1880 - accuracy: 0.94 - ETA: 0s - loss: 0.1907 - accuracy: 0.94 - ETA: 0s - loss: 0.1865 - accuracy: 0.94 - ETA: 0s - loss: 0.1886 - accuracy: 0.93 - ETA: 0s - loss: 0.1851 - accuracy: 0.94 - ETA: 0s - loss: 0.1892 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1881 - accuracy: 0.9368 - val_loss: 0.2473 - val_accuracy: 0.9233\n",
      "Epoch 39/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1445 - accuracy: 0.93 - ETA: 0s - loss: 0.1879 - accuracy: 0.94 - ETA: 0s - loss: 0.1762 - accuracy: 0.94 - ETA: 0s - loss: 0.1728 - accuracy: 0.94 - ETA: 0s - loss: 0.1743 - accuracy: 0.94 - ETA: 0s - loss: 0.1763 - accuracy: 0.94 - ETA: 0s - loss: 0.1775 - accuracy: 0.94 - ETA: 0s - loss: 0.1847 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1853 - accuracy: 0.9394 - val_loss: 0.2492 - val_accuracy: 0.9245\n",
      "Epoch 40/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1730 - accuracy: 0.90 - ETA: 0s - loss: 0.1911 - accuracy: 0.93 - ETA: 0s - loss: 0.1805 - accuracy: 0.94 - ETA: 0s - loss: 0.1720 - accuracy: 0.94 - ETA: 0s - loss: 0.1701 - accuracy: 0.94 - ETA: 0s - loss: 0.1740 - accuracy: 0.94 - ETA: 0s - loss: 0.1750 - accuracy: 0.94 - ETA: 0s - loss: 0.1764 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1772 - accuracy: 0.9391 - val_loss: 0.2544 - val_accuracy: 0.9211\n",
      "Epoch 41/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1717 - accuracy: 0.93 - ETA: 0s - loss: 0.1821 - accuracy: 0.93 - ETA: 0s - loss: 0.1657 - accuracy: 0.94 - ETA: 0s - loss: 0.1622 - accuracy: 0.94 - ETA: 0s - loss: 0.1701 - accuracy: 0.94 - ETA: 0s - loss: 0.1666 - accuracy: 0.94 - ETA: 0s - loss: 0.1692 - accuracy: 0.94 - ETA: 0s - loss: 0.1706 - accuracy: 0.94 - 1s 5ms/step - loss: 0.1740 - accuracy: 0.9400 - val_loss: 0.2421 - val_accuracy: 0.9176\n",
      "Epoch 42/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1780 - accuracy: 0.93 - ETA: 0s - loss: 0.1704 - accuracy: 0.93 - ETA: 0s - loss: 0.1724 - accuracy: 0.93 - ETA: 0s - loss: 0.1695 - accuracy: 0.94 - ETA: 0s - loss: 0.1737 - accuracy: 0.94 - ETA: 0s - loss: 0.1723 - accuracy: 0.93 - ETA: 0s - loss: 0.1767 - accuracy: 0.93 - ETA: 0s - loss: 0.1789 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1793 - accuracy: 0.9371 - val_loss: 0.2400 - val_accuracy: 0.9188\n",
      "Epoch 43/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2805 - accuracy: 0.90 - ETA: 0s - loss: 0.1815 - accuracy: 0.93 - ETA: 0s - loss: 0.1697 - accuracy: 0.94 - ETA: 0s - loss: 0.1658 - accuracy: 0.94 - ETA: 0s - loss: 0.1722 - accuracy: 0.94 - ETA: 0s - loss: 0.1725 - accuracy: 0.94 - ETA: 0s - loss: 0.1747 - accuracy: 0.94 - ETA: 0s - loss: 0.1773 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1796 - accuracy: 0.9380 - val_loss: 0.2402 - val_accuracy: 0.9256\n",
      "Epoch 44/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2309 - accuracy: 0.90 - ETA: 0s - loss: 0.1798 - accuracy: 0.93 - ETA: 0s - loss: 0.1766 - accuracy: 0.94 - ETA: 0s - loss: 0.1719 - accuracy: 0.94 - ETA: 0s - loss: 0.1750 - accuracy: 0.94 - ETA: 0s - loss: 0.1816 - accuracy: 0.93 - ETA: 0s - loss: 0.1845 - accuracy: 0.93 - ETA: 0s - loss: 0.1854 - accuracy: 0.93 - 1s 5ms/step - loss: 0.1880 - accuracy: 0.9348 - val_loss: 0.2349 - val_accuracy: 0.9222\n",
      "Epoch 45/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1476 - accuracy: 0.96 - ETA: 0s - loss: 0.1616 - accuracy: 0.94 - ETA: 0s - loss: 0.1586 - accuracy: 0.94 - ETA: 0s - loss: 0.1626 - accuracy: 0.94 - ETA: 0s - loss: 0.1666 - accuracy: 0.94 - ETA: 0s - loss: 0.1643 - accuracy: 0.94 - ETA: 0s - loss: 0.1666 - accuracy: 0.94 - ETA: 0s - loss: 0.1688 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1710 - accuracy: 0.9380 - val_loss: 0.2445 - val_accuracy: 0.9245\n",
      "Epoch 46/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2395 - accuracy: 0.93 - ETA: 0s - loss: 0.1761 - accuracy: 0.94 - ETA: 0s - loss: 0.1659 - accuracy: 0.94 - ETA: 0s - loss: 0.1581 - accuracy: 0.94 - ETA: 0s - loss: 0.1618 - accuracy: 0.94 - ETA: 0s - loss: 0.1638 - accuracy: 0.94 - ETA: 0s - loss: 0.1665 - accuracy: 0.94 - ETA: 0s - loss: 0.1653 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1675 - accuracy: 0.9420 - val_loss: 0.2475 - val_accuracy: 0.9176\n",
      "Epoch 47/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1225 - accuracy: 0.93 - ETA: 0s - loss: 0.1796 - accuracy: 0.92 - ETA: 0s - loss: 0.1668 - accuracy: 0.93 - ETA: 0s - loss: 0.1710 - accuracy: 0.93 - ETA: 0s - loss: 0.1690 - accuracy: 0.94 - ETA: 0s - loss: 0.1685 - accuracy: 0.94 - ETA: 0s - loss: 0.1677 - accuracy: 0.94 - ETA: 0s - loss: 0.1686 - accuracy: 0.94 - 1s 5ms/step - loss: 0.1713 - accuracy: 0.9400 - val_loss: 0.2315 - val_accuracy: 0.9222\n",
      "Epoch 48/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 2s - loss: 0.1609 - accuracy: 0.93 - ETA: 0s - loss: 0.1710 - accuracy: 0.93 - ETA: 0s - loss: 0.1529 - accuracy: 0.94 - ETA: 0s - loss: 0.1498 - accuracy: 0.94 - ETA: 0s - loss: 0.1550 - accuracy: 0.94 - ETA: 0s - loss: 0.1565 - accuracy: 0.94 - ETA: 0s - loss: 0.1624 - accuracy: 0.94 - ETA: 0s - loss: 0.1632 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1645 - accuracy: 0.9434 - val_loss: 0.2337 - val_accuracy: 0.9233\n",
      "Epoch 49/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1463 - accuracy: 0.93 - ETA: 0s - loss: 0.1586 - accuracy: 0.93 - ETA: 0s - loss: 0.1505 - accuracy: 0.94 - ETA: 0s - loss: 0.1451 - accuracy: 0.94 - ETA: 0s - loss: 0.1508 - accuracy: 0.94 - ETA: 0s - loss: 0.1548 - accuracy: 0.94 - ETA: 0s - loss: 0.1588 - accuracy: 0.94 - ETA: 0s - loss: 0.1619 - accuracy: 0.94 - 1s 5ms/step - loss: 0.1650 - accuracy: 0.9417 - val_loss: 0.2294 - val_accuracy: 0.9245\n",
      "Epoch 50/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1372 - accuracy: 0.96 - ETA: 0s - loss: 0.1615 - accuracy: 0.93 - ETA: 0s - loss: 0.1557 - accuracy: 0.94 - ETA: 0s - loss: 0.1486 - accuracy: 0.95 - ETA: 0s - loss: 0.1531 - accuracy: 0.95 - ETA: 0s - loss: 0.1537 - accuracy: 0.94 - ETA: 0s - loss: 0.1554 - accuracy: 0.94 - ETA: 0s - loss: 0.1564 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1593 - accuracy: 0.9443 - val_loss: 0.2660 - val_accuracy: 0.9085\n",
      "Epoch 51/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1782 - accuracy: 0.93 - ETA: 0s - loss: 0.1768 - accuracy: 0.93 - ETA: 0s - loss: 0.1594 - accuracy: 0.94 - ETA: 0s - loss: 0.1488 - accuracy: 0.95 - ETA: 0s - loss: 0.1539 - accuracy: 0.94 - ETA: 0s - loss: 0.1562 - accuracy: 0.94 - ETA: 0s - loss: 0.1582 - accuracy: 0.94 - ETA: 0s - loss: 0.1598 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1635 - accuracy: 0.9428 - val_loss: 0.2406 - val_accuracy: 0.9245\n",
      "Epoch 52/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1768 - accuracy: 0.93 - ETA: 0s - loss: 0.1583 - accuracy: 0.93 - ETA: 0s - loss: 0.1461 - accuracy: 0.94 - ETA: 0s - loss: 0.1434 - accuracy: 0.94 - ETA: 0s - loss: 0.1471 - accuracy: 0.94 - ETA: 0s - loss: 0.1487 - accuracy: 0.94 - ETA: 0s - loss: 0.1512 - accuracy: 0.94 - ETA: 0s - loss: 0.1550 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1579 - accuracy: 0.9425 - val_loss: 0.2385 - val_accuracy: 0.9245\n",
      "Epoch 53/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1503 - accuracy: 0.93 - ETA: 0s - loss: 0.1648 - accuracy: 0.94 - ETA: 0s - loss: 0.1522 - accuracy: 0.94 - ETA: 0s - loss: 0.1448 - accuracy: 0.94 - ETA: 0s - loss: 0.1462 - accuracy: 0.94 - ETA: 0s - loss: 0.1471 - accuracy: 0.94 - ETA: 0s - loss: 0.1463 - accuracy: 0.94 - ETA: 0s - loss: 0.1477 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1501 - accuracy: 0.9448 - val_loss: 0.2365 - val_accuracy: 0.9314\n",
      "Epoch 54/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1874 - accuracy: 0.93 - ETA: 0s - loss: 0.1479 - accuracy: 0.95 - ETA: 0s - loss: 0.1407 - accuracy: 0.95 - ETA: 0s - loss: 0.1389 - accuracy: 0.95 - ETA: 0s - loss: 0.1421 - accuracy: 0.95 - ETA: 0s - loss: 0.1437 - accuracy: 0.95 - ETA: 0s - loss: 0.1469 - accuracy: 0.94 - ETA: 0s - loss: 0.1474 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1500 - accuracy: 0.9465 - val_loss: 0.2451 - val_accuracy: 0.9233\n",
      "Epoch 55/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1034 - accuracy: 0.96 - ETA: 0s - loss: 0.1478 - accuracy: 0.94 - ETA: 0s - loss: 0.1389 - accuracy: 0.95 - ETA: 0s - loss: 0.1423 - accuracy: 0.95 - ETA: 0s - loss: 0.1464 - accuracy: 0.94 - ETA: 0s - loss: 0.1454 - accuracy: 0.94 - ETA: 0s - loss: 0.1494 - accuracy: 0.94 - ETA: 0s - loss: 0.1547 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1538 - accuracy: 0.9463 - val_loss: 0.2287 - val_accuracy: 0.9279\n",
      "Epoch 56/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.0967 - accuracy: 0.96 - ETA: 0s - loss: 0.1491 - accuracy: 0.94 - ETA: 0s - loss: 0.1458 - accuracy: 0.94 - ETA: 0s - loss: 0.1380 - accuracy: 0.94 - ETA: 0s - loss: 0.1366 - accuracy: 0.95 - ETA: 0s - loss: 0.1400 - accuracy: 0.94 - ETA: 0s - loss: 0.1393 - accuracy: 0.95 - ETA: 0s - loss: 0.1422 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1428 - accuracy: 0.9491 - val_loss: 0.2373 - val_accuracy: 0.9291\n",
      "Epoch 57/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1093 - accuracy: 0.93 - ETA: 0s - loss: 0.1559 - accuracy: 0.94 - ETA: 0s - loss: 0.1600 - accuracy: 0.94 - ETA: 0s - loss: 0.1435 - accuracy: 0.94 - ETA: 0s - loss: 0.1434 - accuracy: 0.94 - ETA: 0s - loss: 0.1475 - accuracy: 0.94 - ETA: 0s - loss: 0.1466 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1509 - accuracy: 0.9451 - val_loss: 0.2422 - val_accuracy: 0.9233\n",
      "Epoch 58/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1122 - accuracy: 0.93 - ETA: 0s - loss: 0.1538 - accuracy: 0.94 - ETA: 0s - loss: 0.1531 - accuracy: 0.94 - ETA: 0s - loss: 0.1446 - accuracy: 0.94 - ETA: 0s - loss: 0.1491 - accuracy: 0.94 - ETA: 0s - loss: 0.1509 - accuracy: 0.94 - ETA: 0s - loss: 0.1508 - accuracy: 0.94 - ETA: 0s - loss: 0.1499 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9448 - val_loss: 0.2331 - val_accuracy: 0.9199\n",
      "Epoch 59/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1168 - accuracy: 0.93 - ETA: 0s - loss: 0.1464 - accuracy: 0.95 - ETA: 0s - loss: 0.1477 - accuracy: 0.95 - ETA: 0s - loss: 0.1364 - accuracy: 0.95 - ETA: 0s - loss: 0.1394 - accuracy: 0.95 - ETA: 0s - loss: 0.1455 - accuracy: 0.94 - ETA: 0s - loss: 0.1451 - accuracy: 0.95 - ETA: 0s - loss: 0.1482 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1485 - accuracy: 0.9494 - val_loss: 0.2462 - val_accuracy: 0.9268\n",
      "Epoch 60/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1014 - accuracy: 0.93 - ETA: 0s - loss: 0.1429 - accuracy: 0.95 - ETA: 0s - loss: 0.1313 - accuracy: 0.95 - ETA: 0s - loss: 0.1217 - accuracy: 0.95 - ETA: 0s - loss: 0.1276 - accuracy: 0.95 - ETA: 0s - loss: 0.1333 - accuracy: 0.95 - ETA: 0s - loss: 0.1356 - accuracy: 0.95 - ETA: 0s - loss: 0.1360 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1403 - accuracy: 0.9480 - val_loss: 0.2729 - val_accuracy: 0.9062\n",
      "Epoch 61/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2505 - accuracy: 0.87 - ETA: 0s - loss: 0.1799 - accuracy: 0.93 - ETA: 0s - loss: 0.1517 - accuracy: 0.94 - ETA: 0s - loss: 0.1341 - accuracy: 0.95 - ETA: 0s - loss: 0.1348 - accuracy: 0.95 - ETA: 0s - loss: 0.1356 - accuracy: 0.95 - ETA: 0s - loss: 0.1375 - accuracy: 0.95 - ETA: 0s - loss: 0.1404 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1401 - accuracy: 0.9508 - val_loss: 0.2446 - val_accuracy: 0.9233\n",
      "Epoch 62/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.0888 - accuracy: 0.96 - ETA: 0s - loss: 0.1319 - accuracy: 0.94 - ETA: 0s - loss: 0.1171 - accuracy: 0.95 - ETA: 0s - loss: 0.1169 - accuracy: 0.95 - ETA: 0s - loss: 0.1226 - accuracy: 0.95 - ETA: 0s - loss: 0.1230 - accuracy: 0.95 - ETA: 0s - loss: 0.1250 - accuracy: 0.95 - ETA: 0s - loss: 0.1327 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1324 - accuracy: 0.9508 - val_loss: 0.2325 - val_accuracy: 0.9233\n",
      "Epoch 63/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.0696 - accuracy: 0.96 - ETA: 0s - loss: 0.1283 - accuracy: 0.95 - ETA: 0s - loss: 0.1336 - accuracy: 0.95 - ETA: 0s - loss: 0.1289 - accuracy: 0.95 - ETA: 0s - loss: 0.1297 - accuracy: 0.95 - ETA: 0s - loss: 0.1284 - accuracy: 0.95 - ETA: 0s - loss: 0.1305 - accuracy: 0.95 - ETA: 0s - loss: 0.1344 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9503 - val_loss: 0.2445 - val_accuracy: 0.9268\n",
      "Epoch 64/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1202 - accuracy: 0.93 - ETA: 0s - loss: 0.1233 - accuracy: 0.95 - ETA: 0s - loss: 0.1224 - accuracy: 0.95 - ETA: 0s - loss: 0.1204 - accuracy: 0.95 - ETA: 0s - loss: 0.1206 - accuracy: 0.95 - ETA: 0s - loss: 0.1235 - accuracy: 0.95 - ETA: 0s - loss: 0.1257 - accuracy: 0.95 - ETA: 0s - loss: 0.1281 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1300 - accuracy: 0.9528 - val_loss: 0.2334 - val_accuracy: 0.9279\n",
      "Epoch 65/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 2s - loss: 0.0717 - accuracy: 0.96 - ETA: 0s - loss: 0.1341 - accuracy: 0.95 - ETA: 0s - loss: 0.1311 - accuracy: 0.95 - ETA: 0s - loss: 0.1290 - accuracy: 0.95 - ETA: 0s - loss: 0.1305 - accuracy: 0.95 - ETA: 0s - loss: 0.1316 - accuracy: 0.95 - ETA: 0s - loss: 0.1314 - accuracy: 0.95 - ETA: 0s - loss: 0.1324 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1342 - accuracy: 0.9511 - val_loss: 0.2360 - val_accuracy: 0.9314\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: dbb269cf2cb850669fb8cbbf8f023f41</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.22869244058217322</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-classification_head_1/dropout_rate: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-classification_head_1/spatial_reduction_1/reduction_type: flatten</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-classification_head_2/dropout_rate: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-classification_head_2/spatial_reduction_1/reduction_type: global_avg</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_block_1/dropout_rate: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dense_block_1/num_layers: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_block_1/units_0: 128</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dense_block_1/use_batchnorm: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/augment: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/block_type: vanilla</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/dropout_rate: 0.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/filters_0_0: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/filters_0_1: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/kernel_size: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/max_pooling: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/num_blocks: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/num_layers: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/separable: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/normalize: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/augment: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/block_type: vanilla</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/dropout_rate: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/filters_0_0: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/filters_0_1: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/filters_1_0: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/filters_1_1: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/kernel_size: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/max_pooling: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/num_blocks: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/num_layers: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/separable: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/normalize: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-optimizer: adam</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 110 steps, validate for 28 steps\n",
      "Epoch 1/1000\n",
      "110/110 [==============================] - ETA: 8:14 - loss: 0.6147 - accuracy: 0.59 - ETA: 2:44 - loss: 1.5819 - accuracy: 0.62 - ETA: 1:38 - loss: 1.2501 - accuracy: 0.61 - ETA: 1:09 - loss: 1.0679 - accuracy: 0.63 - ETA: 54s - loss: 1.0594 - accuracy: 0.6215 - ETA: 43s - loss: 0.9919 - accuracy: 0.613 - ETA: 36s - loss: 0.9649 - accuracy: 0.610 - ETA: 31s - loss: 0.9211 - accuracy: 0.622 - ETA: 27s - loss: 0.8851 - accuracy: 0.623 - ETA: 24s - loss: 0.8559 - accuracy: 0.628 - ETA: 22s - loss: 0.8239 - accuracy: 0.639 - ETA: 19s - loss: 0.7928 - accuracy: 0.652 - ETA: 18s - loss: 0.7756 - accuracy: 0.653 - ETA: 16s - loss: 0.7567 - accuracy: 0.658 - ETA: 15s - loss: 0.7565 - accuracy: 0.661 - ETA: 14s - loss: 0.7436 - accuracy: 0.665 - ETA: 13s - loss: 0.7405 - accuracy: 0.664 - ETA: 12s - loss: 0.7242 - accuracy: 0.671 - ETA: 11s - loss: 0.7129 - accuracy: 0.673 - ETA: 10s - loss: 0.7065 - accuracy: 0.667 - ETA: 9s - loss: 0.7027 - accuracy: 0.667 - ETA: 9s - loss: 0.6951 - accuracy: 0.67 - ETA: 8s - loss: 0.7003 - accuracy: 0.67 - ETA: 8s - loss: 0.6946 - accuracy: 0.67 - ETA: 7s - loss: 0.6887 - accuracy: 0.67 - ETA: 7s - loss: 0.6851 - accuracy: 0.67 - ETA: 6s - loss: 0.6795 - accuracy: 0.67 - ETA: 6s - loss: 0.6765 - accuracy: 0.67 - ETA: 5s - loss: 0.6729 - accuracy: 0.67 - ETA: 5s - loss: 0.6709 - accuracy: 0.67 - ETA: 5s - loss: 0.6692 - accuracy: 0.67 - ETA: 4s - loss: 0.6641 - accuracy: 0.67 - ETA: 4s - loss: 0.6596 - accuracy: 0.68 - ETA: 4s - loss: 0.6566 - accuracy: 0.68 - ETA: 4s - loss: 0.6550 - accuracy: 0.67 - ETA: 3s - loss: 0.6524 - accuracy: 0.67 - ETA: 3s - loss: 0.6508 - accuracy: 0.68 - ETA: 3s - loss: 0.6508 - accuracy: 0.68 - ETA: 2s - loss: 0.6447 - accuracy: 0.68 - ETA: 2s - loss: 0.6443 - accuracy: 0.68 - ETA: 2s - loss: 0.6413 - accuracy: 0.68 - ETA: 2s - loss: 0.6398 - accuracy: 0.68 - ETA: 2s - loss: 0.6385 - accuracy: 0.68 - ETA: 1s - loss: 0.6375 - accuracy: 0.68 - ETA: 1s - loss: 0.6354 - accuracy: 0.68 - ETA: 1s - loss: 0.6344 - accuracy: 0.68 - ETA: 1s - loss: 0.6314 - accuracy: 0.68 - ETA: 1s - loss: 0.6311 - accuracy: 0.68 - ETA: 1s - loss: 0.6297 - accuracy: 0.68 - ETA: 0s - loss: 0.6271 - accuracy: 0.68 - ETA: 0s - loss: 0.6250 - accuracy: 0.68 - ETA: 0s - loss: 0.6229 - accuracy: 0.68 - ETA: 0s - loss: 0.6201 - accuracy: 0.68 - ETA: 0s - loss: 0.6188 - accuracy: 0.68 - ETA: 0s - loss: 0.6160 - accuracy: 0.69 - 10s 89ms/step - loss: 0.6149 - accuracy: 0.6913 - val_loss: 0.6531 - val_accuracy: 0.6682\n",
      "Epoch 2/1000\n",
      "110/110 [==============================] - ETA: 5s - loss: 0.3986 - accuracy: 0.78 - ETA: 4s - loss: 0.3546 - accuracy: 0.82 - ETA: 3s - loss: 0.3256 - accuracy: 0.86 - ETA: 3s - loss: 0.3290 - accuracy: 0.87 - ETA: 3s - loss: 0.3885 - accuracy: 0.85 - ETA: 3s - loss: 0.3812 - accuracy: 0.85 - ETA: 3s - loss: 0.4100 - accuracy: 0.83 - ETA: 3s - loss: 0.4243 - accuracy: 0.83 - ETA: 3s - loss: 0.4139 - accuracy: 0.84 - ETA: 3s - loss: 0.4161 - accuracy: 0.84 - ETA: 2s - loss: 0.4309 - accuracy: 0.83 - ETA: 2s - loss: 0.4181 - accuracy: 0.84 - ETA: 2s - loss: 0.4153 - accuracy: 0.84 - ETA: 2s - loss: 0.4104 - accuracy: 0.84 - ETA: 2s - loss: 0.4181 - accuracy: 0.84 - ETA: 2s - loss: 0.4158 - accuracy: 0.84 - ETA: 2s - loss: 0.4246 - accuracy: 0.84 - ETA: 2s - loss: 0.4154 - accuracy: 0.84 - ETA: 2s - loss: 0.4140 - accuracy: 0.84 - ETA: 2s - loss: 0.4083 - accuracy: 0.84 - ETA: 2s - loss: 0.4019 - accuracy: 0.85 - ETA: 2s - loss: 0.4003 - accuracy: 0.85 - ETA: 2s - loss: 0.3988 - accuracy: 0.85 - ETA: 2s - loss: 0.4007 - accuracy: 0.85 - ETA: 2s - loss: 0.3916 - accuracy: 0.85 - ETA: 1s - loss: 0.3886 - accuracy: 0.85 - ETA: 1s - loss: 0.3887 - accuracy: 0.85 - ETA: 1s - loss: 0.3835 - accuracy: 0.85 - ETA: 1s - loss: 0.3807 - accuracy: 0.85 - ETA: 1s - loss: 0.3810 - accuracy: 0.85 - ETA: 1s - loss: 0.3835 - accuracy: 0.85 - ETA: 1s - loss: 0.3807 - accuracy: 0.85 - ETA: 1s - loss: 0.3802 - accuracy: 0.85 - ETA: 1s - loss: 0.3812 - accuracy: 0.85 - ETA: 1s - loss: 0.3819 - accuracy: 0.85 - ETA: 1s - loss: 0.3810 - accuracy: 0.85 - ETA: 1s - loss: 0.3806 - accuracy: 0.85 - ETA: 1s - loss: 0.3800 - accuracy: 0.85 - ETA: 1s - loss: 0.3752 - accuracy: 0.86 - ETA: 1s - loss: 0.3743 - accuracy: 0.86 - ETA: 0s - loss: 0.3732 - accuracy: 0.85 - ETA: 0s - loss: 0.3731 - accuracy: 0.86 - ETA: 0s - loss: 0.3701 - accuracy: 0.86 - ETA: 0s - loss: 0.3674 - accuracy: 0.86 - ETA: 0s - loss: 0.3642 - accuracy: 0.86 - ETA: 0s - loss: 0.3631 - accuracy: 0.86 - ETA: 0s - loss: 0.3594 - accuracy: 0.86 - ETA: 0s - loss: 0.3579 - accuracy: 0.86 - ETA: 0s - loss: 0.3570 - accuracy: 0.86 - ETA: 0s - loss: 0.3606 - accuracy: 0.86 - ETA: 0s - loss: 0.3601 - accuracy: 0.86 - ETA: 0s - loss: 0.3599 - accuracy: 0.86 - ETA: 0s - loss: 0.3598 - accuracy: 0.86 - ETA: 0s - loss: 0.3601 - accuracy: 0.86 - ETA: 0s - loss: 0.3610 - accuracy: 0.86 - 5s 41ms/step - loss: 0.3632 - accuracy: 0.8631 - val_loss: 0.6301 - val_accuracy: 0.7197\n",
      "Epoch 3/1000\n",
      "110/110 [==============================] - ETA: 5s - loss: 0.1512 - accuracy: 0.93 - ETA: 4s - loss: 0.1803 - accuracy: 0.92 - ETA: 3s - loss: 0.2159 - accuracy: 0.93 - ETA: 3s - loss: 0.2423 - accuracy: 0.91 - ETA: 3s - loss: 0.3032 - accuracy: 0.89 - ETA: 3s - loss: 0.3068 - accuracy: 0.89 - ETA: 3s - loss: 0.3311 - accuracy: 0.88 - ETA: 3s - loss: 0.3407 - accuracy: 0.88 - ETA: 3s - loss: 0.3378 - accuracy: 0.88 - ETA: 3s - loss: 0.3354 - accuracy: 0.88 - ETA: 2s - loss: 0.3329 - accuracy: 0.88 - ETA: 2s - loss: 0.3226 - accuracy: 0.88 - ETA: 2s - loss: 0.3170 - accuracy: 0.88 - ETA: 2s - loss: 0.3140 - accuracy: 0.88 - ETA: 2s - loss: 0.3136 - accuracy: 0.88 - ETA: 2s - loss: 0.3041 - accuracy: 0.89 - ETA: 2s - loss: 0.3050 - accuracy: 0.89 - ETA: 2s - loss: 0.2988 - accuracy: 0.89 - ETA: 2s - loss: 0.2994 - accuracy: 0.89 - ETA: 2s - loss: 0.2940 - accuracy: 0.89 - ETA: 2s - loss: 0.2895 - accuracy: 0.89 - ETA: 2s - loss: 0.2933 - accuracy: 0.89 - ETA: 2s - loss: 0.2950 - accuracy: 0.89 - ETA: 2s - loss: 0.2925 - accuracy: 0.89 - ETA: 1s - loss: 0.2887 - accuracy: 0.89 - ETA: 1s - loss: 0.2871 - accuracy: 0.89 - ETA: 1s - loss: 0.2889 - accuracy: 0.89 - ETA: 1s - loss: 0.2886 - accuracy: 0.89 - ETA: 1s - loss: 0.2883 - accuracy: 0.89 - ETA: 1s - loss: 0.2903 - accuracy: 0.89 - ETA: 1s - loss: 0.2939 - accuracy: 0.89 - ETA: 1s - loss: 0.2915 - accuracy: 0.89 - ETA: 1s - loss: 0.2939 - accuracy: 0.88 - ETA: 1s - loss: 0.2986 - accuracy: 0.88 - ETA: 1s - loss: 0.2994 - accuracy: 0.88 - ETA: 1s - loss: 0.3014 - accuracy: 0.88 - ETA: 1s - loss: 0.3020 - accuracy: 0.88 - ETA: 1s - loss: 0.3034 - accuracy: 0.89 - ETA: 1s - loss: 0.2982 - accuracy: 0.89 - ETA: 1s - loss: 0.2981 - accuracy: 0.89 - ETA: 0s - loss: 0.2967 - accuracy: 0.89 - ETA: 0s - loss: 0.3008 - accuracy: 0.89 - ETA: 0s - loss: 0.3004 - accuracy: 0.89 - ETA: 0s - loss: 0.3006 - accuracy: 0.89 - ETA: 0s - loss: 0.2985 - accuracy: 0.89 - ETA: 0s - loss: 0.2985 - accuracy: 0.89 - ETA: 0s - loss: 0.2946 - accuracy: 0.89 - ETA: 0s - loss: 0.2950 - accuracy: 0.89 - ETA: 0s - loss: 0.2947 - accuracy: 0.89 - ETA: 0s - loss: 0.2925 - accuracy: 0.89 - ETA: 0s - loss: 0.2911 - accuracy: 0.89 - ETA: 0s - loss: 0.2915 - accuracy: 0.89 - ETA: 0s - loss: 0.2915 - accuracy: 0.89 - ETA: 0s - loss: 0.2918 - accuracy: 0.89 - ETA: 0s - loss: 0.2935 - accuracy: 0.89 - 5s 41ms/step - loss: 0.2932 - accuracy: 0.8931 - val_loss: 0.6202 - val_accuracy: 0.7300\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 5s - loss: 0.1760 - accuracy: 0.90 - ETA: 4s - loss: 0.1499 - accuracy: 0.95 - ETA: 3s - loss: 0.2046 - accuracy: 0.92 - ETA: 3s - loss: 0.2290 - accuracy: 0.91 - ETA: 3s - loss: 0.2977 - accuracy: 0.89 - ETA: 3s - loss: 0.2920 - accuracy: 0.89 - ETA: 3s - loss: 0.2973 - accuracy: 0.89 - ETA: 3s - loss: 0.3021 - accuracy: 0.89 - ETA: 3s - loss: 0.2905 - accuracy: 0.90 - ETA: 3s - loss: 0.2851 - accuracy: 0.90 - ETA: 2s - loss: 0.2829 - accuracy: 0.90 - ETA: 2s - loss: 0.2690 - accuracy: 0.91 - ETA: 2s - loss: 0.2682 - accuracy: 0.90 - ETA: 2s - loss: 0.2692 - accuracy: 0.90 - ETA: 2s - loss: 0.2769 - accuracy: 0.90 - ETA: 2s - loss: 0.2698 - accuracy: 0.91 - ETA: 2s - loss: 0.2742 - accuracy: 0.91 - ETA: 2s - loss: 0.2671 - accuracy: 0.91 - ETA: 2s - loss: 0.2689 - accuracy: 0.91 - ETA: 2s - loss: 0.2660 - accuracy: 0.91 - ETA: 2s - loss: 0.2618 - accuracy: 0.91 - ETA: 2s - loss: 0.2619 - accuracy: 0.91 - ETA: 2s - loss: 0.2649 - accuracy: 0.91 - ETA: 2s - loss: 0.2661 - accuracy: 0.91 - ETA: 1s - loss: 0.2628 - accuracy: 0.91 - ETA: 1s - loss: 0.2634 - accuracy: 0.91 - ETA: 1s - loss: 0.2689 - accuracy: 0.90 - ETA: 1s - loss: 0.2690 - accuracy: 0.90 - ETA: 1s - loss: 0.2703 - accuracy: 0.90 - ETA: 1s - loss: 0.2726 - accuracy: 0.90 - ETA: 1s - loss: 0.2746 - accuracy: 0.90 - ETA: 1s - loss: 0.2715 - accuracy: 0.90 - ETA: 1s - loss: 0.2725 - accuracy: 0.90 - ETA: 1s - loss: 0.2781 - accuracy: 0.90 - ETA: 1s - loss: 0.2768 - accuracy: 0.90 - ETA: 1s - loss: 0.2784 - accuracy: 0.90 - ETA: 1s - loss: 0.2785 - accuracy: 0.90 - ETA: 1s - loss: 0.2779 - accuracy: 0.90 - ETA: 1s - loss: 0.2739 - accuracy: 0.90 - ETA: 1s - loss: 0.2736 - accuracy: 0.90 - ETA: 0s - loss: 0.2744 - accuracy: 0.90 - ETA: 0s - loss: 0.2755 - accuracy: 0.90 - ETA: 0s - loss: 0.2763 - accuracy: 0.90 - ETA: 0s - loss: 0.2750 - accuracy: 0.90 - ETA: 0s - loss: 0.2724 - accuracy: 0.90 - ETA: 0s - loss: 0.2740 - accuracy: 0.90 - ETA: 0s - loss: 0.2705 - accuracy: 0.90 - ETA: 0s - loss: 0.2706 - accuracy: 0.90 - ETA: 0s - loss: 0.2711 - accuracy: 0.90 - ETA: 0s - loss: 0.2704 - accuracy: 0.90 - ETA: 0s - loss: 0.2690 - accuracy: 0.90 - ETA: 0s - loss: 0.2687 - accuracy: 0.90 - ETA: 0s - loss: 0.2683 - accuracy: 0.90 - ETA: 0s - loss: 0.2677 - accuracy: 0.90 - ETA: 0s - loss: 0.2719 - accuracy: 0.90 - 5s 41ms/step - loss: 0.2704 - accuracy: 0.9025 - val_loss: 0.2782 - val_accuracy: 0.8890\n",
      "Epoch 5/1000\n",
      "110/110 [==============================] - ETA: 5s - loss: 0.2094 - accuracy: 0.90 - ETA: 4s - loss: 0.1814 - accuracy: 0.93 - ETA: 3s - loss: 0.2062 - accuracy: 0.93 - ETA: 3s - loss: 0.2183 - accuracy: 0.92 - ETA: 3s - loss: 0.2836 - accuracy: 0.90 - ETA: 3s - loss: 0.2643 - accuracy: 0.91 - ETA: 3s - loss: 0.2687 - accuracy: 0.90 - ETA: 3s - loss: 0.2668 - accuracy: 0.91 - ETA: 3s - loss: 0.2565 - accuracy: 0.91 - ETA: 3s - loss: 0.2495 - accuracy: 0.91 - ETA: 2s - loss: 0.2571 - accuracy: 0.91 - ETA: 2s - loss: 0.2482 - accuracy: 0.91 - ETA: 2s - loss: 0.2519 - accuracy: 0.91 - ETA: 2s - loss: 0.2538 - accuracy: 0.91 - ETA: 2s - loss: 0.2563 - accuracy: 0.91 - ETA: 2s - loss: 0.2575 - accuracy: 0.91 - ETA: 2s - loss: 0.2649 - accuracy: 0.91 - ETA: 2s - loss: 0.2621 - accuracy: 0.91 - ETA: 2s - loss: 0.2626 - accuracy: 0.91 - ETA: 2s - loss: 0.2601 - accuracy: 0.91 - ETA: 2s - loss: 0.2569 - accuracy: 0.91 - ETA: 2s - loss: 0.2568 - accuracy: 0.91 - ETA: 2s - loss: 0.2615 - accuracy: 0.91 - ETA: 2s - loss: 0.2596 - accuracy: 0.91 - ETA: 1s - loss: 0.2576 - accuracy: 0.91 - ETA: 1s - loss: 0.2583 - accuracy: 0.91 - ETA: 1s - loss: 0.2616 - accuracy: 0.91 - ETA: 1s - loss: 0.2602 - accuracy: 0.91 - ETA: 1s - loss: 0.2608 - accuracy: 0.91 - ETA: 1s - loss: 0.2628 - accuracy: 0.91 - ETA: 1s - loss: 0.2658 - accuracy: 0.90 - ETA: 1s - loss: 0.2636 - accuracy: 0.91 - ETA: 1s - loss: 0.2658 - accuracy: 0.91 - ETA: 1s - loss: 0.2694 - accuracy: 0.91 - ETA: 1s - loss: 0.2707 - accuracy: 0.90 - ETA: 1s - loss: 0.2712 - accuracy: 0.90 - ETA: 1s - loss: 0.2717 - accuracy: 0.90 - ETA: 1s - loss: 0.2711 - accuracy: 0.90 - ETA: 1s - loss: 0.2681 - accuracy: 0.90 - ETA: 1s - loss: 0.2682 - accuracy: 0.90 - ETA: 0s - loss: 0.2676 - accuracy: 0.90 - ETA: 0s - loss: 0.2710 - accuracy: 0.90 - ETA: 0s - loss: 0.2700 - accuracy: 0.90 - ETA: 0s - loss: 0.2710 - accuracy: 0.90 - ETA: 0s - loss: 0.2676 - accuracy: 0.90 - ETA: 0s - loss: 0.2669 - accuracy: 0.91 - ETA: 0s - loss: 0.2632 - accuracy: 0.91 - ETA: 0s - loss: 0.2651 - accuracy: 0.91 - ETA: 0s - loss: 0.2650 - accuracy: 0.91 - ETA: 0s - loss: 0.2649 - accuracy: 0.91 - ETA: 0s - loss: 0.2647 - accuracy: 0.91 - ETA: 0s - loss: 0.2655 - accuracy: 0.90 - ETA: 0s - loss: 0.2651 - accuracy: 0.90 - ETA: 0s - loss: 0.2649 - accuracy: 0.90 - ETA: 0s - loss: 0.2680 - accuracy: 0.90 - 4s 35ms/step - loss: 0.2679 - accuracy: 0.9065 - val_loss: 0.6157 - val_accuracy: 0.7597\n",
      "Epoch 6/1000\n",
      "110/110 [==============================] - ETA: 5s - loss: 0.1701 - accuracy: 0.90 - ETA: 4s - loss: 0.1575 - accuracy: 0.94 - ETA: 3s - loss: 0.1964 - accuracy: 0.93 - ETA: 3s - loss: 0.1999 - accuracy: 0.94 - ETA: 3s - loss: 0.2446 - accuracy: 0.92 - ETA: 3s - loss: 0.2361 - accuracy: 0.92 - ETA: 3s - loss: 0.2519 - accuracy: 0.91 - ETA: 3s - loss: 0.2524 - accuracy: 0.91 - ETA: 3s - loss: 0.2438 - accuracy: 0.91 - ETA: 3s - loss: 0.2374 - accuracy: 0.91 - ETA: 2s - loss: 0.2410 - accuracy: 0.91 - ETA: 2s - loss: 0.2371 - accuracy: 0.91 - ETA: 2s - loss: 0.2336 - accuracy: 0.92 - ETA: 2s - loss: 0.2314 - accuracy: 0.92 - ETA: 2s - loss: 0.2301 - accuracy: 0.92 - ETA: 2s - loss: 0.2258 - accuracy: 0.92 - ETA: 2s - loss: 0.2249 - accuracy: 0.92 - ETA: 2s - loss: 0.2201 - accuracy: 0.92 - ETA: 2s - loss: 0.2256 - accuracy: 0.92 - ETA: 2s - loss: 0.2208 - accuracy: 0.92 - ETA: 2s - loss: 0.2177 - accuracy: 0.92 - ETA: 2s - loss: 0.2204 - accuracy: 0.92 - ETA: 2s - loss: 0.2254 - accuracy: 0.92 - ETA: 2s - loss: 0.2266 - accuracy: 0.92 - ETA: 2s - loss: 0.2253 - accuracy: 0.92 - ETA: 1s - loss: 0.2287 - accuracy: 0.92 - ETA: 1s - loss: 0.2298 - accuracy: 0.92 - ETA: 1s - loss: 0.2273 - accuracy: 0.92 - ETA: 1s - loss: 0.2287 - accuracy: 0.92 - ETA: 1s - loss: 0.2308 - accuracy: 0.92 - ETA: 1s - loss: 0.2333 - accuracy: 0.92 - ETA: 1s - loss: 0.2299 - accuracy: 0.92 - ETA: 1s - loss: 0.2317 - accuracy: 0.92 - ETA: 1s - loss: 0.2373 - accuracy: 0.92 - ETA: 1s - loss: 0.2355 - accuracy: 0.92 - ETA: 1s - loss: 0.2368 - accuracy: 0.91 - ETA: 1s - loss: 0.2365 - accuracy: 0.92 - ETA: 1s - loss: 0.2354 - accuracy: 0.92 - ETA: 1s - loss: 0.2316 - accuracy: 0.92 - ETA: 1s - loss: 0.2329 - accuracy: 0.92 - ETA: 0s - loss: 0.2330 - accuracy: 0.92 - ETA: 0s - loss: 0.2348 - accuracy: 0.92 - ETA: 0s - loss: 0.2342 - accuracy: 0.92 - ETA: 0s - loss: 0.2336 - accuracy: 0.92 - ETA: 0s - loss: 0.2308 - accuracy: 0.92 - ETA: 0s - loss: 0.2298 - accuracy: 0.92 - ETA: 0s - loss: 0.2258 - accuracy: 0.92 - ETA: 0s - loss: 0.2256 - accuracy: 0.92 - ETA: 0s - loss: 0.2259 - accuracy: 0.92 - ETA: 0s - loss: 0.2250 - accuracy: 0.92 - ETA: 0s - loss: 0.2243 - accuracy: 0.92 - ETA: 0s - loss: 0.2244 - accuracy: 0.92 - ETA: 0s - loss: 0.2236 - accuracy: 0.92 - ETA: 0s - loss: 0.2237 - accuracy: 0.92 - ETA: 0s - loss: 0.2279 - accuracy: 0.92 - 4s 35ms/step - loss: 0.2265 - accuracy: 0.9222 - val_loss: 0.2835 - val_accuracy: 0.8879\n",
      "Epoch 7/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 5s - loss: 0.1802 - accuracy: 0.90 - ETA: 4s - loss: 0.2136 - accuracy: 0.91 - ETA: 3s - loss: 0.2464 - accuracy: 0.90 - ETA: 3s - loss: 0.2512 - accuracy: 0.90 - ETA: 3s - loss: 0.3113 - accuracy: 0.88 - ETA: 3s - loss: 0.2842 - accuracy: 0.89 - ETA: 3s - loss: 0.2824 - accuracy: 0.89 - ETA: 3s - loss: 0.2846 - accuracy: 0.89 - ETA: 3s - loss: 0.2697 - accuracy: 0.90 - ETA: 3s - loss: 0.2660 - accuracy: 0.90 - ETA: 2s - loss: 0.2661 - accuracy: 0.90 - ETA: 2s - loss: 0.2575 - accuracy: 0.90 - ETA: 2s - loss: 0.2508 - accuracy: 0.90 - ETA: 2s - loss: 0.2483 - accuracy: 0.90 - ETA: 2s - loss: 0.2510 - accuracy: 0.90 - ETA: 2s - loss: 0.2435 - accuracy: 0.91 - ETA: 2s - loss: 0.2407 - accuracy: 0.91 - ETA: 2s - loss: 0.2331 - accuracy: 0.91 - ETA: 2s - loss: 0.2339 - accuracy: 0.91 - ETA: 2s - loss: 0.2284 - accuracy: 0.92 - ETA: 2s - loss: 0.2240 - accuracy: 0.92 - ETA: 2s - loss: 0.2242 - accuracy: 0.92 - ETA: 2s - loss: 0.2297 - accuracy: 0.92 - ETA: 2s - loss: 0.2334 - accuracy: 0.91 - ETA: 2s - loss: 0.2307 - accuracy: 0.91 - ETA: 1s - loss: 0.2324 - accuracy: 0.91 - ETA: 1s - loss: 0.2358 - accuracy: 0.91 - ETA: 1s - loss: 0.2339 - accuracy: 0.91 - ETA: 1s - loss: 0.2356 - accuracy: 0.91 - ETA: 1s - loss: 0.2377 - accuracy: 0.91 - ETA: 1s - loss: 0.2401 - accuracy: 0.91 - ETA: 1s - loss: 0.2370 - accuracy: 0.91 - ETA: 1s - loss: 0.2390 - accuracy: 0.91 - ETA: 1s - loss: 0.2427 - accuracy: 0.91 - ETA: 1s - loss: 0.2415 - accuracy: 0.91 - ETA: 1s - loss: 0.2439 - accuracy: 0.91 - ETA: 1s - loss: 0.2434 - accuracy: 0.91 - ETA: 1s - loss: 0.2434 - accuracy: 0.91 - ETA: 1s - loss: 0.2408 - accuracy: 0.91 - ETA: 1s - loss: 0.2408 - accuracy: 0.91 - ETA: 0s - loss: 0.2403 - accuracy: 0.91 - ETA: 0s - loss: 0.2421 - accuracy: 0.91 - ETA: 0s - loss: 0.2419 - accuracy: 0.91 - ETA: 0s - loss: 0.2416 - accuracy: 0.91 - ETA: 0s - loss: 0.2391 - accuracy: 0.91 - ETA: 0s - loss: 0.2383 - accuracy: 0.91 - ETA: 0s - loss: 0.2347 - accuracy: 0.91 - ETA: 0s - loss: 0.2350 - accuracy: 0.91 - ETA: 0s - loss: 0.2348 - accuracy: 0.91 - ETA: 0s - loss: 0.2333 - accuracy: 0.91 - ETA: 0s - loss: 0.2317 - accuracy: 0.92 - ETA: 0s - loss: 0.2311 - accuracy: 0.92 - ETA: 0s - loss: 0.2295 - accuracy: 0.92 - ETA: 0s - loss: 0.2304 - accuracy: 0.92 - ETA: 0s - loss: 0.2325 - accuracy: 0.91 - 5s 41ms/step - loss: 0.2312 - accuracy: 0.9188 - val_loss: 0.2732 - val_accuracy: 0.8970\n",
      "Epoch 8/1000\n",
      "110/110 [==============================] - ETA: 5s - loss: 0.2178 - accuracy: 0.90 - ETA: 4s - loss: 0.1628 - accuracy: 0.93 - ETA: 3s - loss: 0.1883 - accuracy: 0.93 - ETA: 3s - loss: 0.1974 - accuracy: 0.92 - ETA: 3s - loss: 0.2473 - accuracy: 0.91 - ETA: 3s - loss: 0.2374 - accuracy: 0.91 - ETA: 3s - loss: 0.2397 - accuracy: 0.90 - ETA: 3s - loss: 0.2466 - accuracy: 0.90 - ETA: 3s - loss: 0.2342 - accuracy: 0.91 - ETA: 3s - loss: 0.2303 - accuracy: 0.91 - ETA: 2s - loss: 0.2295 - accuracy: 0.91 - ETA: 2s - loss: 0.2218 - accuracy: 0.91 - ETA: 2s - loss: 0.2139 - accuracy: 0.92 - ETA: 2s - loss: 0.2133 - accuracy: 0.92 - ETA: 2s - loss: 0.2149 - accuracy: 0.92 - ETA: 2s - loss: 0.2101 - accuracy: 0.92 - ETA: 2s - loss: 0.2080 - accuracy: 0.92 - ETA: 2s - loss: 0.2051 - accuracy: 0.93 - ETA: 2s - loss: 0.2107 - accuracy: 0.92 - ETA: 2s - loss: 0.2087 - accuracy: 0.92 - ETA: 2s - loss: 0.2086 - accuracy: 0.92 - ETA: 2s - loss: 0.2101 - accuracy: 0.92 - ETA: 2s - loss: 0.2153 - accuracy: 0.92 - ETA: 2s - loss: 0.2145 - accuracy: 0.92 - ETA: 1s - loss: 0.2131 - accuracy: 0.92 - ETA: 1s - loss: 0.2165 - accuracy: 0.92 - ETA: 1s - loss: 0.2186 - accuracy: 0.92 - ETA: 1s - loss: 0.2160 - accuracy: 0.92 - ETA: 1s - loss: 0.2173 - accuracy: 0.92 - ETA: 1s - loss: 0.2202 - accuracy: 0.92 - ETA: 1s - loss: 0.2248 - accuracy: 0.92 - ETA: 1s - loss: 0.2225 - accuracy: 0.92 - ETA: 1s - loss: 0.2249 - accuracy: 0.92 - ETA: 1s - loss: 0.2287 - accuracy: 0.92 - ETA: 1s - loss: 0.2280 - accuracy: 0.92 - ETA: 1s - loss: 0.2295 - accuracy: 0.92 - ETA: 1s - loss: 0.2290 - accuracy: 0.92 - ETA: 1s - loss: 0.2286 - accuracy: 0.92 - ETA: 1s - loss: 0.2260 - accuracy: 0.92 - ETA: 1s - loss: 0.2259 - accuracy: 0.92 - ETA: 0s - loss: 0.2268 - accuracy: 0.92 - ETA: 0s - loss: 0.2307 - accuracy: 0.92 - ETA: 0s - loss: 0.2308 - accuracy: 0.92 - ETA: 0s - loss: 0.2325 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.92 - ETA: 0s - loss: 0.2286 - accuracy: 0.92 - ETA: 0s - loss: 0.2256 - accuracy: 0.92 - ETA: 0s - loss: 0.2256 - accuracy: 0.92 - ETA: 0s - loss: 0.2259 - accuracy: 0.92 - ETA: 0s - loss: 0.2250 - accuracy: 0.92 - ETA: 0s - loss: 0.2248 - accuracy: 0.92 - ETA: 0s - loss: 0.2253 - accuracy: 0.92 - ETA: 0s - loss: 0.2251 - accuracy: 0.92 - ETA: 0s - loss: 0.2255 - accuracy: 0.92 - ETA: 0s - loss: 0.2289 - accuracy: 0.92 - 4s 35ms/step - loss: 0.2277 - accuracy: 0.9202 - val_loss: 0.4469 - val_accuracy: 0.7803\n",
      "Epoch 9/1000\n",
      "110/110 [==============================] - ETA: 5s - loss: 0.1336 - accuracy: 0.93 - ETA: 4s - loss: 0.1321 - accuracy: 0.94 - ETA: 3s - loss: 0.1745 - accuracy: 0.93 - ETA: 3s - loss: 0.1833 - accuracy: 0.94 - ETA: 3s - loss: 0.2417 - accuracy: 0.92 - ETA: 3s - loss: 0.2354 - accuracy: 0.92 - ETA: 3s - loss: 0.2435 - accuracy: 0.91 - ETA: 3s - loss: 0.2444 - accuracy: 0.91 - ETA: 3s - loss: 0.2343 - accuracy: 0.92 - ETA: 3s - loss: 0.2288 - accuracy: 0.91 - ETA: 2s - loss: 0.2337 - accuracy: 0.91 - ETA: 2s - loss: 0.2270 - accuracy: 0.92 - ETA: 2s - loss: 0.2216 - accuracy: 0.92 - ETA: 2s - loss: 0.2202 - accuracy: 0.92 - ETA: 2s - loss: 0.2162 - accuracy: 0.92 - ETA: 2s - loss: 0.2091 - accuracy: 0.92 - ETA: 2s - loss: 0.2096 - accuracy: 0.93 - ETA: 2s - loss: 0.2040 - accuracy: 0.93 - ETA: 2s - loss: 0.2086 - accuracy: 0.93 - ETA: 2s - loss: 0.2031 - accuracy: 0.93 - ETA: 2s - loss: 0.2001 - accuracy: 0.93 - ETA: 2s - loss: 0.2012 - accuracy: 0.93 - ETA: 2s - loss: 0.2084 - accuracy: 0.93 - ETA: 2s - loss: 0.2061 - accuracy: 0.93 - ETA: 1s - loss: 0.2032 - accuracy: 0.93 - ETA: 1s - loss: 0.2036 - accuracy: 0.93 - ETA: 1s - loss: 0.2039 - accuracy: 0.93 - ETA: 1s - loss: 0.2014 - accuracy: 0.93 - ETA: 1s - loss: 0.2037 - accuracy: 0.93 - ETA: 1s - loss: 0.2061 - accuracy: 0.93 - ETA: 1s - loss: 0.2076 - accuracy: 0.92 - ETA: 1s - loss: 0.2056 - accuracy: 0.93 - ETA: 1s - loss: 0.2072 - accuracy: 0.92 - ETA: 1s - loss: 0.2118 - accuracy: 0.92 - ETA: 1s - loss: 0.2107 - accuracy: 0.92 - ETA: 1s - loss: 0.2120 - accuracy: 0.92 - ETA: 1s - loss: 0.2116 - accuracy: 0.93 - ETA: 1s - loss: 0.2124 - accuracy: 0.92 - ETA: 1s - loss: 0.2115 - accuracy: 0.93 - ETA: 1s - loss: 0.2114 - accuracy: 0.93 - ETA: 0s - loss: 0.2124 - accuracy: 0.92 - ETA: 0s - loss: 0.2153 - accuracy: 0.92 - ETA: 0s - loss: 0.2156 - accuracy: 0.92 - ETA: 0s - loss: 0.2171 - accuracy: 0.92 - ETA: 0s - loss: 0.2155 - accuracy: 0.92 - ETA: 0s - loss: 0.2166 - accuracy: 0.92 - ETA: 0s - loss: 0.2134 - accuracy: 0.92 - ETA: 0s - loss: 0.2142 - accuracy: 0.92 - ETA: 0s - loss: 0.2156 - accuracy: 0.92 - ETA: 0s - loss: 0.2154 - accuracy: 0.92 - ETA: 0s - loss: 0.2146 - accuracy: 0.92 - ETA: 0s - loss: 0.2149 - accuracy: 0.92 - ETA: 0s - loss: 0.2156 - accuracy: 0.92 - ETA: 0s - loss: 0.2165 - accuracy: 0.92 - ETA: 0s - loss: 0.2187 - accuracy: 0.92 - 4s 35ms/step - loss: 0.2175 - accuracy: 0.9234 - val_loss: 0.4242 - val_accuracy: 0.8066\n",
      "Epoch 10/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 5s - loss: 0.1945 - accuracy: 0.93 - ETA: 4s - loss: 0.1924 - accuracy: 0.92 - ETA: 3s - loss: 0.2027 - accuracy: 0.92 - ETA: 3s - loss: 0.2020 - accuracy: 0.92 - ETA: 3s - loss: 0.2297 - accuracy: 0.91 - ETA: 3s - loss: 0.2194 - accuracy: 0.92 - ETA: 3s - loss: 0.2277 - accuracy: 0.91 - ETA: 3s - loss: 0.2286 - accuracy: 0.91 - ETA: 3s - loss: 0.2214 - accuracy: 0.91 - ETA: 3s - loss: 0.2172 - accuracy: 0.91 - ETA: 2s - loss: 0.2120 - accuracy: 0.91 - ETA: 2s - loss: 0.2070 - accuracy: 0.92 - ETA: 2s - loss: 0.1992 - accuracy: 0.92 - ETA: 2s - loss: 0.1973 - accuracy: 0.92 - ETA: 2s - loss: 0.1958 - accuracy: 0.92 - ETA: 2s - loss: 0.1910 - accuracy: 0.93 - ETA: 2s - loss: 0.1940 - accuracy: 0.93 - ETA: 2s - loss: 0.1881 - accuracy: 0.93 - ETA: 2s - loss: 0.1928 - accuracy: 0.93 - ETA: 2s - loss: 0.1875 - accuracy: 0.93 - ETA: 2s - loss: 0.1854 - accuracy: 0.93 - ETA: 2s - loss: 0.1857 - accuracy: 0.93 - ETA: 2s - loss: 0.1973 - accuracy: 0.93 - ETA: 2s - loss: 0.1942 - accuracy: 0.93 - ETA: 2s - loss: 0.1933 - accuracy: 0.93 - ETA: 1s - loss: 0.1951 - accuracy: 0.93 - ETA: 1s - loss: 0.1961 - accuracy: 0.93 - ETA: 1s - loss: 0.1937 - accuracy: 0.93 - ETA: 1s - loss: 0.1967 - accuracy: 0.93 - ETA: 1s - loss: 0.1998 - accuracy: 0.93 - ETA: 1s - loss: 0.2007 - accuracy: 0.92 - ETA: 1s - loss: 0.1983 - accuracy: 0.93 - ETA: 1s - loss: 0.2032 - accuracy: 0.92 - ETA: 1s - loss: 0.2073 - accuracy: 0.92 - ETA: 1s - loss: 0.2070 - accuracy: 0.92 - ETA: 1s - loss: 0.2090 - accuracy: 0.92 - ETA: 1s - loss: 0.2080 - accuracy: 0.92 - ETA: 1s - loss: 0.2067 - accuracy: 0.92 - ETA: 1s - loss: 0.2073 - accuracy: 0.92 - ETA: 1s - loss: 0.2073 - accuracy: 0.92 - ETA: 0s - loss: 0.2063 - accuracy: 0.92 - ETA: 0s - loss: 0.2080 - accuracy: 0.92 - ETA: 0s - loss: 0.2074 - accuracy: 0.92 - ETA: 0s - loss: 0.2075 - accuracy: 0.92 - ETA: 0s - loss: 0.2055 - accuracy: 0.92 - ETA: 0s - loss: 0.2052 - accuracy: 0.92 - ETA: 0s - loss: 0.2021 - accuracy: 0.93 - ETA: 0s - loss: 0.2032 - accuracy: 0.92 - ETA: 0s - loss: 0.2038 - accuracy: 0.92 - ETA: 0s - loss: 0.2029 - accuracy: 0.92 - ETA: 0s - loss: 0.2016 - accuracy: 0.93 - ETA: 0s - loss: 0.2012 - accuracy: 0.93 - ETA: 0s - loss: 0.2015 - accuracy: 0.92 - ETA: 0s - loss: 0.2021 - accuracy: 0.92 - ETA: 0s - loss: 0.2043 - accuracy: 0.92 - 4s 35ms/step - loss: 0.2035 - accuracy: 0.9280 - val_loss: 0.5152 - val_accuracy: 0.7632\n",
      "Epoch 11/1000\n",
      "110/110 [==============================] - ETA: 5s - loss: 0.1594 - accuracy: 0.90 - ETA: 4s - loss: 0.1332 - accuracy: 0.94 - ETA: 3s - loss: 0.1866 - accuracy: 0.92 - ETA: 3s - loss: 0.1943 - accuracy: 0.91 - ETA: 3s - loss: 0.2139 - accuracy: 0.91 - ETA: 3s - loss: 0.2106 - accuracy: 0.91 - ETA: 3s - loss: 0.2176 - accuracy: 0.91 - ETA: 3s - loss: 0.2172 - accuracy: 0.91 - ETA: 3s - loss: 0.2135 - accuracy: 0.91 - ETA: 3s - loss: 0.2068 - accuracy: 0.91 - ETA: 2s - loss: 0.2055 - accuracy: 0.91 - ETA: 2s - loss: 0.2009 - accuracy: 0.92 - ETA: 2s - loss: 0.1911 - accuracy: 0.92 - ETA: 2s - loss: 0.1903 - accuracy: 0.92 - ETA: 2s - loss: 0.1924 - accuracy: 0.92 - ETA: 2s - loss: 0.1889 - accuracy: 0.92 - ETA: 2s - loss: 0.2006 - accuracy: 0.92 - ETA: 2s - loss: 0.2007 - accuracy: 0.92 - ETA: 2s - loss: 0.2044 - accuracy: 0.92 - ETA: 2s - loss: 0.1995 - accuracy: 0.92 - ETA: 2s - loss: 0.1987 - accuracy: 0.92 - ETA: 2s - loss: 0.2007 - accuracy: 0.92 - ETA: 2s - loss: 0.2052 - accuracy: 0.92 - ETA: 2s - loss: 0.2020 - accuracy: 0.92 - ETA: 2s - loss: 0.2010 - accuracy: 0.92 - ETA: 1s - loss: 0.2034 - accuracy: 0.92 - ETA: 1s - loss: 0.2042 - accuracy: 0.92 - ETA: 1s - loss: 0.2027 - accuracy: 0.92 - ETA: 1s - loss: 0.2033 - accuracy: 0.92 - ETA: 1s - loss: 0.2073 - accuracy: 0.92 - ETA: 1s - loss: 0.2109 - accuracy: 0.92 - ETA: 1s - loss: 0.2085 - accuracy: 0.92 - ETA: 1s - loss: 0.2110 - accuracy: 0.92 - ETA: 1s - loss: 0.2148 - accuracy: 0.92 - ETA: 1s - loss: 0.2132 - accuracy: 0.92 - ETA: 1s - loss: 0.2147 - accuracy: 0.92 - ETA: 1s - loss: 0.2134 - accuracy: 0.92 - ETA: 1s - loss: 0.2117 - accuracy: 0.92 - ETA: 1s - loss: 0.2096 - accuracy: 0.92 - ETA: 1s - loss: 0.2104 - accuracy: 0.92 - ETA: 0s - loss: 0.2100 - accuracy: 0.92 - ETA: 0s - loss: 0.2112 - accuracy: 0.92 - ETA: 0s - loss: 0.2105 - accuracy: 0.92 - ETA: 0s - loss: 0.2105 - accuracy: 0.92 - ETA: 0s - loss: 0.2089 - accuracy: 0.92 - ETA: 0s - loss: 0.2083 - accuracy: 0.92 - ETA: 0s - loss: 0.2052 - accuracy: 0.92 - ETA: 0s - loss: 0.2066 - accuracy: 0.92 - ETA: 0s - loss: 0.2072 - accuracy: 0.92 - ETA: 0s - loss: 0.2063 - accuracy: 0.92 - ETA: 0s - loss: 0.2051 - accuracy: 0.92 - ETA: 0s - loss: 0.2044 - accuracy: 0.92 - ETA: 0s - loss: 0.2035 - accuracy: 0.92 - ETA: 0s - loss: 0.2045 - accuracy: 0.92 - ETA: 0s - loss: 0.2065 - accuracy: 0.92 - 4s 35ms/step - loss: 0.2053 - accuracy: 0.9268 - val_loss: 0.3683 - val_accuracy: 0.8181\n",
      "Epoch 12/1000\n",
      "110/110 [==============================] - ETA: 5s - loss: 0.1618 - accuracy: 0.93 - ETA: 4s - loss: 0.1171 - accuracy: 0.95 - ETA: 3s - loss: 0.1826 - accuracy: 0.93 - ETA: 3s - loss: 0.1721 - accuracy: 0.94 - ETA: 3s - loss: 0.1984 - accuracy: 0.92 - ETA: 3s - loss: 0.2003 - accuracy: 0.92 - ETA: 3s - loss: 0.2078 - accuracy: 0.92 - ETA: 3s - loss: 0.2059 - accuracy: 0.92 - ETA: 3s - loss: 0.1966 - accuracy: 0.93 - ETA: 3s - loss: 0.1942 - accuracy: 0.93 - ETA: 2s - loss: 0.1866 - accuracy: 0.93 - ETA: 2s - loss: 0.1778 - accuracy: 0.93 - ETA: 2s - loss: 0.1732 - accuracy: 0.93 - ETA: 2s - loss: 0.1766 - accuracy: 0.93 - ETA: 2s - loss: 0.1758 - accuracy: 0.93 - ETA: 2s - loss: 0.1734 - accuracy: 0.93 - ETA: 2s - loss: 0.1755 - accuracy: 0.94 - ETA: 2s - loss: 0.1736 - accuracy: 0.94 - ETA: 2s - loss: 0.1743 - accuracy: 0.94 - ETA: 2s - loss: 0.1690 - accuracy: 0.94 - ETA: 2s - loss: 0.1683 - accuracy: 0.94 - ETA: 2s - loss: 0.1713 - accuracy: 0.94 - ETA: 2s - loss: 0.1763 - accuracy: 0.94 - ETA: 2s - loss: 0.1754 - accuracy: 0.94 - ETA: 1s - loss: 0.1750 - accuracy: 0.94 - ETA: 1s - loss: 0.1791 - accuracy: 0.93 - ETA: 1s - loss: 0.1832 - accuracy: 0.93 - ETA: 1s - loss: 0.1814 - accuracy: 0.93 - ETA: 1s - loss: 0.1832 - accuracy: 0.93 - ETA: 1s - loss: 0.1870 - accuracy: 0.93 - ETA: 1s - loss: 0.1897 - accuracy: 0.93 - ETA: 1s - loss: 0.1877 - accuracy: 0.93 - ETA: 1s - loss: 0.1888 - accuracy: 0.93 - ETA: 1s - loss: 0.1933 - accuracy: 0.93 - ETA: 1s - loss: 0.1930 - accuracy: 0.93 - ETA: 1s - loss: 0.1958 - accuracy: 0.93 - ETA: 1s - loss: 0.1957 - accuracy: 0.93 - ETA: 1s - loss: 0.1946 - accuracy: 0.93 - ETA: 1s - loss: 0.1939 - accuracy: 0.93 - ETA: 1s - loss: 0.1940 - accuracy: 0.93 - ETA: 0s - loss: 0.1935 - accuracy: 0.93 - ETA: 0s - loss: 0.1947 - accuracy: 0.93 - ETA: 0s - loss: 0.1942 - accuracy: 0.93 - ETA: 0s - loss: 0.1942 - accuracy: 0.93 - ETA: 0s - loss: 0.1926 - accuracy: 0.93 - ETA: 0s - loss: 0.1922 - accuracy: 0.93 - ETA: 0s - loss: 0.1892 - accuracy: 0.93 - ETA: 0s - loss: 0.1896 - accuracy: 0.93 - ETA: 0s - loss: 0.1920 - accuracy: 0.93 - ETA: 0s - loss: 0.1912 - accuracy: 0.93 - ETA: 0s - loss: 0.1904 - accuracy: 0.93 - ETA: 0s - loss: 0.1897 - accuracy: 0.93 - ETA: 0s - loss: 0.1890 - accuracy: 0.93 - ETA: 0s - loss: 0.1899 - accuracy: 0.93 - ETA: 0s - loss: 0.1919 - accuracy: 0.93 - 4s 35ms/step - loss: 0.1906 - accuracy: 0.9331 - val_loss: 0.3354 - val_accuracy: 0.8673\n",
      "Epoch 13/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 5s - loss: 0.1621 - accuracy: 0.93 - ETA: 4s - loss: 0.1063 - accuracy: 0.95 - ETA: 3s - loss: 0.1604 - accuracy: 0.94 - ETA: 3s - loss: 0.1614 - accuracy: 0.94 - ETA: 3s - loss: 0.1924 - accuracy: 0.93 - ETA: 3s - loss: 0.1939 - accuracy: 0.92 - ETA: 3s - loss: 0.1980 - accuracy: 0.92 - ETA: 3s - loss: 0.1971 - accuracy: 0.92 - ETA: 3s - loss: 0.1950 - accuracy: 0.92 - ETA: 3s - loss: 0.1903 - accuracy: 0.92 - ETA: 2s - loss: 0.1852 - accuracy: 0.93 - ETA: 2s - loss: 0.1773 - accuracy: 0.93 - ETA: 2s - loss: 0.1673 - accuracy: 0.93 - ETA: 2s - loss: 0.1662 - accuracy: 0.93 - ETA: 2s - loss: 0.1646 - accuracy: 0.94 - ETA: 2s - loss: 0.1615 - accuracy: 0.94 - ETA: 2s - loss: 0.1635 - accuracy: 0.94 - ETA: 2s - loss: 0.1618 - accuracy: 0.94 - ETA: 2s - loss: 0.1611 - accuracy: 0.94 - ETA: 2s - loss: 0.1576 - accuracy: 0.94 - ETA: 2s - loss: 0.1548 - accuracy: 0.94 - ETA: 2s - loss: 0.1606 - accuracy: 0.94 - ETA: 2s - loss: 0.1683 - accuracy: 0.94 - ETA: 2s - loss: 0.1670 - accuracy: 0.94 - ETA: 2s - loss: 0.1695 - accuracy: 0.94 - ETA: 1s - loss: 0.1772 - accuracy: 0.93 - ETA: 1s - loss: 0.1808 - accuracy: 0.93 - ETA: 1s - loss: 0.1797 - accuracy: 0.93 - ETA: 1s - loss: 0.1823 - accuracy: 0.93 - ETA: 1s - loss: 0.1850 - accuracy: 0.93 - ETA: 1s - loss: 0.1892 - accuracy: 0.93 - ETA: 1s - loss: 0.1881 - accuracy: 0.93 - ETA: 1s - loss: 0.1906 - accuracy: 0.92 - ETA: 1s - loss: 0.1955 - accuracy: 0.92 - ETA: 1s - loss: 0.1950 - accuracy: 0.92 - ETA: 1s - loss: 0.1982 - accuracy: 0.92 - ETA: 1s - loss: 0.1978 - accuracy: 0.92 - ETA: 1s - loss: 0.1976 - accuracy: 0.92 - ETA: 1s - loss: 0.1957 - accuracy: 0.92 - ETA: 1s - loss: 0.1965 - accuracy: 0.92 - ETA: 0s - loss: 0.1962 - accuracy: 0.92 - ETA: 0s - loss: 0.1987 - accuracy: 0.92 - ETA: 0s - loss: 0.1977 - accuracy: 0.92 - ETA: 0s - loss: 0.1964 - accuracy: 0.92 - ETA: 0s - loss: 0.1957 - accuracy: 0.93 - ETA: 0s - loss: 0.1958 - accuracy: 0.93 - ETA: 0s - loss: 0.1929 - accuracy: 0.93 - ETA: 0s - loss: 0.1931 - accuracy: 0.93 - ETA: 0s - loss: 0.1956 - accuracy: 0.93 - ETA: 0s - loss: 0.1953 - accuracy: 0.93 - ETA: 0s - loss: 0.1954 - accuracy: 0.93 - ETA: 0s - loss: 0.1964 - accuracy: 0.92 - ETA: 0s - loss: 0.1970 - accuracy: 0.92 - ETA: 0s - loss: 0.2015 - accuracy: 0.92 - ETA: 0s - loss: 0.2042 - accuracy: 0.92 - 4s 35ms/step - loss: 0.2035 - accuracy: 0.9265 - val_loss: 0.2962 - val_accuracy: 0.8799\n",
      "Epoch 14/1000\n",
      "110/110 [==============================] - ETA: 5s - loss: 0.1180 - accuracy: 0.96 - ETA: 4s - loss: 0.1488 - accuracy: 0.93 - ETA: 3s - loss: 0.1855 - accuracy: 0.93 - ETA: 3s - loss: 0.1903 - accuracy: 0.93 - ETA: 3s - loss: 0.2385 - accuracy: 0.91 - ETA: 3s - loss: 0.2221 - accuracy: 0.92 - ETA: 3s - loss: 0.2226 - accuracy: 0.92 - ETA: 3s - loss: 0.2247 - accuracy: 0.92 - ETA: 3s - loss: 0.2152 - accuracy: 0.92 - ETA: 3s - loss: 0.2127 - accuracy: 0.92 - ETA: 3s - loss: 0.2141 - accuracy: 0.92 - ETA: 2s - loss: 0.2064 - accuracy: 0.92 - ETA: 2s - loss: 0.1978 - accuracy: 0.93 - ETA: 2s - loss: 0.1967 - accuracy: 0.93 - ETA: 2s - loss: 0.1920 - accuracy: 0.93 - ETA: 2s - loss: 0.1873 - accuracy: 0.93 - ETA: 2s - loss: 0.1884 - accuracy: 0.93 - ETA: 2s - loss: 0.1840 - accuracy: 0.93 - ETA: 2s - loss: 0.1868 - accuracy: 0.93 - ETA: 2s - loss: 0.1848 - accuracy: 0.93 - ETA: 2s - loss: 0.1798 - accuracy: 0.93 - ETA: 2s - loss: 0.1818 - accuracy: 0.93 - ETA: 2s - loss: 0.1876 - accuracy: 0.93 - ETA: 2s - loss: 0.1843 - accuracy: 0.93 - ETA: 2s - loss: 0.1842 - accuracy: 0.93 - ETA: 1s - loss: 0.1841 - accuracy: 0.93 - ETA: 1s - loss: 0.1859 - accuracy: 0.93 - ETA: 1s - loss: 0.1832 - accuracy: 0.93 - ETA: 1s - loss: 0.1854 - accuracy: 0.93 - ETA: 1s - loss: 0.1878 - accuracy: 0.93 - ETA: 1s - loss: 0.1882 - accuracy: 0.93 - ETA: 1s - loss: 0.1867 - accuracy: 0.93 - ETA: 1s - loss: 0.1893 - accuracy: 0.93 - ETA: 1s - loss: 0.1928 - accuracy: 0.93 - ETA: 1s - loss: 0.1924 - accuracy: 0.93 - ETA: 1s - loss: 0.1946 - accuracy: 0.93 - ETA: 1s - loss: 0.1948 - accuracy: 0.93 - ETA: 1s - loss: 0.1945 - accuracy: 0.93 - ETA: 1s - loss: 0.1940 - accuracy: 0.93 - ETA: 1s - loss: 0.1944 - accuracy: 0.93 - ETA: 0s - loss: 0.1936 - accuracy: 0.93 - ETA: 0s - loss: 0.1947 - accuracy: 0.93 - ETA: 0s - loss: 0.1934 - accuracy: 0.93 - ETA: 0s - loss: 0.1939 - accuracy: 0.93 - ETA: 0s - loss: 0.1920 - accuracy: 0.93 - ETA: 0s - loss: 0.1919 - accuracy: 0.93 - ETA: 0s - loss: 0.1890 - accuracy: 0.93 - ETA: 0s - loss: 0.1895 - accuracy: 0.93 - ETA: 0s - loss: 0.1899 - accuracy: 0.93 - ETA: 0s - loss: 0.1887 - accuracy: 0.93 - ETA: 0s - loss: 0.1882 - accuracy: 0.93 - ETA: 0s - loss: 0.1872 - accuracy: 0.93 - ETA: 0s - loss: 0.1861 - accuracy: 0.93 - ETA: 0s - loss: 0.1871 - accuracy: 0.93 - ETA: 0s - loss: 0.1906 - accuracy: 0.93 - 4s 35ms/step - loss: 0.1896 - accuracy: 0.9317 - val_loss: 0.4562 - val_accuracy: 0.7941\n",
      "Epoch 15/1000\n",
      "110/110 [==============================] - ETA: 5s - loss: 0.1705 - accuracy: 0.93 - ETA: 4s - loss: 0.1244 - accuracy: 0.95 - ETA: 3s - loss: 0.1765 - accuracy: 0.95 - ETA: 3s - loss: 0.1737 - accuracy: 0.95 - ETA: 3s - loss: 0.2009 - accuracy: 0.93 - ETA: 3s - loss: 0.1923 - accuracy: 0.93 - ETA: 3s - loss: 0.1925 - accuracy: 0.93 - ETA: 3s - loss: 0.1970 - accuracy: 0.93 - ETA: 3s - loss: 0.1913 - accuracy: 0.93 - ETA: 3s - loss: 0.1939 - accuracy: 0.93 - ETA: 2s - loss: 0.1900 - accuracy: 0.93 - ETA: 2s - loss: 0.1831 - accuracy: 0.94 - ETA: 2s - loss: 0.1764 - accuracy: 0.94 - ETA: 2s - loss: 0.1789 - accuracy: 0.94 - ETA: 2s - loss: 0.1792 - accuracy: 0.94 - ETA: 2s - loss: 0.1747 - accuracy: 0.94 - ETA: 2s - loss: 0.1757 - accuracy: 0.94 - ETA: 2s - loss: 0.1734 - accuracy: 0.94 - ETA: 2s - loss: 0.1732 - accuracy: 0.94 - ETA: 2s - loss: 0.1692 - accuracy: 0.94 - ETA: 2s - loss: 0.1703 - accuracy: 0.94 - ETA: 2s - loss: 0.1733 - accuracy: 0.94 - ETA: 2s - loss: 0.1792 - accuracy: 0.94 - ETA: 2s - loss: 0.1763 - accuracy: 0.94 - ETA: 2s - loss: 0.1768 - accuracy: 0.94 - ETA: 1s - loss: 0.1806 - accuracy: 0.94 - ETA: 1s - loss: 0.1839 - accuracy: 0.93 - ETA: 1s - loss: 0.1820 - accuracy: 0.94 - ETA: 1s - loss: 0.1832 - accuracy: 0.93 - ETA: 1s - loss: 0.1862 - accuracy: 0.93 - ETA: 1s - loss: 0.1881 - accuracy: 0.93 - ETA: 1s - loss: 0.1869 - accuracy: 0.93 - ETA: 1s - loss: 0.1879 - accuracy: 0.93 - ETA: 1s - loss: 0.1913 - accuracy: 0.93 - ETA: 1s - loss: 0.1896 - accuracy: 0.93 - ETA: 1s - loss: 0.1914 - accuracy: 0.93 - ETA: 1s - loss: 0.1906 - accuracy: 0.93 - ETA: 1s - loss: 0.1894 - accuracy: 0.93 - ETA: 1s - loss: 0.1882 - accuracy: 0.93 - ETA: 1s - loss: 0.1889 - accuracy: 0.93 - ETA: 0s - loss: 0.1889 - accuracy: 0.93 - ETA: 0s - loss: 0.1902 - accuracy: 0.93 - ETA: 0s - loss: 0.1903 - accuracy: 0.93 - ETA: 0s - loss: 0.1900 - accuracy: 0.93 - ETA: 0s - loss: 0.1885 - accuracy: 0.93 - ETA: 0s - loss: 0.1872 - accuracy: 0.93 - ETA: 0s - loss: 0.1841 - accuracy: 0.94 - ETA: 0s - loss: 0.1849 - accuracy: 0.94 - ETA: 0s - loss: 0.1870 - accuracy: 0.93 - ETA: 0s - loss: 0.1863 - accuracy: 0.93 - ETA: 0s - loss: 0.1858 - accuracy: 0.93 - ETA: 0s - loss: 0.1850 - accuracy: 0.93 - ETA: 0s - loss: 0.1847 - accuracy: 0.93 - ETA: 0s - loss: 0.1858 - accuracy: 0.93 - ETA: 0s - loss: 0.1879 - accuracy: 0.93 - 4s 35ms/step - loss: 0.1867 - accuracy: 0.9383 - val_loss: 0.3653 - val_accuracy: 0.8432\n",
      "Epoch 16/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 5s - loss: 0.2014 - accuracy: 0.90 - ETA: 4s - loss: 0.1303 - accuracy: 0.95 - ETA: 3s - loss: 0.1716 - accuracy: 0.94 - ETA: 3s - loss: 0.1715 - accuracy: 0.94 - ETA: 3s - loss: 0.1938 - accuracy: 0.93 - ETA: 3s - loss: 0.1861 - accuracy: 0.93 - ETA: 3s - loss: 0.1874 - accuracy: 0.92 - ETA: 3s - loss: 0.1860 - accuracy: 0.92 - ETA: 3s - loss: 0.1784 - accuracy: 0.93 - ETA: 3s - loss: 0.1757 - accuracy: 0.93 - ETA: 3s - loss: 0.1742 - accuracy: 0.93 - ETA: 2s - loss: 0.1678 - accuracy: 0.93 - ETA: 2s - loss: 0.1597 - accuracy: 0.93 - ETA: 2s - loss: 0.1598 - accuracy: 0.93 - ETA: 2s - loss: 0.1574 - accuracy: 0.94 - ETA: 2s - loss: 0.1529 - accuracy: 0.94 - ETA: 2s - loss: 0.1551 - accuracy: 0.94 - ETA: 2s - loss: 0.1543 - accuracy: 0.94 - ETA: 2s - loss: 0.1537 - accuracy: 0.94 - ETA: 2s - loss: 0.1499 - accuracy: 0.94 - ETA: 2s - loss: 0.1516 - accuracy: 0.94 - ETA: 2s - loss: 0.1526 - accuracy: 0.94 - ETA: 2s - loss: 0.1616 - accuracy: 0.94 - ETA: 2s - loss: 0.1606 - accuracy: 0.94 - ETA: 2s - loss: 0.1608 - accuracy: 0.94 - ETA: 1s - loss: 0.1640 - accuracy: 0.94 - ETA: 1s - loss: 0.1656 - accuracy: 0.94 - ETA: 1s - loss: 0.1642 - accuracy: 0.94 - ETA: 1s - loss: 0.1660 - accuracy: 0.94 - ETA: 1s - loss: 0.1689 - accuracy: 0.94 - ETA: 1s - loss: 0.1708 - accuracy: 0.93 - ETA: 1s - loss: 0.1688 - accuracy: 0.94 - ETA: 1s - loss: 0.1719 - accuracy: 0.93 - ETA: 1s - loss: 0.1750 - accuracy: 0.93 - ETA: 1s - loss: 0.1736 - accuracy: 0.93 - ETA: 1s - loss: 0.1762 - accuracy: 0.93 - ETA: 1s - loss: 0.1765 - accuracy: 0.93 - ETA: 1s - loss: 0.1755 - accuracy: 0.93 - ETA: 1s - loss: 0.1745 - accuracy: 0.93 - ETA: 1s - loss: 0.1744 - accuracy: 0.93 - ETA: 0s - loss: 0.1742 - accuracy: 0.93 - ETA: 0s - loss: 0.1758 - accuracy: 0.93 - ETA: 0s - loss: 0.1749 - accuracy: 0.93 - ETA: 0s - loss: 0.1749 - accuracy: 0.93 - ETA: 0s - loss: 0.1733 - accuracy: 0.93 - ETA: 0s - loss: 0.1726 - accuracy: 0.93 - ETA: 0s - loss: 0.1696 - accuracy: 0.94 - ETA: 0s - loss: 0.1697 - accuracy: 0.94 - ETA: 0s - loss: 0.1723 - accuracy: 0.93 - ETA: 0s - loss: 0.1730 - accuracy: 0.93 - ETA: 0s - loss: 0.1740 - accuracy: 0.93 - ETA: 0s - loss: 0.1746 - accuracy: 0.93 - ETA: 0s - loss: 0.1769 - accuracy: 0.93 - ETA: 0s - loss: 0.1793 - accuracy: 0.93 - ETA: 0s - loss: 0.1818 - accuracy: 0.93 - 4s 35ms/step - loss: 0.1811 - accuracy: 0.9354 - val_loss: 0.4352 - val_accuracy: 0.8478\n",
      "Epoch 17/1000\n",
      "110/110 [==============================] - ETA: 5s - loss: 0.1806 - accuracy: 0.93 - ETA: 4s - loss: 0.1376 - accuracy: 0.95 - ETA: 3s - loss: 0.1713 - accuracy: 0.95 - ETA: 3s - loss: 0.1652 - accuracy: 0.95 - ETA: 3s - loss: 0.2013 - accuracy: 0.94 - ETA: 3s - loss: 0.1918 - accuracy: 0.94 - ETA: 3s - loss: 0.1974 - accuracy: 0.93 - ETA: 3s - loss: 0.1992 - accuracy: 0.93 - ETA: 3s - loss: 0.1972 - accuracy: 0.93 - ETA: 3s - loss: 0.1921 - accuracy: 0.93 - ETA: 2s - loss: 0.1975 - accuracy: 0.93 - ETA: 2s - loss: 0.1910 - accuracy: 0.93 - ETA: 2s - loss: 0.1870 - accuracy: 0.93 - ETA: 2s - loss: 0.1884 - accuracy: 0.93 - ETA: 2s - loss: 0.1832 - accuracy: 0.93 - ETA: 2s - loss: 0.1778 - accuracy: 0.93 - ETA: 2s - loss: 0.1766 - accuracy: 0.93 - ETA: 2s - loss: 0.1711 - accuracy: 0.94 - ETA: 2s - loss: 0.1657 - accuracy: 0.94 - ETA: 2s - loss: 0.1630 - accuracy: 0.94 - ETA: 2s - loss: 0.1635 - accuracy: 0.94 - ETA: 2s - loss: 0.1626 - accuracy: 0.94 - ETA: 2s - loss: 0.1740 - accuracy: 0.93 - ETA: 2s - loss: 0.1730 - accuracy: 0.93 - ETA: 2s - loss: 0.1724 - accuracy: 0.94 - ETA: 1s - loss: 0.1727 - accuracy: 0.93 - ETA: 1s - loss: 0.1735 - accuracy: 0.93 - ETA: 1s - loss: 0.1712 - accuracy: 0.94 - ETA: 1s - loss: 0.1754 - accuracy: 0.93 - ETA: 1s - loss: 0.1777 - accuracy: 0.93 - ETA: 1s - loss: 0.1785 - accuracy: 0.93 - ETA: 1s - loss: 0.1764 - accuracy: 0.93 - ETA: 1s - loss: 0.1778 - accuracy: 0.93 - ETA: 1s - loss: 0.1807 - accuracy: 0.93 - ETA: 1s - loss: 0.1797 - accuracy: 0.93 - ETA: 1s - loss: 0.1811 - accuracy: 0.93 - ETA: 1s - loss: 0.1811 - accuracy: 0.93 - ETA: 1s - loss: 0.1799 - accuracy: 0.93 - ETA: 1s - loss: 0.1787 - accuracy: 0.93 - ETA: 1s - loss: 0.1789 - accuracy: 0.93 - ETA: 0s - loss: 0.1777 - accuracy: 0.93 - ETA: 0s - loss: 0.1791 - accuracy: 0.93 - ETA: 0s - loss: 0.1789 - accuracy: 0.93 - ETA: 0s - loss: 0.1784 - accuracy: 0.93 - ETA: 0s - loss: 0.1769 - accuracy: 0.93 - ETA: 0s - loss: 0.1771 - accuracy: 0.93 - ETA: 0s - loss: 0.1748 - accuracy: 0.93 - ETA: 0s - loss: 0.1747 - accuracy: 0.93 - ETA: 0s - loss: 0.1756 - accuracy: 0.93 - ETA: 0s - loss: 0.1750 - accuracy: 0.93 - ETA: 0s - loss: 0.1742 - accuracy: 0.93 - ETA: 0s - loss: 0.1734 - accuracy: 0.93 - ETA: 0s - loss: 0.1731 - accuracy: 0.93 - ETA: 0s - loss: 0.1746 - accuracy: 0.93 - ETA: 0s - loss: 0.1769 - accuracy: 0.93 - 4s 35ms/step - loss: 0.1755 - accuracy: 0.9371 - val_loss: 0.4074 - val_accuracy: 0.8169\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 2554386fdb8644c03d0b9c6307085b14</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.2731662334075996</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-classification_head_1/dropout_rate: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dense_block_1/dropout_rate: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_block_1/num_layers: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dense_block_1/units_0: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_block_1/units_1: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dense_block_1/use_batchnorm: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/augment: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/block_type: resnet</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/normalize: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/res_net_block_1/conv3_depth: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/res_net_block_1/conv4_depth: 6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/res_net_block_1/pooling: avg</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/res_net_block_1/version: v2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-optimizer: adam</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 110 steps, validate for 28 steps\n",
      "Epoch 1/1000\n",
      "110/110 [==============================] - ETA: 41s - loss: 0.7027 - accuracy: 0.468 - ETA: 2s - loss: 0.6293 - accuracy: 0.589 - ETA: 1s - loss: 0.5947 - accuracy: 0.64 - ETA: 0s - loss: 0.5784 - accuracy: 0.66 - ETA: 0s - loss: 0.5833 - accuracy: 0.66 - ETA: 0s - loss: 0.5779 - accuracy: 0.67 - ETA: 0s - loss: 0.5752 - accuracy: 0.68 - ETA: 0s - loss: 0.5726 - accuracy: 0.68 - 1s 9ms/step - loss: 0.5717 - accuracy: 0.6790 - val_loss: 0.5634 - val_accuracy: 0.6819\n",
      "Epoch 2/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.5312 - accuracy: 0.65 - ETA: 0s - loss: 0.5624 - accuracy: 0.68 - ETA: 0s - loss: 0.5424 - accuracy: 0.70 - ETA: 0s - loss: 0.5477 - accuracy: 0.70 - ETA: 0s - loss: 0.5435 - accuracy: 0.71 - ETA: 0s - loss: 0.5392 - accuracy: 0.71 - ETA: 0s - loss: 0.5383 - accuracy: 0.71 - ETA: 0s - loss: 0.5342 - accuracy: 0.72 - 0s 5ms/step - loss: 0.5339 - accuracy: 0.7244 - val_loss: 0.5334 - val_accuracy: 0.7094\n",
      "Epoch 3/1000\n",
      "110/110 [==============================] - ETA: 3s - loss: 0.5133 - accuracy: 0.68 - ETA: 0s - loss: 0.5177 - accuracy: 0.73 - ETA: 0s - loss: 0.4835 - accuracy: 0.76 - ETA: 0s - loss: 0.4843 - accuracy: 0.76 - ETA: 0s - loss: 0.4838 - accuracy: 0.76 - ETA: 0s - loss: 0.4796 - accuracy: 0.76 - ETA: 0s - loss: 0.4819 - accuracy: 0.77 - ETA: 0s - loss: 0.4778 - accuracy: 0.77 - 1s 5ms/step - loss: 0.4772 - accuracy: 0.7744 - val_loss: 0.4788 - val_accuracy: 0.7620\n",
      "Epoch 4/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.4094 - accuracy: 0.71 - ETA: 0s - loss: 0.4650 - accuracy: 0.78 - ETA: 0s - loss: 0.4291 - accuracy: 0.80 - ETA: 0s - loss: 0.4244 - accuracy: 0.81 - ETA: 0s - loss: 0.4269 - accuracy: 0.81 - ETA: 0s - loss: 0.4288 - accuracy: 0.80 - ETA: 0s - loss: 0.4317 - accuracy: 0.80 - ETA: 0s - loss: 0.4315 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4308 - accuracy: 0.8105 - val_loss: 0.4413 - val_accuracy: 0.7941\n",
      "Epoch 5/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.4521 - accuracy: 0.84 - ETA: 0s - loss: 0.4132 - accuracy: 0.83 - ETA: 0s - loss: 0.3839 - accuracy: 0.84 - ETA: 0s - loss: 0.3873 - accuracy: 0.83 - ETA: 0s - loss: 0.3892 - accuracy: 0.83 - ETA: 0s - loss: 0.3895 - accuracy: 0.83 - ETA: 0s - loss: 0.3933 - accuracy: 0.83 - ETA: 0s - loss: 0.3923 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3934 - accuracy: 0.8353 - val_loss: 0.4344 - val_accuracy: 0.8078\n",
      "Epoch 6/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.4136 - accuracy: 0.90 - ETA: 0s - loss: 0.4005 - accuracy: 0.85 - ETA: 0s - loss: 0.3792 - accuracy: 0.86 - ETA: 0s - loss: 0.3787 - accuracy: 0.85 - ETA: 0s - loss: 0.3803 - accuracy: 0.84 - ETA: 0s - loss: 0.3790 - accuracy: 0.84 - ETA: 0s - loss: 0.3778 - accuracy: 0.84 - ETA: 0s - loss: 0.3728 - accuracy: 0.85 - 1s 5ms/step - loss: 0.3743 - accuracy: 0.8528 - val_loss: 0.3933 - val_accuracy: 0.8330\n",
      "Epoch 7/1000\n",
      "110/110 [==============================] - ETA: 3s - loss: 0.4671 - accuracy: 0.87 - ETA: 0s - loss: 0.3761 - accuracy: 0.86 - ETA: 0s - loss: 0.3438 - accuracy: 0.87 - ETA: 0s - loss: 0.3405 - accuracy: 0.87 - ETA: 0s - loss: 0.3437 - accuracy: 0.87 - ETA: 0s - loss: 0.3502 - accuracy: 0.86 - ETA: 0s - loss: 0.3518 - accuracy: 0.86 - ETA: 0s - loss: 0.3515 - accuracy: 0.86 - 1s 5ms/step - loss: 0.3540 - accuracy: 0.8631 - val_loss: 0.3667 - val_accuracy: 0.8547\n",
      "Epoch 8/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3667 - accuracy: 0.90 - ETA: 0s - loss: 0.3557 - accuracy: 0.88 - ETA: 0s - loss: 0.3327 - accuracy: 0.88 - ETA: 0s - loss: 0.3326 - accuracy: 0.88 - ETA: 0s - loss: 0.3368 - accuracy: 0.87 - ETA: 0s - loss: 0.3351 - accuracy: 0.87 - ETA: 0s - loss: 0.3353 - accuracy: 0.87 - ETA: 0s - loss: 0.3352 - accuracy: 0.87 - 1s 5ms/step - loss: 0.3362 - accuracy: 0.8714 - val_loss: 0.3464 - val_accuracy: 0.8673\n",
      "Epoch 9/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2157 - accuracy: 0.96 - ETA: 0s - loss: 0.3277 - accuracy: 0.89 - ETA: 0s - loss: 0.3169 - accuracy: 0.89 - ETA: 0s - loss: 0.3131 - accuracy: 0.88 - ETA: 0s - loss: 0.3139 - accuracy: 0.88 - ETA: 0s - loss: 0.3170 - accuracy: 0.88 - ETA: 0s - loss: 0.3193 - accuracy: 0.88 - ETA: 0s - loss: 0.3175 - accuracy: 0.88 - 1s 5ms/step - loss: 0.3211 - accuracy: 0.8814 - val_loss: 0.3282 - val_accuracy: 0.8822\n",
      "Epoch 10/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3226 - accuracy: 0.90 - ETA: 0s - loss: 0.3349 - accuracy: 0.86 - ETA: 0s - loss: 0.3036 - accuracy: 0.88 - ETA: 0s - loss: 0.3086 - accuracy: 0.87 - ETA: 0s - loss: 0.3127 - accuracy: 0.87 - ETA: 0s - loss: 0.3094 - accuracy: 0.88 - ETA: 0s - loss: 0.3112 - accuracy: 0.88 - ETA: 0s - loss: 0.3113 - accuracy: 0.88 - 1s 5ms/step - loss: 0.3135 - accuracy: 0.8808 - val_loss: 0.3246 - val_accuracy: 0.8902\n",
      "Epoch 11/1000\n",
      "110/110 [==============================] - ETA: 3s - loss: 0.2274 - accuracy: 0.90 - ETA: 0s - loss: 0.3055 - accuracy: 0.88 - ETA: 0s - loss: 0.2968 - accuracy: 0.89 - ETA: 0s - loss: 0.2944 - accuracy: 0.89 - ETA: 0s - loss: 0.3006 - accuracy: 0.89 - ETA: 0s - loss: 0.3039 - accuracy: 0.88 - ETA: 0s - loss: 0.3019 - accuracy: 0.89 - ETA: 0s - loss: 0.3010 - accuracy: 0.88 - 1s 5ms/step - loss: 0.3038 - accuracy: 0.8882 - val_loss: 0.3154 - val_accuracy: 0.8913\n",
      "Epoch 12/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3127 - accuracy: 0.93 - ETA: 0s - loss: 0.3287 - accuracy: 0.89 - ETA: 0s - loss: 0.3013 - accuracy: 0.89 - ETA: 0s - loss: 0.2969 - accuracy: 0.89 - ETA: 0s - loss: 0.2955 - accuracy: 0.89 - ETA: 0s - loss: 0.2948 - accuracy: 0.89 - ETA: 0s - loss: 0.2939 - accuracy: 0.89 - ETA: 0s - loss: 0.2921 - accuracy: 0.89 - 1s 5ms/step - loss: 0.2944 - accuracy: 0.8928 - val_loss: 0.3060 - val_accuracy: 0.8947\n",
      "Epoch 13/1000\n",
      "110/110 [==============================] - ETA: 3s - loss: 0.2555 - accuracy: 0.93 - ETA: 0s - loss: 0.2836 - accuracy: 0.90 - ETA: 0s - loss: 0.2721 - accuracy: 0.90 - ETA: 0s - loss: 0.2741 - accuracy: 0.90 - ETA: 0s - loss: 0.2740 - accuracy: 0.90 - ETA: 0s - loss: 0.2742 - accuracy: 0.90 - ETA: 0s - loss: 0.2774 - accuracy: 0.89 - ETA: 0s - loss: 0.2807 - accuracy: 0.89 - 1s 5ms/step - loss: 0.2835 - accuracy: 0.8948 - val_loss: 0.3040 - val_accuracy: 0.9027\n",
      "Epoch 14/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3116 - accuracy: 0.87 - ETA: 0s - loss: 0.3043 - accuracy: 0.89 - ETA: 0s - loss: 0.2788 - accuracy: 0.89 - ETA: 0s - loss: 0.2732 - accuracy: 0.90 - ETA: 0s - loss: 0.2745 - accuracy: 0.90 - ETA: 0s - loss: 0.2823 - accuracy: 0.89 - ETA: 0s - loss: 0.2808 - accuracy: 0.90 - ETA: 0s - loss: 0.2831 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2836 - accuracy: 0.8979 - val_loss: 0.2958 - val_accuracy: 0.8993\n",
      "Epoch 15/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2780 - accuracy: 0.93 - ETA: 0s - loss: 0.2906 - accuracy: 0.90 - ETA: 0s - loss: 0.2667 - accuracy: 0.90 - ETA: 0s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2707 - accuracy: 0.90 - ETA: 0s - loss: 0.2717 - accuracy: 0.90 - ETA: 0s - loss: 0.2722 - accuracy: 0.90 - ETA: 0s - loss: 0.2736 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2784 - accuracy: 0.9002 - val_loss: 0.2980 - val_accuracy: 0.8959\n",
      "Epoch 16/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2682 - accuracy: 0.90 - ETA: 0s - loss: 0.2691 - accuracy: 0.91 - ETA: 0s - loss: 0.2492 - accuracy: 0.91 - ETA: 0s - loss: 0.2452 - accuracy: 0.91 - ETA: 0s - loss: 0.2474 - accuracy: 0.91 - ETA: 0s - loss: 0.2517 - accuracy: 0.90 - ETA: 0s - loss: 0.2515 - accuracy: 0.90 - ETA: 0s - loss: 0.2577 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2582 - accuracy: 0.9057 - val_loss: 0.2957 - val_accuracy: 0.9085\n",
      "Epoch 17/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3435 - accuracy: 0.90 - ETA: 0s - loss: 0.2845 - accuracy: 0.89 - ETA: 0s - loss: 0.2578 - accuracy: 0.90 - ETA: 0s - loss: 0.2585 - accuracy: 0.90 - ETA: 0s - loss: 0.2610 - accuracy: 0.90 - ETA: 0s - loss: 0.2612 - accuracy: 0.90 - ETA: 0s - loss: 0.2618 - accuracy: 0.91 - ETA: 0s - loss: 0.2643 - accuracy: 0.90 - 0s 5ms/step - loss: 0.2660 - accuracy: 0.9079 - val_loss: 0.2877 - val_accuracy: 0.9142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3311 - accuracy: 0.87 - ETA: 0s - loss: 0.2735 - accuracy: 0.89 - ETA: 0s - loss: 0.2530 - accuracy: 0.90 - ETA: 0s - loss: 0.2441 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.90 - ETA: 0s - loss: 0.2537 - accuracy: 0.90 - ETA: 0s - loss: 0.2510 - accuracy: 0.90 - ETA: 0s - loss: 0.2599 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2609 - accuracy: 0.9042 - val_loss: 0.2878 - val_accuracy: 0.9085\n",
      "Epoch 19/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2861 - accuracy: 0.87 - ETA: 0s - loss: 0.2698 - accuracy: 0.91 - ETA: 0s - loss: 0.2555 - accuracy: 0.91 - ETA: 0s - loss: 0.2457 - accuracy: 0.91 - ETA: 0s - loss: 0.2469 - accuracy: 0.91 - ETA: 0s - loss: 0.2458 - accuracy: 0.91 - ETA: 0s - loss: 0.2458 - accuracy: 0.91 - ETA: 0s - loss: 0.2540 - accuracy: 0.91 - 0s 5ms/step - loss: 0.2543 - accuracy: 0.9114 - val_loss: 0.2748 - val_accuracy: 0.9108\n",
      "Epoch 20/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1993 - accuracy: 0.93 - ETA: 0s - loss: 0.2491 - accuracy: 0.92 - ETA: 0s - loss: 0.2369 - accuracy: 0.92 - ETA: 0s - loss: 0.2334 - accuracy: 0.92 - ETA: 0s - loss: 0.2364 - accuracy: 0.92 - ETA: 0s - loss: 0.2402 - accuracy: 0.91 - ETA: 0s - loss: 0.2420 - accuracy: 0.91 - ETA: 0s - loss: 0.2422 - accuracy: 0.91 - 0s 4ms/step - loss: 0.2450 - accuracy: 0.9165 - val_loss: 0.2768 - val_accuracy: 0.9073\n",
      "Epoch 21/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2048 - accuracy: 0.90 - ETA: 0s - loss: 0.2304 - accuracy: 0.92 - ETA: 0s - loss: 0.2191 - accuracy: 0.92 - ETA: 0s - loss: 0.2228 - accuracy: 0.92 - ETA: 0s - loss: 0.2302 - accuracy: 0.92 - ETA: 0s - loss: 0.2327 - accuracy: 0.92 - ETA: 0s - loss: 0.2336 - accuracy: 0.92 - ETA: 0s - loss: 0.2342 - accuracy: 0.92 - 0s 5ms/step - loss: 0.2386 - accuracy: 0.9202 - val_loss: 0.2740 - val_accuracy: 0.9142\n",
      "Epoch 22/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2381 - accuracy: 0.93 - ETA: 0s - loss: 0.2498 - accuracy: 0.91 - ETA: 0s - loss: 0.2380 - accuracy: 0.91 - ETA: 0s - loss: 0.2322 - accuracy: 0.92 - ETA: 0s - loss: 0.2341 - accuracy: 0.92 - ETA: 0s - loss: 0.2369 - accuracy: 0.91 - ETA: 0s - loss: 0.2388 - accuracy: 0.91 - ETA: 0s - loss: 0.2400 - accuracy: 0.91 - 0s 4ms/step - loss: 0.2421 - accuracy: 0.9142 - val_loss: 0.2750 - val_accuracy: 0.9119\n",
      "Epoch 23/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2012 - accuracy: 0.93 - ETA: 0s - loss: 0.2375 - accuracy: 0.90 - ETA: 0s - loss: 0.2284 - accuracy: 0.91 - ETA: 0s - loss: 0.2270 - accuracy: 0.91 - ETA: 0s - loss: 0.2283 - accuracy: 0.91 - ETA: 0s - loss: 0.2319 - accuracy: 0.91 - ETA: 0s - loss: 0.2321 - accuracy: 0.91 - ETA: 0s - loss: 0.2336 - accuracy: 0.91 - 0s 4ms/step - loss: 0.2363 - accuracy: 0.9114 - val_loss: 0.2803 - val_accuracy: 0.9073\n",
      "Epoch 24/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1571 - accuracy: 0.96 - ETA: 0s - loss: 0.2354 - accuracy: 0.91 - ETA: 0s - loss: 0.2249 - accuracy: 0.91 - ETA: 0s - loss: 0.2232 - accuracy: 0.91 - ETA: 0s - loss: 0.2241 - accuracy: 0.92 - ETA: 0s - loss: 0.2248 - accuracy: 0.91 - ETA: 0s - loss: 0.2238 - accuracy: 0.92 - ETA: 0s - loss: 0.2253 - accuracy: 0.92 - 1s 5ms/step - loss: 0.2279 - accuracy: 0.9208 - val_loss: 0.2707 - val_accuracy: 0.9153\n",
      "Epoch 25/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2375 - accuracy: 0.84 - ETA: 0s - loss: 0.2388 - accuracy: 0.92 - ETA: 0s - loss: 0.2159 - accuracy: 0.92 - ETA: 0s - loss: 0.2139 - accuracy: 0.92 - ETA: 0s - loss: 0.2178 - accuracy: 0.92 - ETA: 0s - loss: 0.2200 - accuracy: 0.92 - ETA: 0s - loss: 0.2206 - accuracy: 0.92 - ETA: 0s - loss: 0.2228 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2251 - accuracy: 0.9234 - val_loss: 0.2716 - val_accuracy: 0.9199\n",
      "Epoch 26/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.3291 - accuracy: 0.90 - ETA: 0s - loss: 0.2240 - accuracy: 0.92 - ETA: 0s - loss: 0.2053 - accuracy: 0.93 - ETA: 0s - loss: 0.2090 - accuracy: 0.93 - ETA: 0s - loss: 0.2151 - accuracy: 0.93 - ETA: 0s - loss: 0.2195 - accuracy: 0.92 - ETA: 0s - loss: 0.2251 - accuracy: 0.92 - ETA: 0s - loss: 0.2271 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2304 - accuracy: 0.9211 - val_loss: 0.2751 - val_accuracy: 0.9085\n",
      "Epoch 27/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1706 - accuracy: 0.93 - ETA: 0s - loss: 0.2356 - accuracy: 0.92 - ETA: 0s - loss: 0.2125 - accuracy: 0.93 - ETA: 0s - loss: 0.2120 - accuracy: 0.92 - ETA: 0s - loss: 0.2163 - accuracy: 0.92 - ETA: 0s - loss: 0.2204 - accuracy: 0.92 - ETA: 0s - loss: 0.2181 - accuracy: 0.92 - ETA: 0s - loss: 0.2191 - accuracy: 0.92 - 1s 5ms/step - loss: 0.2218 - accuracy: 0.9225 - val_loss: 0.2589 - val_accuracy: 0.9188\n",
      "Epoch 28/1000\n",
      "110/110 [==============================] - ETA: 3s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2234 - accuracy: 0.91 - ETA: 0s - loss: 0.2116 - accuracy: 0.92 - ETA: 0s - loss: 0.2098 - accuracy: 0.92 - ETA: 0s - loss: 0.2095 - accuracy: 0.92 - ETA: 0s - loss: 0.2119 - accuracy: 0.92 - ETA: 0s - loss: 0.2120 - accuracy: 0.92 - ETA: 0s - loss: 0.2128 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2154 - accuracy: 0.9217 - val_loss: 0.2681 - val_accuracy: 0.9085\n",
      "Epoch 29/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1604 - accuracy: 0.96 - ETA: 0s - loss: 0.2119 - accuracy: 0.92 - ETA: 0s - loss: 0.2108 - accuracy: 0.92 - ETA: 0s - loss: 0.2037 - accuracy: 0.93 - ETA: 0s - loss: 0.2036 - accuracy: 0.93 - ETA: 0s - loss: 0.2097 - accuracy: 0.92 - ETA: 0s - loss: 0.2099 - accuracy: 0.92 - ETA: 0s - loss: 0.2082 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2110 - accuracy: 0.9254 - val_loss: 0.2721 - val_accuracy: 0.9153\n",
      "Epoch 30/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2442 - accuracy: 0.93 - ETA: 0s - loss: 0.2190 - accuracy: 0.92 - ETA: 0s - loss: 0.2051 - accuracy: 0.92 - ETA: 0s - loss: 0.2040 - accuracy: 0.93 - ETA: 0s - loss: 0.2030 - accuracy: 0.93 - ETA: 0s - loss: 0.2072 - accuracy: 0.92 - ETA: 0s - loss: 0.2109 - accuracy: 0.92 - ETA: 0s - loss: 0.2129 - accuracy: 0.92 - 0s 5ms/step - loss: 0.2146 - accuracy: 0.9211 - val_loss: 0.2530 - val_accuracy: 0.9199\n",
      "Epoch 31/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1928 - accuracy: 0.93 - ETA: 0s - loss: 0.1972 - accuracy: 0.92 - ETA: 0s - loss: 0.1888 - accuracy: 0.93 - ETA: 0s - loss: 0.1907 - accuracy: 0.93 - ETA: 0s - loss: 0.1930 - accuracy: 0.93 - ETA: 0s - loss: 0.1955 - accuracy: 0.93 - ETA: 0s - loss: 0.1990 - accuracy: 0.92 - ETA: 0s - loss: 0.1999 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2026 - accuracy: 0.9277 - val_loss: 0.2557 - val_accuracy: 0.9176\n",
      "Epoch 32/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2027 - accuracy: 0.96 - ETA: 0s - loss: 0.2070 - accuracy: 0.93 - ETA: 0s - loss: 0.1988 - accuracy: 0.93 - ETA: 0s - loss: 0.1903 - accuracy: 0.93 - ETA: 0s - loss: 0.1962 - accuracy: 0.93 - ETA: 0s - loss: 0.2017 - accuracy: 0.93 - ETA: 0s - loss: 0.2000 - accuracy: 0.93 - 0s 4ms/step - loss: 0.2060 - accuracy: 0.9277 - val_loss: 0.2572 - val_accuracy: 0.9153\n",
      "Epoch 33/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2427 - accuracy: 0.93 - ETA: 0s - loss: 0.2020 - accuracy: 0.93 - ETA: 0s - loss: 0.1938 - accuracy: 0.93 - ETA: 0s - loss: 0.1845 - accuracy: 0.94 - ETA: 0s - loss: 0.1919 - accuracy: 0.93 - ETA: 0s - loss: 0.1939 - accuracy: 0.93 - ETA: 0s - loss: 0.1978 - accuracy: 0.93 - ETA: 0s - loss: 0.1959 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1996 - accuracy: 0.9342 - val_loss: 0.2617 - val_accuracy: 0.9108\n",
      "Epoch 34/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1541 - accuracy: 0.93 - ETA: 0s - loss: 0.2045 - accuracy: 0.92 - ETA: 0s - loss: 0.2020 - accuracy: 0.93 - ETA: 0s - loss: 0.1988 - accuracy: 0.93 - ETA: 0s - loss: 0.1938 - accuracy: 0.93 - ETA: 0s - loss: 0.1961 - accuracy: 0.93 - ETA: 0s - loss: 0.1962 - accuracy: 0.93 - ETA: 0s - loss: 0.2023 - accuracy: 0.93 - 0s 4ms/step - loss: 0.2025 - accuracy: 0.9308 - val_loss: 0.2540 - val_accuracy: 0.9130\n",
      "Epoch 35/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 2s - loss: 0.1625 - accuracy: 0.96 - ETA: 0s - loss: 0.1952 - accuracy: 0.93 - ETA: 0s - loss: 0.1879 - accuracy: 0.93 - ETA: 0s - loss: 0.1851 - accuracy: 0.93 - ETA: 0s - loss: 0.1898 - accuracy: 0.93 - ETA: 0s - loss: 0.1873 - accuracy: 0.93 - ETA: 0s - loss: 0.1904 - accuracy: 0.93 - ETA: 0s - loss: 0.1914 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1935 - accuracy: 0.9334 - val_loss: 0.2573 - val_accuracy: 0.9176\n",
      "Epoch 36/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2230 - accuracy: 0.96 - ETA: 0s - loss: 0.1955 - accuracy: 0.93 - ETA: 0s - loss: 0.1742 - accuracy: 0.93 - ETA: 0s - loss: 0.1712 - accuracy: 0.93 - ETA: 0s - loss: 0.1760 - accuracy: 0.93 - ETA: 0s - loss: 0.1809 - accuracy: 0.93 - ETA: 0s - loss: 0.1844 - accuracy: 0.93 - ETA: 0s - loss: 0.1832 - accuracy: 0.93 - 1s 5ms/step - loss: 0.1898 - accuracy: 0.9357 - val_loss: 0.2460 - val_accuracy: 0.9130\n",
      "Epoch 37/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2112 - accuracy: 0.90 - ETA: 0s - loss: 0.2055 - accuracy: 0.92 - ETA: 0s - loss: 0.1853 - accuracy: 0.94 - ETA: 0s - loss: 0.1888 - accuracy: 0.93 - ETA: 0s - loss: 0.1865 - accuracy: 0.93 - ETA: 0s - loss: 0.1905 - accuracy: 0.93 - ETA: 0s - loss: 0.1891 - accuracy: 0.93 - ETA: 0s - loss: 0.1950 - accuracy: 0.93 - 0s 5ms/step - loss: 0.1954 - accuracy: 0.9308 - val_loss: 0.2425 - val_accuracy: 0.9165\n",
      "Epoch 38/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1799 - accuracy: 0.93 - ETA: 0s - loss: 0.1902 - accuracy: 0.93 - ETA: 0s - loss: 0.1742 - accuracy: 0.94 - ETA: 0s - loss: 0.1713 - accuracy: 0.94 - ETA: 0s - loss: 0.1773 - accuracy: 0.93 - ETA: 0s - loss: 0.1795 - accuracy: 0.93 - ETA: 0s - loss: 0.1800 - accuracy: 0.93 - ETA: 0s - loss: 0.1812 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1850 - accuracy: 0.9322 - val_loss: 0.2503 - val_accuracy: 0.9153\n",
      "Epoch 39/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1189 - accuracy: 0.96 - ETA: 0s - loss: 0.1798 - accuracy: 0.94 - ETA: 0s - loss: 0.1797 - accuracy: 0.93 - ETA: 0s - loss: 0.1782 - accuracy: 0.93 - ETA: 0s - loss: 0.1835 - accuracy: 0.93 - ETA: 0s - loss: 0.1838 - accuracy: 0.93 - ETA: 0s - loss: 0.1875 - accuracy: 0.93 - ETA: 0s - loss: 0.1866 - accuracy: 0.93 - 0s 4ms/step - loss: 0.1883 - accuracy: 0.9334 - val_loss: 0.2455 - val_accuracy: 0.9188\n",
      "Epoch 40/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1432 - accuracy: 0.96 - ETA: 0s - loss: 0.1811 - accuracy: 0.94 - ETA: 0s - loss: 0.1772 - accuracy: 0.94 - ETA: 0s - loss: 0.1688 - accuracy: 0.94 - ETA: 0s - loss: 0.1723 - accuracy: 0.94 - ETA: 0s - loss: 0.1727 - accuracy: 0.94 - ETA: 0s - loss: 0.1734 - accuracy: 0.94 - ETA: 0s - loss: 0.1734 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1768 - accuracy: 0.9397 - val_loss: 0.2516 - val_accuracy: 0.9130\n",
      "Epoch 41/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1253 - accuracy: 0.96 - ETA: 0s - loss: 0.1842 - accuracy: 0.93 - ETA: 0s - loss: 0.1709 - accuracy: 0.94 - ETA: 0s - loss: 0.1721 - accuracy: 0.94 - ETA: 0s - loss: 0.1718 - accuracy: 0.94 - ETA: 0s - loss: 0.1745 - accuracy: 0.93 - ETA: 0s - loss: 0.1759 - accuracy: 0.93 - ETA: 0s - loss: 0.1746 - accuracy: 0.93 - 1s 5ms/step - loss: 0.1798 - accuracy: 0.9380 - val_loss: 0.2417 - val_accuracy: 0.9176\n",
      "Epoch 42/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1186 - accuracy: 0.96 - ETA: 0s - loss: 0.1697 - accuracy: 0.94 - ETA: 0s - loss: 0.1638 - accuracy: 0.94 - ETA: 0s - loss: 0.1615 - accuracy: 0.94 - ETA: 0s - loss: 0.1666 - accuracy: 0.94 - ETA: 0s - loss: 0.1733 - accuracy: 0.93 - ETA: 0s - loss: 0.1739 - accuracy: 0.93 - ETA: 0s - loss: 0.1697 - accuracy: 0.94 - 1s 5ms/step - loss: 0.1738 - accuracy: 0.9400 - val_loss: 0.2373 - val_accuracy: 0.9233\n",
      "Epoch 43/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1544 - accuracy: 0.93 - ETA: 0s - loss: 0.1729 - accuracy: 0.94 - ETA: 0s - loss: 0.1683 - accuracy: 0.94 - ETA: 0s - loss: 0.1683 - accuracy: 0.94 - ETA: 0s - loss: 0.1714 - accuracy: 0.93 - ETA: 0s - loss: 0.1732 - accuracy: 0.93 - ETA: 0s - loss: 0.1733 - accuracy: 0.93 - ETA: 0s - loss: 0.1727 - accuracy: 0.93 - 1s 5ms/step - loss: 0.1760 - accuracy: 0.9385 - val_loss: 0.2404 - val_accuracy: 0.9222\n",
      "Epoch 44/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1296 - accuracy: 0.96 - ETA: 0s - loss: 0.1598 - accuracy: 0.94 - ETA: 0s - loss: 0.1580 - accuracy: 0.94 - ETA: 0s - loss: 0.1556 - accuracy: 0.94 - ETA: 0s - loss: 0.1563 - accuracy: 0.94 - ETA: 0s - loss: 0.1640 - accuracy: 0.94 - ETA: 0s - loss: 0.1629 - accuracy: 0.94 - ETA: 0s - loss: 0.1630 - accuracy: 0.94 - 1s 5ms/step - loss: 0.1682 - accuracy: 0.9437 - val_loss: 0.2340 - val_accuracy: 0.9256\n",
      "Epoch 45/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1819 - accuracy: 0.96 - ETA: 0s - loss: 0.1701 - accuracy: 0.94 - ETA: 0s - loss: 0.1882 - accuracy: 0.94 - ETA: 0s - loss: 0.1760 - accuracy: 0.94 - ETA: 0s - loss: 0.1736 - accuracy: 0.94 - ETA: 0s - loss: 0.1831 - accuracy: 0.93 - ETA: 0s - loss: 0.1799 - accuracy: 0.93 - ETA: 0s - loss: 0.1778 - accuracy: 0.93 - 1s 5ms/step - loss: 0.1839 - accuracy: 0.9357 - val_loss: 0.2445 - val_accuracy: 0.9176\n",
      "Epoch 46/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2483 - accuracy: 0.90 - ETA: 0s - loss: 0.1678 - accuracy: 0.94 - ETA: 0s - loss: 0.1654 - accuracy: 0.95 - ETA: 0s - loss: 0.1579 - accuracy: 0.95 - ETA: 0s - loss: 0.1574 - accuracy: 0.95 - ETA: 0s - loss: 0.1647 - accuracy: 0.94 - ETA: 0s - loss: 0.1656 - accuracy: 0.94 - ETA: 0s - loss: 0.1652 - accuracy: 0.94 - 0s 5ms/step - loss: 0.1672 - accuracy: 0.9448 - val_loss: 0.2359 - val_accuracy: 0.9233\n",
      "Epoch 47/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1672 - accuracy: 0.93 - ETA: 0s - loss: 0.1717 - accuracy: 0.94 - ETA: 0s - loss: 0.1592 - accuracy: 0.94 - ETA: 0s - loss: 0.1590 - accuracy: 0.94 - ETA: 0s - loss: 0.1544 - accuracy: 0.94 - ETA: 0s - loss: 0.1586 - accuracy: 0.94 - ETA: 0s - loss: 0.1602 - accuracy: 0.94 - ETA: 0s - loss: 0.1584 - accuracy: 0.94 - 1s 5ms/step - loss: 0.1630 - accuracy: 0.9443 - val_loss: 0.2353 - val_accuracy: 0.9211\n",
      "Epoch 48/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1203 - accuracy: 0.96 - ETA: 0s - loss: 0.1600 - accuracy: 0.94 - ETA: 0s - loss: 0.1604 - accuracy: 0.94 - ETA: 0s - loss: 0.1502 - accuracy: 0.94 - ETA: 0s - loss: 0.1562 - accuracy: 0.94 - ETA: 0s - loss: 0.1595 - accuracy: 0.94 - ETA: 0s - loss: 0.1616 - accuracy: 0.94 - ETA: 0s - loss: 0.1590 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1613 - accuracy: 0.9428 - val_loss: 0.2385 - val_accuracy: 0.9233\n",
      "Epoch 49/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1731 - accuracy: 0.93 - ETA: 0s - loss: 0.1566 - accuracy: 0.94 - ETA: 0s - loss: 0.1534 - accuracy: 0.94 - ETA: 0s - loss: 0.1459 - accuracy: 0.94 - ETA: 0s - loss: 0.1477 - accuracy: 0.94 - ETA: 0s - loss: 0.1587 - accuracy: 0.94 - ETA: 0s - loss: 0.1604 - accuracy: 0.94 - ETA: 0s - loss: 0.1592 - accuracy: 0.94 - 1s 5ms/step - loss: 0.1614 - accuracy: 0.9417 - val_loss: 0.2367 - val_accuracy: 0.9233\n",
      "Epoch 50/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1346 - accuracy: 0.93 - ETA: 0s - loss: 0.1503 - accuracy: 0.94 - ETA: 0s - loss: 0.1477 - accuracy: 0.94 - ETA: 0s - loss: 0.1363 - accuracy: 0.95 - ETA: 0s - loss: 0.1388 - accuracy: 0.95 - ETA: 0s - loss: 0.1502 - accuracy: 0.94 - ETA: 0s - loss: 0.1527 - accuracy: 0.94 - ETA: 0s - loss: 0.1527 - accuracy: 0.94 - 1s 5ms/step - loss: 0.1564 - accuracy: 0.9423 - val_loss: 0.2338 - val_accuracy: 0.9256\n",
      "Epoch 51/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.0943 - accuracy: 0.96 - ETA: 0s - loss: 0.1519 - accuracy: 0.94 - ETA: 0s - loss: 0.1530 - accuracy: 0.94 - ETA: 0s - loss: 0.1460 - accuracy: 0.94 - ETA: 0s - loss: 0.1498 - accuracy: 0.94 - ETA: 0s - loss: 0.1532 - accuracy: 0.94 - ETA: 0s - loss: 0.1552 - accuracy: 0.94 - ETA: 0s - loss: 0.1510 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1536 - accuracy: 0.9434 - val_loss: 0.2389 - val_accuracy: 0.9256\n",
      "Epoch 52/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 2s - loss: 0.1543 - accuracy: 0.96 - ETA: 0s - loss: 0.1519 - accuracy: 0.94 - ETA: 0s - loss: 0.1394 - accuracy: 0.95 - ETA: 0s - loss: 0.1373 - accuracy: 0.94 - ETA: 0s - loss: 0.1438 - accuracy: 0.94 - ETA: 0s - loss: 0.1502 - accuracy: 0.94 - ETA: 0s - loss: 0.1512 - accuracy: 0.94 - ETA: 0s - loss: 0.1496 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1517 - accuracy: 0.9460 - val_loss: 0.2414 - val_accuracy: 0.9165\n",
      "Epoch 53/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1707 - accuracy: 0.93 - ETA: 0s - loss: 0.1558 - accuracy: 0.93 - ETA: 0s - loss: 0.1516 - accuracy: 0.94 - ETA: 0s - loss: 0.1455 - accuracy: 0.94 - ETA: 0s - loss: 0.1428 - accuracy: 0.94 - ETA: 0s - loss: 0.1465 - accuracy: 0.94 - ETA: 0s - loss: 0.1456 - accuracy: 0.94 - ETA: 0s - loss: 0.1480 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1486 - accuracy: 0.9480 - val_loss: 0.2357 - val_accuracy: 0.9233\n",
      "Epoch 54/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1399 - accuracy: 0.93 - ETA: 0s - loss: 0.1601 - accuracy: 0.95 - ETA: 0s - loss: 0.1613 - accuracy: 0.94 - ETA: 0s - loss: 0.1568 - accuracy: 0.94 - ETA: 0s - loss: 0.1570 - accuracy: 0.95 - ETA: 0s - loss: 0.1584 - accuracy: 0.94 - ETA: 0s - loss: 0.1578 - accuracy: 0.94 - ETA: 0s - loss: 0.1563 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1578 - accuracy: 0.9474 - val_loss: 0.2392 - val_accuracy: 0.9188\n",
      "Epoch 55/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1557 - accuracy: 0.93 - ETA: 0s - loss: 0.1611 - accuracy: 0.94 - ETA: 0s - loss: 0.1464 - accuracy: 0.95 - ETA: 0s - loss: 0.1437 - accuracy: 0.95 - ETA: 0s - loss: 0.1463 - accuracy: 0.94 - ETA: 0s - loss: 0.1505 - accuracy: 0.94 - ETA: 0s - loss: 0.1498 - accuracy: 0.94 - ETA: 0s - loss: 0.1502 - accuracy: 0.94 - 1s 5ms/step - loss: 0.1525 - accuracy: 0.9431 - val_loss: 0.2334 - val_accuracy: 0.9256\n",
      "Epoch 56/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1152 - accuracy: 0.96 - ETA: 0s - loss: 0.1712 - accuracy: 0.94 - ETA: 0s - loss: 0.1508 - accuracy: 0.95 - ETA: 0s - loss: 0.1389 - accuracy: 0.95 - ETA: 0s - loss: 0.1399 - accuracy: 0.95 - ETA: 0s - loss: 0.1435 - accuracy: 0.95 - ETA: 0s - loss: 0.1437 - accuracy: 0.95 - ETA: 0s - loss: 0.1468 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1469 - accuracy: 0.9494 - val_loss: 0.2396 - val_accuracy: 0.9222\n",
      "Epoch 57/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1430 - accuracy: 0.93 - ETA: 0s - loss: 0.1510 - accuracy: 0.94 - ETA: 0s - loss: 0.1418 - accuracy: 0.95 - ETA: 0s - loss: 0.1342 - accuracy: 0.95 - ETA: 0s - loss: 0.1489 - accuracy: 0.94 - ETA: 0s - loss: 0.1496 - accuracy: 0.94 - ETA: 0s - loss: 0.1517 - accuracy: 0.94 - ETA: 0s - loss: 0.1521 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1546 - accuracy: 0.9440 - val_loss: 0.2503 - val_accuracy: 0.9233\n",
      "Epoch 58/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2462 - accuracy: 0.87 - ETA: 0s - loss: 0.1589 - accuracy: 0.94 - ETA: 0s - loss: 0.1443 - accuracy: 0.95 - ETA: 0s - loss: 0.1364 - accuracy: 0.95 - ETA: 0s - loss: 0.1364 - accuracy: 0.95 - ETA: 0s - loss: 0.1372 - accuracy: 0.95 - ETA: 0s - loss: 0.1369 - accuracy: 0.95 - ETA: 0s - loss: 0.1357 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1372 - accuracy: 0.9511 - val_loss: 0.2371 - val_accuracy: 0.9245\n",
      "Epoch 59/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.0914 - accuracy: 0.96 - ETA: 0s - loss: 0.1308 - accuracy: 0.95 - ETA: 0s - loss: 0.1269 - accuracy: 0.95 - ETA: 0s - loss: 0.1233 - accuracy: 0.95 - ETA: 0s - loss: 0.1249 - accuracy: 0.95 - ETA: 0s - loss: 0.1310 - accuracy: 0.95 - ETA: 0s - loss: 0.1350 - accuracy: 0.95 - ETA: 0s - loss: 0.1355 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1376 - accuracy: 0.9520 - val_loss: 0.2337 - val_accuracy: 0.9233\n",
      "Epoch 60/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1667 - accuracy: 0.96 - ETA: 0s - loss: 0.1344 - accuracy: 0.95 - ETA: 0s - loss: 0.1283 - accuracy: 0.95 - ETA: 0s - loss: 0.1248 - accuracy: 0.95 - ETA: 0s - loss: 0.1268 - accuracy: 0.95 - ETA: 0s - loss: 0.1332 - accuracy: 0.95 - ETA: 0s - loss: 0.1317 - accuracy: 0.95 - ETA: 0s - loss: 0.1367 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1359 - accuracy: 0.9500 - val_loss: 0.2283 - val_accuracy: 0.9279\n",
      "Epoch 61/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1094 - accuracy: 0.96 - ETA: 0s - loss: 0.1380 - accuracy: 0.94 - ETA: 0s - loss: 0.1333 - accuracy: 0.95 - ETA: 0s - loss: 0.1272 - accuracy: 0.95 - ETA: 0s - loss: 0.1327 - accuracy: 0.95 - ETA: 0s - loss: 0.1392 - accuracy: 0.94 - ETA: 0s - loss: 0.1378 - accuracy: 0.95 - ETA: 0s - loss: 0.1344 - accuracy: 0.95 - 0s 5ms/step - loss: 0.1375 - accuracy: 0.9517 - val_loss: 0.2374 - val_accuracy: 0.9245\n",
      "Epoch 62/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1219 - accuracy: 0.93 - ETA: 0s - loss: 0.1487 - accuracy: 0.94 - ETA: 0s - loss: 0.1354 - accuracy: 0.95 - ETA: 0s - loss: 0.1308 - accuracy: 0.94 - ETA: 0s - loss: 0.1313 - accuracy: 0.95 - ETA: 0s - loss: 0.1304 - accuracy: 0.95 - ETA: 0s - loss: 0.1328 - accuracy: 0.95 - ETA: 0s - loss: 0.1306 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9534 - val_loss: 0.2353 - val_accuracy: 0.9256\n",
      "Epoch 63/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.0918 - accuracy: 0.96 - ETA: 0s - loss: 0.1427 - accuracy: 0.95 - ETA: 0s - loss: 0.1305 - accuracy: 0.95 - ETA: 0s - loss: 0.1168 - accuracy: 0.95 - ETA: 0s - loss: 0.1183 - accuracy: 0.95 - ETA: 0s - loss: 0.1227 - accuracy: 0.95 - ETA: 0s - loss: 0.1268 - accuracy: 0.95 - ETA: 0s - loss: 0.1281 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1291 - accuracy: 0.9543 - val_loss: 0.2462 - val_accuracy: 0.9222\n",
      "Epoch 64/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1228 - accuracy: 0.93 - ETA: 0s - loss: 0.1449 - accuracy: 0.94 - ETA: 0s - loss: 0.1326 - accuracy: 0.95 - ETA: 0s - loss: 0.1293 - accuracy: 0.95 - ETA: 0s - loss: 0.1285 - accuracy: 0.95 - ETA: 0s - loss: 0.1257 - accuracy: 0.95 - ETA: 0s - loss: 0.1281 - accuracy: 0.95 - ETA: 0s - loss: 0.1313 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1342 - accuracy: 0.9485 - val_loss: 0.2364 - val_accuracy: 0.9279\n",
      "Epoch 65/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1401 - accuracy: 0.93 - ETA: 0s - loss: 0.1230 - accuracy: 0.96 - ETA: 0s - loss: 0.1182 - accuracy: 0.96 - ETA: 0s - loss: 0.1145 - accuracy: 0.96 - ETA: 0s - loss: 0.1211 - accuracy: 0.95 - ETA: 0s - loss: 0.1224 - accuracy: 0.95 - ETA: 0s - loss: 0.1254 - accuracy: 0.95 - ETA: 0s - loss: 0.1254 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1269 - accuracy: 0.9520 - val_loss: 0.2446 - val_accuracy: 0.9233\n",
      "Epoch 66/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1037 - accuracy: 0.93 - ETA: 0s - loss: 0.1221 - accuracy: 0.94 - ETA: 0s - loss: 0.1096 - accuracy: 0.95 - ETA: 0s - loss: 0.1105 - accuracy: 0.96 - ETA: 0s - loss: 0.1132 - accuracy: 0.95 - ETA: 0s - loss: 0.1183 - accuracy: 0.95 - ETA: 0s - loss: 0.1202 - accuracy: 0.95 - ETA: 0s - loss: 0.1202 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1216 - accuracy: 0.9557 - val_loss: 0.2438 - val_accuracy: 0.9222\n",
      "Epoch 67/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1176 - accuracy: 0.93 - ETA: 0s - loss: 0.1183 - accuracy: 0.95 - ETA: 0s - loss: 0.1145 - accuracy: 0.96 - ETA: 0s - loss: 0.1185 - accuracy: 0.95 - ETA: 0s - loss: 0.1126 - accuracy: 0.96 - ETA: 0s - loss: 0.1204 - accuracy: 0.95 - ETA: 0s - loss: 0.1191 - accuracy: 0.95 - ETA: 0s - loss: 0.1191 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1206 - accuracy: 0.9580 - val_loss: 0.2370 - val_accuracy: 0.9279\n",
      "Epoch 68/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.1149 - accuracy: 0.93 - ETA: 0s - loss: 0.1213 - accuracy: 0.94 - ETA: 0s - loss: 0.1176 - accuracy: 0.95 - ETA: 0s - loss: 0.1149 - accuracy: 0.95 - ETA: 0s - loss: 0.1141 - accuracy: 0.95 - ETA: 0s - loss: 0.1238 - accuracy: 0.96 - ETA: 0s - loss: 0.1252 - accuracy: 0.95 - ETA: 0s - loss: 0.1225 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9583 - val_loss: 0.2449 - val_accuracy: 0.9245\n",
      "Epoch 69/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 2s - loss: 0.1347 - accuracy: 0.93 - ETA: 0s - loss: 0.1281 - accuracy: 0.96 - ETA: 0s - loss: 0.1134 - accuracy: 0.96 - ETA: 0s - loss: 0.1063 - accuracy: 0.96 - ETA: 0s - loss: 0.1085 - accuracy: 0.96 - ETA: 0s - loss: 0.1088 - accuracy: 0.96 - ETA: 0s - loss: 0.1108 - accuracy: 0.96 - ETA: 0s - loss: 0.1134 - accuracy: 0.96 - 0s 4ms/step - loss: 0.1150 - accuracy: 0.9594 - val_loss: 0.2545 - val_accuracy: 0.9199\n",
      "Epoch 70/1000\n",
      "110/110 [==============================] - ETA: 2s - loss: 0.2105 - accuracy: 0.90 - ETA: 0s - loss: 0.1363 - accuracy: 0.94 - ETA: 0s - loss: 0.1198 - accuracy: 0.95 - ETA: 0s - loss: 0.1151 - accuracy: 0.95 - ETA: 0s - loss: 0.1159 - accuracy: 0.95 - ETA: 0s - loss: 0.1232 - accuracy: 0.95 - ETA: 0s - loss: 0.1229 - accuracy: 0.95 - ETA: 0s - loss: 0.1195 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1223 - accuracy: 0.9571 - val_loss: 0.2502 - val_accuracy: 0.9222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 8654cb3d411b3ba05ccb82cb514a02a5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.22827939742377826</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-classification_head_1/dropout_rate: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-classification_head_1/spatial_reduction_1/reduction_type: flatten</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-classification_head_2/dropout_rate: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-classification_head_2/spatial_reduction_1/reduction_type: global_avg</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_block_1/dropout_rate: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dense_block_1/num_layers: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_block_1/units_0: 128</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dense_block_1/use_batchnorm: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/augment: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/block_type: vanilla</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/dropout_rate: 0.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/filters_0_0: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/filters_0_1: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/kernel_size: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/max_pooling: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/num_blocks: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/num_layers: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/separable: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/normalize: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/augment: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/block_type: vanilla</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/dropout_rate: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/filters_0_0: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/filters_0_1: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/filters_1_0: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/filters_1_1: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/kernel_size: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/max_pooling: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/num_blocks: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/num_layers: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/separable: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/normalize: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-optimizer: adam</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "Train for 138 steps, validate for 28 steps\n",
      "Epoch 1/1000\n",
      "138/138 [==============================] - ETA: 54s - loss: 0.7804 - accuracy: 0.375 - ETA: 3s - loss: 0.6411 - accuracy: 0.585 - ETA: 1s - loss: 0.5977 - accuracy: 0.64 - ETA: 1s - loss: 0.5863 - accuracy: 0.66 - ETA: 0s - loss: 0.5823 - accuracy: 0.66 - ETA: 0s - loss: 0.5767 - accuracy: 0.67 - ETA: 0s - loss: 0.5714 - accuracy: 0.68 - ETA: 0s - loss: 0.5694 - accuracy: 0.67 - ETA: 0s - loss: 0.5683 - accuracy: 0.68 - ETA: 0s - loss: 0.5673 - accuracy: 0.68 - 1s 7ms/step - loss: 0.5680 - accuracy: 0.6816 - val_loss: 0.5520 - val_accuracy: 0.6854\n",
      "Epoch 2/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.5232 - accuracy: 0.68 - ETA: 0s - loss: 0.5586 - accuracy: 0.69 - ETA: 0s - loss: 0.5324 - accuracy: 0.72 - ETA: 0s - loss: 0.5291 - accuracy: 0.72 - ETA: 0s - loss: 0.5267 - accuracy: 0.72 - ETA: 0s - loss: 0.5226 - accuracy: 0.73 - ETA: 0s - loss: 0.5174 - accuracy: 0.73 - ETA: 0s - loss: 0.5128 - accuracy: 0.73 - ETA: 0s - loss: 0.5110 - accuracy: 0.74 - ETA: 0s - loss: 0.5092 - accuracy: 0.74 - 1s 4ms/step - loss: 0.5091 - accuracy: 0.7475 - val_loss: 0.4823 - val_accuracy: 0.7414\n",
      "Epoch 3/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.4167 - accuracy: 0.75 - ETA: 0s - loss: 0.4665 - accuracy: 0.78 - ETA: 0s - loss: 0.4410 - accuracy: 0.80 - ETA: 0s - loss: 0.4443 - accuracy: 0.80 - ETA: 0s - loss: 0.4458 - accuracy: 0.80 - ETA: 0s - loss: 0.4449 - accuracy: 0.79 - ETA: 0s - loss: 0.4457 - accuracy: 0.80 - ETA: 0s - loss: 0.4459 - accuracy: 0.80 - ETA: 0s - loss: 0.4456 - accuracy: 0.80 - ETA: 0s - loss: 0.4450 - accuracy: 0.80 - 1s 4ms/step - loss: 0.4449 - accuracy: 0.8056 - val_loss: 0.4210 - val_accuracy: 0.7963\n",
      "Epoch 4/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.3217 - accuracy: 0.87 - ETA: 0s - loss: 0.4141 - accuracy: 0.83 - ETA: 0s - loss: 0.3843 - accuracy: 0.85 - ETA: 0s - loss: 0.3834 - accuracy: 0.84 - ETA: 0s - loss: 0.3871 - accuracy: 0.84 - ETA: 0s - loss: 0.3903 - accuracy: 0.84 - ETA: 0s - loss: 0.3892 - accuracy: 0.84 - ETA: 0s - loss: 0.3893 - accuracy: 0.84 - ETA: 0s - loss: 0.3894 - accuracy: 0.84 - ETA: 0s - loss: 0.3914 - accuracy: 0.84 - 1s 4ms/step - loss: 0.3916 - accuracy: 0.8429 - val_loss: 0.3944 - val_accuracy: 0.8227\n",
      "Epoch 5/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2972 - accuracy: 0.84 - ETA: 0s - loss: 0.3780 - accuracy: 0.85 - ETA: 0s - loss: 0.3511 - accuracy: 0.87 - ETA: 0s - loss: 0.3573 - accuracy: 0.86 - ETA: 0s - loss: 0.3642 - accuracy: 0.86 - ETA: 0s - loss: 0.3652 - accuracy: 0.86 - ETA: 0s - loss: 0.3731 - accuracy: 0.85 - ETA: 0s - loss: 0.3708 - accuracy: 0.85 - ETA: 0s - loss: 0.3727 - accuracy: 0.85 - ETA: 0s - loss: 0.3760 - accuracy: 0.85 - 1s 4ms/step - loss: 0.3767 - accuracy: 0.8570 - val_loss: 0.3606 - val_accuracy: 0.8478\n",
      "Epoch 6/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.3688 - accuracy: 0.87 - ETA: 0s - loss: 0.3644 - accuracy: 0.87 - ETA: 0s - loss: 0.3345 - accuracy: 0.88 - ETA: 0s - loss: 0.3402 - accuracy: 0.88 - ETA: 0s - loss: 0.3387 - accuracy: 0.87 - ETA: 0s - loss: 0.3369 - accuracy: 0.87 - ETA: 0s - loss: 0.3415 - accuracy: 0.87 - ETA: 0s - loss: 0.3402 - accuracy: 0.87 - ETA: 0s - loss: 0.3411 - accuracy: 0.87 - 1s 4ms/step - loss: 0.3444 - accuracy: 0.8753 - val_loss: 0.3288 - val_accuracy: 0.8833\n",
      "Epoch 7/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2797 - accuracy: 0.90 - ETA: 0s - loss: 0.3266 - accuracy: 0.89 - ETA: 0s - loss: 0.3102 - accuracy: 0.89 - ETA: 0s - loss: 0.3158 - accuracy: 0.89 - ETA: 0s - loss: 0.3222 - accuracy: 0.88 - ETA: 0s - loss: 0.3260 - accuracy: 0.88 - ETA: 0s - loss: 0.3245 - accuracy: 0.88 - ETA: 0s - loss: 0.3207 - accuracy: 0.88 - ETA: 0s - loss: 0.3223 - accuracy: 0.88 - ETA: 0s - loss: 0.3293 - accuracy: 0.88 - 1s 4ms/step - loss: 0.3300 - accuracy: 0.8815 - val_loss: 0.3190 - val_accuracy: 0.8833\n",
      "Epoch 8/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.4064 - accuracy: 0.87 - ETA: 0s - loss: 0.3413 - accuracy: 0.88 - ETA: 0s - loss: 0.3101 - accuracy: 0.90 - ETA: 0s - loss: 0.3146 - accuracy: 0.89 - ETA: 0s - loss: 0.3149 - accuracy: 0.89 - ETA: 0s - loss: 0.3116 - accuracy: 0.88 - ETA: 0s - loss: 0.3116 - accuracy: 0.88 - ETA: 0s - loss: 0.3122 - accuracy: 0.88 - ETA: 0s - loss: 0.3126 - accuracy: 0.88 - ETA: 0s - loss: 0.3199 - accuracy: 0.88 - 1s 4ms/step - loss: 0.3221 - accuracy: 0.8829 - val_loss: 0.3307 - val_accuracy: 0.8661\n",
      "Epoch 9/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2339 - accuracy: 0.93 - ETA: 0s - loss: 0.3054 - accuracy: 0.89 - ETA: 0s - loss: 0.2926 - accuracy: 0.90 - ETA: 0s - loss: 0.2966 - accuracy: 0.89 - ETA: 0s - loss: 0.2951 - accuracy: 0.89 - ETA: 0s - loss: 0.2978 - accuracy: 0.89 - ETA: 0s - loss: 0.2988 - accuracy: 0.89 - ETA: 0s - loss: 0.2990 - accuracy: 0.89 - ETA: 0s - loss: 0.2977 - accuracy: 0.89 - ETA: 0s - loss: 0.3046 - accuracy: 0.89 - 1s 4ms/step - loss: 0.3050 - accuracy: 0.8943 - val_loss: 0.3089 - val_accuracy: 0.8833\n",
      "Epoch 10/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.4161 - accuracy: 0.87 - ETA: 0s - loss: 0.3326 - accuracy: 0.88 - ETA: 0s - loss: 0.3014 - accuracy: 0.89 - ETA: 0s - loss: 0.2985 - accuracy: 0.89 - ETA: 0s - loss: 0.3039 - accuracy: 0.89 - ETA: 0s - loss: 0.3027 - accuracy: 0.89 - ETA: 0s - loss: 0.3031 - accuracy: 0.89 - ETA: 0s - loss: 0.3010 - accuracy: 0.89 - ETA: 0s - loss: 0.3009 - accuracy: 0.89 - ETA: 0s - loss: 0.3076 - accuracy: 0.88 - 1s 4ms/step - loss: 0.3092 - accuracy: 0.8893 - val_loss: 0.2870 - val_accuracy: 0.8959\n",
      "Epoch 11/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2935 - accuracy: 0.90 - ETA: 0s - loss: 0.2776 - accuracy: 0.91 - ETA: 0s - loss: 0.2756 - accuracy: 0.90 - ETA: 0s - loss: 0.2803 - accuracy: 0.90 - ETA: 0s - loss: 0.2802 - accuracy: 0.90 - ETA: 0s - loss: 0.2809 - accuracy: 0.90 - ETA: 0s - loss: 0.2816 - accuracy: 0.90 - ETA: 0s - loss: 0.2789 - accuracy: 0.90 - ETA: 0s - loss: 0.2822 - accuracy: 0.90 - ETA: 0s - loss: 0.2862 - accuracy: 0.90 - 1s 4ms/step - loss: 0.2879 - accuracy: 0.9010 - val_loss: 0.2810 - val_accuracy: 0.9005\n",
      "Epoch 12/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2314 - accuracy: 0.90 - ETA: 0s - loss: 0.2651 - accuracy: 0.91 - ETA: 0s - loss: 0.2661 - accuracy: 0.90 - ETA: 0s - loss: 0.2683 - accuracy: 0.90 - ETA: 0s - loss: 0.2790 - accuracy: 0.90 - ETA: 0s - loss: 0.2745 - accuracy: 0.90 - ETA: 0s - loss: 0.2743 - accuracy: 0.90 - ETA: 0s - loss: 0.2796 - accuracy: 0.90 - 1s 4ms/step - loss: 0.2808 - accuracy: 0.9010 - val_loss: 0.2699 - val_accuracy: 0.9085\n",
      "Epoch 13/1000\n",
      "138/138 [==============================] - ETA: 4s - loss: 0.3106 - accuracy: 0.90 - ETA: 0s - loss: 0.2759 - accuracy: 0.91 - ETA: 0s - loss: 0.2640 - accuracy: 0.91 - ETA: 0s - loss: 0.2592 - accuracy: 0.91 - ETA: 0s - loss: 0.2637 - accuracy: 0.91 - ETA: 0s - loss: 0.2656 - accuracy: 0.90 - ETA: 0s - loss: 0.2616 - accuracy: 0.91 - ETA: 0s - loss: 0.2667 - accuracy: 0.90 - ETA: 0s - loss: 0.2723 - accuracy: 0.90 - 1s 4ms/step - loss: 0.2766 - accuracy: 0.9032 - val_loss: 0.2778 - val_accuracy: 0.8947\n",
      "Epoch 14/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2113 - accuracy: 0.93 - ETA: 0s - loss: 0.2667 - accuracy: 0.91 - ETA: 0s - loss: 0.2589 - accuracy: 0.91 - ETA: 0s - loss: 0.2569 - accuracy: 0.91 - ETA: 0s - loss: 0.2580 - accuracy: 0.91 - ETA: 0s - loss: 0.2594 - accuracy: 0.91 - ETA: 0s - loss: 0.2595 - accuracy: 0.91 - ETA: 0s - loss: 0.2586 - accuracy: 0.91 - ETA: 0s - loss: 0.2600 - accuracy: 0.90 - ETA: 0s - loss: 0.2684 - accuracy: 0.90 - 1s 4ms/step - loss: 0.2696 - accuracy: 0.9062 - val_loss: 0.2614 - val_accuracy: 0.9119\n",
      "Epoch 15/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 5s - loss: 0.2177 - accuracy: 0.93 - ETA: 0s - loss: 0.2604 - accuracy: 0.92 - ETA: 0s - loss: 0.2481 - accuracy: 0.92 - ETA: 0s - loss: 0.2484 - accuracy: 0.91 - ETA: 0s - loss: 0.2489 - accuracy: 0.91 - ETA: 0s - loss: 0.2475 - accuracy: 0.91 - ETA: 0s - loss: 0.2475 - accuracy: 0.91 - ETA: 0s - loss: 0.2474 - accuracy: 0.91 - ETA: 0s - loss: 0.2478 - accuracy: 0.91 - ETA: 0s - loss: 0.2576 - accuracy: 0.90 - 1s 4ms/step - loss: 0.2582 - accuracy: 0.9078 - val_loss: 0.2610 - val_accuracy: 0.9027\n",
      "Epoch 16/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1934 - accuracy: 0.93 - ETA: 0s - loss: 0.2471 - accuracy: 0.91 - ETA: 0s - loss: 0.2440 - accuracy: 0.92 - ETA: 0s - loss: 0.2396 - accuracy: 0.92 - ETA: 0s - loss: 0.2485 - accuracy: 0.91 - ETA: 0s - loss: 0.2473 - accuracy: 0.91 - ETA: 0s - loss: 0.2464 - accuracy: 0.91 - ETA: 0s - loss: 0.2447 - accuracy: 0.91 - ETA: 0s - loss: 0.2454 - accuracy: 0.91 - ETA: 0s - loss: 0.2539 - accuracy: 0.91 - 1s 4ms/step - loss: 0.2558 - accuracy: 0.9101 - val_loss: 0.2510 - val_accuracy: 0.9165\n",
      "Epoch 17/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.3129 - accuracy: 0.90 - ETA: 0s - loss: 0.2746 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.91 - ETA: 0s - loss: 0.2398 - accuracy: 0.92 - ETA: 0s - loss: 0.2406 - accuracy: 0.91 - ETA: 0s - loss: 0.2424 - accuracy: 0.91 - ETA: 0s - loss: 0.2417 - accuracy: 0.91 - ETA: 0s - loss: 0.2378 - accuracy: 0.91 - ETA: 0s - loss: 0.2374 - accuracy: 0.91 - ETA: 0s - loss: 0.2446 - accuracy: 0.91 - 1s 4ms/step - loss: 0.2461 - accuracy: 0.9147 - val_loss: 0.2586 - val_accuracy: 0.9222\n",
      "Epoch 18/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2659 - accuracy: 0.93 - ETA: 0s - loss: 0.2647 - accuracy: 0.91 - ETA: 0s - loss: 0.2439 - accuracy: 0.92 - ETA: 0s - loss: 0.2391 - accuracy: 0.92 - ETA: 0s - loss: 0.2409 - accuracy: 0.92 - ETA: 0s - loss: 0.2383 - accuracy: 0.91 - ETA: 0s - loss: 0.2352 - accuracy: 0.91 - ETA: 0s - loss: 0.2317 - accuracy: 0.92 - ETA: 0s - loss: 0.2329 - accuracy: 0.91 - ETA: 0s - loss: 0.2405 - accuracy: 0.91 - 1s 4ms/step - loss: 0.2425 - accuracy: 0.9151 - val_loss: 0.2487 - val_accuracy: 0.9062\n",
      "Epoch 19/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2511 - accuracy: 0.93 - ETA: 0s - loss: 0.2570 - accuracy: 0.91 - ETA: 0s - loss: 0.2313 - accuracy: 0.92 - ETA: 0s - loss: 0.2222 - accuracy: 0.92 - ETA: 0s - loss: 0.2292 - accuracy: 0.92 - ETA: 0s - loss: 0.2305 - accuracy: 0.92 - ETA: 0s - loss: 0.2272 - accuracy: 0.92 - ETA: 0s - loss: 0.2265 - accuracy: 0.92 - ETA: 0s - loss: 0.2261 - accuracy: 0.92 - ETA: 0s - loss: 0.2333 - accuracy: 0.91 - 1s 4ms/step - loss: 0.2340 - accuracy: 0.9181 - val_loss: 0.2326 - val_accuracy: 0.9222\n",
      "Epoch 20/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2736 - accuracy: 0.90 - ETA: 0s - loss: 0.2397 - accuracy: 0.91 - ETA: 0s - loss: 0.2277 - accuracy: 0.92 - ETA: 0s - loss: 0.2243 - accuracy: 0.92 - ETA: 0s - loss: 0.2259 - accuracy: 0.92 - ETA: 0s - loss: 0.2311 - accuracy: 0.91 - ETA: 0s - loss: 0.2305 - accuracy: 0.91 - ETA: 0s - loss: 0.2344 - accuracy: 0.91 - ETA: 0s - loss: 0.2360 - accuracy: 0.91 - 1s 4ms/step - loss: 0.2412 - accuracy: 0.9151 - val_loss: 0.2459 - val_accuracy: 0.9165\n",
      "Epoch 21/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2714 - accuracy: 0.90 - ETA: 0s - loss: 0.2482 - accuracy: 0.91 - ETA: 0s - loss: 0.2349 - accuracy: 0.92 - ETA: 0s - loss: 0.2277 - accuracy: 0.92 - ETA: 0s - loss: 0.2266 - accuracy: 0.92 - ETA: 0s - loss: 0.2275 - accuracy: 0.92 - ETA: 0s - loss: 0.2275 - accuracy: 0.92 - ETA: 0s - loss: 0.2278 - accuracy: 0.92 - ETA: 0s - loss: 0.2313 - accuracy: 0.92 - 1s 4ms/step - loss: 0.2359 - accuracy: 0.9183 - val_loss: 0.2337 - val_accuracy: 0.9222\n",
      "Epoch 22/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.3574 - accuracy: 0.84 - ETA: 0s - loss: 0.2496 - accuracy: 0.91 - ETA: 0s - loss: 0.2378 - accuracy: 0.91 - ETA: 0s - loss: 0.2216 - accuracy: 0.92 - ETA: 0s - loss: 0.2217 - accuracy: 0.92 - ETA: 0s - loss: 0.2269 - accuracy: 0.92 - ETA: 0s - loss: 0.2239 - accuracy: 0.92 - ETA: 0s - loss: 0.2201 - accuracy: 0.92 - ETA: 0s - loss: 0.2209 - accuracy: 0.92 - ETA: 0s - loss: 0.2269 - accuracy: 0.92 - 1s 4ms/step - loss: 0.2292 - accuracy: 0.9206 - val_loss: 0.2234 - val_accuracy: 0.9302\n",
      "Epoch 23/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1805 - accuracy: 0.93 - ETA: 0s - loss: 0.2357 - accuracy: 0.92 - ETA: 0s - loss: 0.2186 - accuracy: 0.92 - ETA: 0s - loss: 0.2064 - accuracy: 0.93 - ETA: 0s - loss: 0.2139 - accuracy: 0.92 - ETA: 0s - loss: 0.2208 - accuracy: 0.92 - ETA: 0s - loss: 0.2205 - accuracy: 0.92 - ETA: 0s - loss: 0.2157 - accuracy: 0.92 - ETA: 0s - loss: 0.2154 - accuracy: 0.92 - ETA: 0s - loss: 0.2249 - accuracy: 0.92 - 1s 4ms/step - loss: 0.2273 - accuracy: 0.9213 - val_loss: 0.2267 - val_accuracy: 0.9211\n",
      "Epoch 24/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1700 - accuracy: 0.90 - ETA: 0s - loss: 0.2252 - accuracy: 0.92 - ETA: 0s - loss: 0.2102 - accuracy: 0.93 - ETA: 0s - loss: 0.2140 - accuracy: 0.93 - ETA: 0s - loss: 0.2202 - accuracy: 0.92 - ETA: 0s - loss: 0.2203 - accuracy: 0.92 - ETA: 0s - loss: 0.2167 - accuracy: 0.92 - ETA: 0s - loss: 0.2129 - accuracy: 0.92 - ETA: 0s - loss: 0.2127 - accuracy: 0.92 - ETA: 0s - loss: 0.2219 - accuracy: 0.92 - 1s 4ms/step - loss: 0.2225 - accuracy: 0.9229 - val_loss: 0.2216 - val_accuracy: 0.9233\n",
      "Epoch 25/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2238 - accuracy: 0.93 - ETA: 0s - loss: 0.2179 - accuracy: 0.92 - ETA: 0s - loss: 0.2038 - accuracy: 0.93 - ETA: 0s - loss: 0.2007 - accuracy: 0.93 - ETA: 0s - loss: 0.2067 - accuracy: 0.93 - ETA: 0s - loss: 0.2086 - accuracy: 0.92 - ETA: 0s - loss: 0.2066 - accuracy: 0.92 - ETA: 0s - loss: 0.2054 - accuracy: 0.93 - ETA: 0s - loss: 0.2057 - accuracy: 0.92 - ETA: 0s - loss: 0.2155 - accuracy: 0.92 - 1s 4ms/step - loss: 0.2163 - accuracy: 0.9259 - val_loss: 0.2151 - val_accuracy: 0.9256\n",
      "Epoch 26/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1871 - accuracy: 0.93 - ETA: 0s - loss: 0.2204 - accuracy: 0.92 - ETA: 0s - loss: 0.2174 - accuracy: 0.92 - ETA: 0s - loss: 0.2074 - accuracy: 0.93 - ETA: 0s - loss: 0.2072 - accuracy: 0.93 - ETA: 0s - loss: 0.2041 - accuracy: 0.93 - ETA: 0s - loss: 0.2044 - accuracy: 0.93 - ETA: 0s - loss: 0.2034 - accuracy: 0.92 - ETA: 0s - loss: 0.2035 - accuracy: 0.93 - ETA: 0s - loss: 0.2107 - accuracy: 0.92 - 1s 4ms/step - loss: 0.2142 - accuracy: 0.9257 - val_loss: 0.2115 - val_accuracy: 0.9302\n",
      "Epoch 27/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2010 - accuracy: 0.93 - ETA: 0s - loss: 0.2237 - accuracy: 0.92 - ETA: 0s - loss: 0.2108 - accuracy: 0.92 - ETA: 0s - loss: 0.2024 - accuracy: 0.93 - ETA: 0s - loss: 0.1988 - accuracy: 0.93 - ETA: 0s - loss: 0.2070 - accuracy: 0.92 - ETA: 0s - loss: 0.2052 - accuracy: 0.93 - ETA: 0s - loss: 0.1987 - accuracy: 0.93 - ETA: 0s - loss: 0.1978 - accuracy: 0.93 - ETA: 0s - loss: 0.2069 - accuracy: 0.93 - 1s 4ms/step - loss: 0.2099 - accuracy: 0.9293 - val_loss: 0.2117 - val_accuracy: 0.9279\n",
      "Epoch 28/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1863 - accuracy: 0.93 - ETA: 0s - loss: 0.2247 - accuracy: 0.92 - ETA: 0s - loss: 0.2106 - accuracy: 0.92 - ETA: 0s - loss: 0.1985 - accuracy: 0.93 - ETA: 0s - loss: 0.1990 - accuracy: 0.93 - ETA: 0s - loss: 0.2005 - accuracy: 0.92 - ETA: 0s - loss: 0.1978 - accuracy: 0.92 - ETA: 0s - loss: 0.1937 - accuracy: 0.93 - ETA: 0s - loss: 0.1948 - accuracy: 0.93 - ETA: 0s - loss: 0.2040 - accuracy: 0.92 - 1s 4ms/step - loss: 0.2060 - accuracy: 0.9277 - val_loss: 0.2047 - val_accuracy: 0.9336\n",
      "Epoch 29/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 5s - loss: 0.1394 - accuracy: 0.93 - ETA: 0s - loss: 0.1967 - accuracy: 0.93 - ETA: 0s - loss: 0.1863 - accuracy: 0.93 - ETA: 0s - loss: 0.1916 - accuracy: 0.93 - ETA: 0s - loss: 0.1947 - accuracy: 0.93 - ETA: 0s - loss: 0.1957 - accuracy: 0.93 - ETA: 0s - loss: 0.1945 - accuracy: 0.92 - ETA: 0s - loss: 0.1927 - accuracy: 0.93 - ETA: 0s - loss: 0.1920 - accuracy: 0.93 - ETA: 0s - loss: 0.2012 - accuracy: 0.92 - 1s 4ms/step - loss: 0.2022 - accuracy: 0.9282 - val_loss: 0.2022 - val_accuracy: 0.9382\n",
      "Epoch 30/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2196 - accuracy: 0.87 - ETA: 0s - loss: 0.1989 - accuracy: 0.92 - ETA: 0s - loss: 0.1870 - accuracy: 0.93 - ETA: 0s - loss: 0.1848 - accuracy: 0.93 - ETA: 0s - loss: 0.1891 - accuracy: 0.93 - ETA: 0s - loss: 0.1916 - accuracy: 0.93 - ETA: 0s - loss: 0.1907 - accuracy: 0.93 - ETA: 0s - loss: 0.1884 - accuracy: 0.93 - ETA: 0s - loss: 0.1889 - accuracy: 0.93 - ETA: 0s - loss: 0.1975 - accuracy: 0.93 - 1s 4ms/step - loss: 0.1982 - accuracy: 0.9309 - val_loss: 0.1987 - val_accuracy: 0.9314\n",
      "Epoch 31/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2280 - accuracy: 0.90 - ETA: 0s - loss: 0.2205 - accuracy: 0.92 - ETA: 0s - loss: 0.1910 - accuracy: 0.93 - ETA: 0s - loss: 0.1885 - accuracy: 0.93 - ETA: 0s - loss: 0.1878 - accuracy: 0.93 - ETA: 0s - loss: 0.1901 - accuracy: 0.93 - ETA: 0s - loss: 0.1914 - accuracy: 0.93 - ETA: 0s - loss: 0.1951 - accuracy: 0.93 - ETA: 0s - loss: 0.1970 - accuracy: 0.93 - 1s 4ms/step - loss: 0.2013 - accuracy: 0.9318 - val_loss: 0.2054 - val_accuracy: 0.9245\n",
      "Epoch 32/1000\n",
      "138/138 [==============================] - ETA: 6s - loss: 0.1843 - accuracy: 0.93 - ETA: 0s - loss: 0.2069 - accuracy: 0.93 - ETA: 0s - loss: 0.1864 - accuracy: 0.93 - ETA: 0s - loss: 0.1807 - accuracy: 0.94 - ETA: 0s - loss: 0.1793 - accuracy: 0.94 - ETA: 0s - loss: 0.1845 - accuracy: 0.93 - ETA: 0s - loss: 0.1865 - accuracy: 0.93 - ETA: 0s - loss: 0.1852 - accuracy: 0.93 - ETA: 0s - loss: 0.1905 - accuracy: 0.93 - 1s 4ms/step - loss: 0.1936 - accuracy: 0.9357 - val_loss: 0.1964 - val_accuracy: 0.9314\n",
      "Epoch 33/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1585 - accuracy: 0.93 - ETA: 0s - loss: 0.1878 - accuracy: 0.94 - ETA: 0s - loss: 0.1741 - accuracy: 0.94 - ETA: 0s - loss: 0.1747 - accuracy: 0.94 - ETA: 0s - loss: 0.1806 - accuracy: 0.93 - ETA: 0s - loss: 0.1804 - accuracy: 0.93 - ETA: 0s - loss: 0.1806 - accuracy: 0.93 - ETA: 0s - loss: 0.1788 - accuracy: 0.93 - ETA: 0s - loss: 0.1780 - accuracy: 0.93 - ETA: 0s - loss: 0.1875 - accuracy: 0.93 - 1s 4ms/step - loss: 0.1883 - accuracy: 0.9362 - val_loss: 0.1995 - val_accuracy: 0.9302\n",
      "Epoch 34/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1814 - accuracy: 0.93 - ETA: 0s - loss: 0.1915 - accuracy: 0.93 - ETA: 0s - loss: 0.1739 - accuracy: 0.94 - ETA: 0s - loss: 0.1735 - accuracy: 0.94 - ETA: 0s - loss: 0.1781 - accuracy: 0.94 - ETA: 0s - loss: 0.1816 - accuracy: 0.94 - ETA: 0s - loss: 0.1814 - accuracy: 0.94 - ETA: 0s - loss: 0.1795 - accuracy: 0.94 - ETA: 0s - loss: 0.1787 - accuracy: 0.94 - ETA: 0s - loss: 0.1870 - accuracy: 0.93 - 1s 4ms/step - loss: 0.1892 - accuracy: 0.9364 - val_loss: 0.1907 - val_accuracy: 0.9439\n",
      "Epoch 35/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.2166 - accuracy: 0.90 - ETA: 0s - loss: 0.2007 - accuracy: 0.92 - ETA: 0s - loss: 0.1824 - accuracy: 0.93 - ETA: 0s - loss: 0.1783 - accuracy: 0.93 - ETA: 0s - loss: 0.1804 - accuracy: 0.93 - ETA: 0s - loss: 0.1805 - accuracy: 0.93 - ETA: 0s - loss: 0.1782 - accuracy: 0.93 - ETA: 0s - loss: 0.1754 - accuracy: 0.93 - ETA: 0s - loss: 0.1762 - accuracy: 0.93 - ETA: 0s - loss: 0.1850 - accuracy: 0.93 - 1s 4ms/step - loss: 0.1867 - accuracy: 0.9350 - val_loss: 0.1830 - val_accuracy: 0.9416\n",
      "Epoch 36/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1556 - accuracy: 0.93 - ETA: 0s - loss: 0.1940 - accuracy: 0.93 - ETA: 0s - loss: 0.1781 - accuracy: 0.94 - ETA: 0s - loss: 0.1753 - accuracy: 0.94 - ETA: 0s - loss: 0.1788 - accuracy: 0.94 - ETA: 0s - loss: 0.1792 - accuracy: 0.94 - ETA: 0s - loss: 0.1792 - accuracy: 0.93 - ETA: 0s - loss: 0.1761 - accuracy: 0.94 - ETA: 0s - loss: 0.1754 - accuracy: 0.94 - ETA: 0s - loss: 0.1855 - accuracy: 0.93 - 1s 4ms/step - loss: 0.1859 - accuracy: 0.9373 - val_loss: 0.1833 - val_accuracy: 0.9439\n",
      "Epoch 37/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1234 - accuracy: 0.93 - ETA: 0s - loss: 0.1675 - accuracy: 0.94 - ETA: 0s - loss: 0.1626 - accuracy: 0.94 - ETA: 0s - loss: 0.1622 - accuracy: 0.94 - ETA: 0s - loss: 0.1645 - accuracy: 0.94 - ETA: 0s - loss: 0.1707 - accuracy: 0.94 - ETA: 0s - loss: 0.1693 - accuracy: 0.94 - ETA: 0s - loss: 0.1660 - accuracy: 0.94 - ETA: 0s - loss: 0.1661 - accuracy: 0.94 - ETA: 0s - loss: 0.1758 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1798 - accuracy: 0.9389 - val_loss: 0.1772 - val_accuracy: 0.9474\n",
      "Epoch 38/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1694 - accuracy: 0.90 - ETA: 0s - loss: 0.1841 - accuracy: 0.93 - ETA: 0s - loss: 0.1699 - accuracy: 0.94 - ETA: 0s - loss: 0.1641 - accuracy: 0.94 - ETA: 0s - loss: 0.1675 - accuracy: 0.94 - ETA: 0s - loss: 0.1698 - accuracy: 0.94 - ETA: 0s - loss: 0.1710 - accuracy: 0.93 - ETA: 0s - loss: 0.1690 - accuracy: 0.94 - ETA: 0s - loss: 0.1665 - accuracy: 0.94 - ETA: 0s - loss: 0.1755 - accuracy: 0.93 - 1s 4ms/step - loss: 0.1791 - accuracy: 0.9387 - val_loss: 0.1755 - val_accuracy: 0.9474\n",
      "Epoch 39/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1405 - accuracy: 0.96 - ETA: 0s - loss: 0.1693 - accuracy: 0.94 - ETA: 0s - loss: 0.1594 - accuracy: 0.94 - ETA: 0s - loss: 0.1578 - accuracy: 0.95 - ETA: 0s - loss: 0.1624 - accuracy: 0.94 - ETA: 0s - loss: 0.1629 - accuracy: 0.94 - ETA: 0s - loss: 0.1645 - accuracy: 0.94 - ETA: 0s - loss: 0.1643 - accuracy: 0.94 - ETA: 0s - loss: 0.1637 - accuracy: 0.94 - ETA: 0s - loss: 0.1732 - accuracy: 0.93 - 1s 4ms/step - loss: 0.1739 - accuracy: 0.9385 - val_loss: 0.1750 - val_accuracy: 0.9439\n",
      "Epoch 40/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1880 - accuracy: 0.90 - ETA: 0s - loss: 0.1699 - accuracy: 0.94 - ETA: 0s - loss: 0.1610 - accuracy: 0.94 - ETA: 0s - loss: 0.1578 - accuracy: 0.94 - ETA: 0s - loss: 0.1677 - accuracy: 0.94 - ETA: 0s - loss: 0.1733 - accuracy: 0.93 - ETA: 0s - loss: 0.1746 - accuracy: 0.93 - ETA: 0s - loss: 0.1705 - accuracy: 0.93 - ETA: 0s - loss: 0.1702 - accuracy: 0.94 - ETA: 0s - loss: 0.1803 - accuracy: 0.93 - 1s 4ms/step - loss: 0.1819 - accuracy: 0.9364 - val_loss: 0.1975 - val_accuracy: 0.9314\n",
      "Epoch 41/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0982 - accuracy: 0.93 - ETA: 0s - loss: 0.1685 - accuracy: 0.94 - ETA: 0s - loss: 0.1587 - accuracy: 0.94 - ETA: 0s - loss: 0.1594 - accuracy: 0.95 - ETA: 0s - loss: 0.1602 - accuracy: 0.94 - ETA: 0s - loss: 0.1662 - accuracy: 0.94 - ETA: 0s - loss: 0.1665 - accuracy: 0.94 - ETA: 0s - loss: 0.1664 - accuracy: 0.94 - ETA: 0s - loss: 0.1692 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1735 - accuracy: 0.9426 - val_loss: 0.1796 - val_accuracy: 0.9325\n",
      "Epoch 42/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0927 - accuracy: 0.96 - ETA: 0s - loss: 0.1609 - accuracy: 0.94 - ETA: 0s - loss: 0.1479 - accuracy: 0.94 - ETA: 0s - loss: 0.1454 - accuracy: 0.95 - ETA: 0s - loss: 0.1498 - accuracy: 0.95 - ETA: 0s - loss: 0.1497 - accuracy: 0.94 - ETA: 0s - loss: 0.1532 - accuracy: 0.94 - ETA: 0s - loss: 0.1547 - accuracy: 0.94 - ETA: 0s - loss: 0.1608 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1647 - accuracy: 0.9449 - val_loss: 0.1721 - val_accuracy: 0.9405\n",
      "Epoch 43/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 5s - loss: 0.1080 - accuracy: 0.93 - ETA: 0s - loss: 0.1728 - accuracy: 0.94 - ETA: 0s - loss: 0.1536 - accuracy: 0.95 - ETA: 0s - loss: 0.1516 - accuracy: 0.95 - ETA: 0s - loss: 0.1596 - accuracy: 0.94 - ETA: 0s - loss: 0.1620 - accuracy: 0.94 - ETA: 0s - loss: 0.1588 - accuracy: 0.94 - ETA: 0s - loss: 0.1572 - accuracy: 0.94 - ETA: 0s - loss: 0.1553 - accuracy: 0.94 - ETA: 0s - loss: 0.1630 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1646 - accuracy: 0.9440 - val_loss: 0.1659 - val_accuracy: 0.9462\n",
      "Epoch 44/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1198 - accuracy: 0.96 - ETA: 0s - loss: 0.1614 - accuracy: 0.94 - ETA: 0s - loss: 0.1507 - accuracy: 0.95 - ETA: 0s - loss: 0.1519 - accuracy: 0.95 - ETA: 0s - loss: 0.1556 - accuracy: 0.94 - ETA: 0s - loss: 0.1562 - accuracy: 0.94 - ETA: 0s - loss: 0.1557 - accuracy: 0.94 - ETA: 0s - loss: 0.1525 - accuracy: 0.94 - ETA: 0s - loss: 0.1509 - accuracy: 0.94 - ETA: 0s - loss: 0.1592 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1622 - accuracy: 0.9440 - val_loss: 0.1614 - val_accuracy: 0.9497\n",
      "Epoch 45/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1461 - accuracy: 0.93 - ETA: 0s - loss: 0.1595 - accuracy: 0.94 - ETA: 0s - loss: 0.1528 - accuracy: 0.94 - ETA: 0s - loss: 0.1476 - accuracy: 0.94 - ETA: 0s - loss: 0.1541 - accuracy: 0.94 - ETA: 0s - loss: 0.1587 - accuracy: 0.94 - ETA: 0s - loss: 0.1592 - accuracy: 0.94 - ETA: 0s - loss: 0.1565 - accuracy: 0.94 - ETA: 0s - loss: 0.1553 - accuracy: 0.94 - ETA: 0s - loss: 0.1629 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1672 - accuracy: 0.9412 - val_loss: 0.1975 - val_accuracy: 0.9268\n",
      "Epoch 46/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1213 - accuracy: 0.93 - ETA: 0s - loss: 0.1777 - accuracy: 0.93 - ETA: 0s - loss: 0.1745 - accuracy: 0.94 - ETA: 0s - loss: 0.1607 - accuracy: 0.94 - ETA: 0s - loss: 0.1622 - accuracy: 0.94 - ETA: 0s - loss: 0.1638 - accuracy: 0.94 - ETA: 0s - loss: 0.1606 - accuracy: 0.94 - ETA: 0s - loss: 0.1627 - accuracy: 0.94 - ETA: 0s - loss: 0.1637 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1675 - accuracy: 0.9405 - val_loss: 0.1605 - val_accuracy: 0.9439\n",
      "Epoch 47/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0834 - accuracy: 0.93 - ETA: 0s - loss: 0.1473 - accuracy: 0.94 - ETA: 0s - loss: 0.1411 - accuracy: 0.94 - ETA: 0s - loss: 0.1414 - accuracy: 0.95 - ETA: 0s - loss: 0.1404 - accuracy: 0.95 - ETA: 0s - loss: 0.1448 - accuracy: 0.95 - ETA: 0s - loss: 0.1497 - accuracy: 0.94 - ETA: 0s - loss: 0.1504 - accuracy: 0.94 - ETA: 0s - loss: 0.1527 - accuracy: 0.94 - ETA: 0s - loss: 0.1609 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1614 - accuracy: 0.9451 - val_loss: 0.1629 - val_accuracy: 0.9451\n",
      "Epoch 48/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0981 - accuracy: 0.96 - ETA: 0s - loss: 0.1413 - accuracy: 0.95 - ETA: 0s - loss: 0.1317 - accuracy: 0.95 - ETA: 0s - loss: 0.1343 - accuracy: 0.95 - ETA: 0s - loss: 0.1371 - accuracy: 0.95 - ETA: 0s - loss: 0.1426 - accuracy: 0.95 - ETA: 0s - loss: 0.1445 - accuracy: 0.94 - ETA: 0s - loss: 0.1440 - accuracy: 0.95 - ETA: 0s - loss: 0.1453 - accuracy: 0.94 - ETA: 0s - loss: 0.1550 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1554 - accuracy: 0.9460 - val_loss: 0.1598 - val_accuracy: 0.9428\n",
      "Epoch 49/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1171 - accuracy: 0.93 - ETA: 0s - loss: 0.1519 - accuracy: 0.94 - ETA: 0s - loss: 0.1403 - accuracy: 0.94 - ETA: 0s - loss: 0.1372 - accuracy: 0.95 - ETA: 0s - loss: 0.1438 - accuracy: 0.94 - ETA: 0s - loss: 0.1449 - accuracy: 0.95 - ETA: 0s - loss: 0.1429 - accuracy: 0.95 - ETA: 0s - loss: 0.1416 - accuracy: 0.94 - ETA: 0s - loss: 0.1403 - accuracy: 0.94 - ETA: 0s - loss: 0.1516 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1528 - accuracy: 0.9474 - val_loss: 0.1716 - val_accuracy: 0.9394\n",
      "Epoch 50/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0897 - accuracy: 0.93 - ETA: 0s - loss: 0.1400 - accuracy: 0.94 - ETA: 0s - loss: 0.1356 - accuracy: 0.94 - ETA: 0s - loss: 0.1350 - accuracy: 0.95 - ETA: 0s - loss: 0.1421 - accuracy: 0.94 - ETA: 0s - loss: 0.1415 - accuracy: 0.94 - ETA: 0s - loss: 0.1401 - accuracy: 0.94 - ETA: 0s - loss: 0.1412 - accuracy: 0.94 - ETA: 0s - loss: 0.1419 - accuracy: 0.94 - ETA: 0s - loss: 0.1499 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1502 - accuracy: 0.9465 - val_loss: 0.1508 - val_accuracy: 0.9462\n",
      "Epoch 51/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0753 - accuracy: 0.96 - ETA: 0s - loss: 0.1555 - accuracy: 0.95 - ETA: 0s - loss: 0.1439 - accuracy: 0.95 - ETA: 0s - loss: 0.1446 - accuracy: 0.95 - ETA: 0s - loss: 0.1432 - accuracy: 0.95 - ETA: 0s - loss: 0.1511 - accuracy: 0.95 - ETA: 0s - loss: 0.1490 - accuracy: 0.95 - ETA: 0s - loss: 0.1458 - accuracy: 0.95 - ETA: 0s - loss: 0.1482 - accuracy: 0.95 - ETA: 0s - loss: 0.1555 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1574 - accuracy: 0.9497 - val_loss: 0.1564 - val_accuracy: 0.9451\n",
      "Epoch 52/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0929 - accuracy: 0.96 - ETA: 0s - loss: 0.1457 - accuracy: 0.95 - ETA: 0s - loss: 0.1456 - accuracy: 0.95 - ETA: 0s - loss: 0.1405 - accuracy: 0.95 - ETA: 0s - loss: 0.1463 - accuracy: 0.95 - ETA: 0s - loss: 0.1479 - accuracy: 0.94 - ETA: 0s - loss: 0.1483 - accuracy: 0.94 - ETA: 0s - loss: 0.1457 - accuracy: 0.94 - ETA: 0s - loss: 0.1460 - accuracy: 0.94 - ETA: 0s - loss: 0.1522 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1531 - accuracy: 0.9474 - val_loss: 0.1626 - val_accuracy: 0.9416\n",
      "Epoch 53/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0866 - accuracy: 0.93 - ETA: 0s - loss: 0.1421 - accuracy: 0.95 - ETA: 0s - loss: 0.1337 - accuracy: 0.95 - ETA: 0s - loss: 0.1325 - accuracy: 0.95 - ETA: 0s - loss: 0.1343 - accuracy: 0.95 - ETA: 0s - loss: 0.1400 - accuracy: 0.95 - ETA: 0s - loss: 0.1365 - accuracy: 0.95 - ETA: 0s - loss: 0.1368 - accuracy: 0.95 - ETA: 0s - loss: 0.1393 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1416 - accuracy: 0.9545 - val_loss: 0.1400 - val_accuracy: 0.9508\n",
      "Epoch 54/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1047 - accuracy: 0.96 - ETA: 0s - loss: 0.1379 - accuracy: 0.95 - ETA: 0s - loss: 0.1421 - accuracy: 0.95 - ETA: 0s - loss: 0.1386 - accuracy: 0.95 - ETA: 0s - loss: 0.1358 - accuracy: 0.95 - ETA: 0s - loss: 0.1367 - accuracy: 0.95 - ETA: 0s - loss: 0.1393 - accuracy: 0.95 - ETA: 0s - loss: 0.1380 - accuracy: 0.95 - ETA: 0s - loss: 0.1385 - accuracy: 0.95 - ETA: 0s - loss: 0.1464 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1487 - accuracy: 0.9497 - val_loss: 0.1467 - val_accuracy: 0.9451\n",
      "Epoch 55/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1470 - accuracy: 0.93 - ETA: 0s - loss: 0.1530 - accuracy: 0.94 - ETA: 0s - loss: 0.1339 - accuracy: 0.95 - ETA: 0s - loss: 0.1340 - accuracy: 0.94 - ETA: 0s - loss: 0.1354 - accuracy: 0.94 - ETA: 0s - loss: 0.1359 - accuracy: 0.94 - ETA: 0s - loss: 0.1393 - accuracy: 0.94 - ETA: 0s - loss: 0.1367 - accuracy: 0.95 - ETA: 0s - loss: 0.1381 - accuracy: 0.95 - ETA: 0s - loss: 0.1446 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1460 - accuracy: 0.9483 - val_loss: 0.1473 - val_accuracy: 0.9474\n",
      "Epoch 56/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1557 - accuracy: 0.93 - ETA: 0s - loss: 0.1311 - accuracy: 0.95 - ETA: 0s - loss: 0.1216 - accuracy: 0.96 - ETA: 0s - loss: 0.1178 - accuracy: 0.96 - ETA: 0s - loss: 0.1227 - accuracy: 0.95 - ETA: 0s - loss: 0.1254 - accuracy: 0.95 - ETA: 0s - loss: 0.1251 - accuracy: 0.95 - ETA: 0s - loss: 0.1233 - accuracy: 0.95 - ETA: 0s - loss: 0.1235 - accuracy: 0.95 - ETA: 0s - loss: 0.1336 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1354 - accuracy: 0.9524 - val_loss: 0.1382 - val_accuracy: 0.9508\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 5s - loss: 0.0735 - accuracy: 0.96 - ETA: 0s - loss: 0.1345 - accuracy: 0.95 - ETA: 0s - loss: 0.1297 - accuracy: 0.95 - ETA: 0s - loss: 0.1294 - accuracy: 0.95 - ETA: 0s - loss: 0.1270 - accuracy: 0.95 - ETA: 0s - loss: 0.1305 - accuracy: 0.95 - ETA: 0s - loss: 0.1319 - accuracy: 0.95 - ETA: 0s - loss: 0.1297 - accuracy: 0.95 - ETA: 0s - loss: 0.1298 - accuracy: 0.95 - ETA: 0s - loss: 0.1397 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1394 - accuracy: 0.9527 - val_loss: 0.1371 - val_accuracy: 0.9542\n",
      "Epoch 58/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0838 - accuracy: 1.00 - ETA: 0s - loss: 0.1390 - accuracy: 0.95 - ETA: 0s - loss: 0.1290 - accuracy: 0.95 - ETA: 0s - loss: 0.1325 - accuracy: 0.95 - ETA: 0s - loss: 0.1331 - accuracy: 0.95 - ETA: 0s - loss: 0.1317 - accuracy: 0.95 - ETA: 0s - loss: 0.1301 - accuracy: 0.95 - ETA: 0s - loss: 0.1307 - accuracy: 0.95 - ETA: 0s - loss: 0.1320 - accuracy: 0.95 - ETA: 0s - loss: 0.1407 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1414 - accuracy: 0.9511 - val_loss: 0.1344 - val_accuracy: 0.9519\n",
      "Epoch 59/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1271 - accuracy: 0.96 - ETA: 0s - loss: 0.1574 - accuracy: 0.94 - ETA: 0s - loss: 0.1389 - accuracy: 0.94 - ETA: 0s - loss: 0.1305 - accuracy: 0.95 - ETA: 0s - loss: 0.1333 - accuracy: 0.95 - ETA: 0s - loss: 0.1363 - accuracy: 0.95 - ETA: 0s - loss: 0.1373 - accuracy: 0.95 - ETA: 0s - loss: 0.1359 - accuracy: 0.95 - ETA: 0s - loss: 0.1362 - accuracy: 0.95 - ETA: 0s - loss: 0.1424 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1431 - accuracy: 0.9488 - val_loss: 0.1267 - val_accuracy: 0.9611\n",
      "Epoch 60/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1601 - accuracy: 0.96 - ETA: 0s - loss: 0.1355 - accuracy: 0.95 - ETA: 0s - loss: 0.1322 - accuracy: 0.95 - ETA: 0s - loss: 0.1334 - accuracy: 0.95 - ETA: 0s - loss: 0.1308 - accuracy: 0.95 - ETA: 0s - loss: 0.1313 - accuracy: 0.95 - ETA: 0s - loss: 0.1324 - accuracy: 0.95 - ETA: 0s - loss: 0.1297 - accuracy: 0.95 - ETA: 0s - loss: 0.1308 - accuracy: 0.95 - ETA: 0s - loss: 0.1393 - accuracy: 0.94 - 1s 4ms/step - loss: 0.1396 - accuracy: 0.9495 - val_loss: 0.1318 - val_accuracy: 0.9531\n",
      "Epoch 61/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1574 - accuracy: 0.93 - ETA: 0s - loss: 0.1155 - accuracy: 0.95 - ETA: 0s - loss: 0.1140 - accuracy: 0.95 - ETA: 0s - loss: 0.1131 - accuracy: 0.95 - ETA: 0s - loss: 0.1247 - accuracy: 0.95 - ETA: 0s - loss: 0.1259 - accuracy: 0.95 - ETA: 0s - loss: 0.1245 - accuracy: 0.95 - ETA: 0s - loss: 0.1218 - accuracy: 0.95 - ETA: 0s - loss: 0.1277 - accuracy: 0.95 - ETA: 0s - loss: 0.1343 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1344 - accuracy: 0.9527 - val_loss: 0.1265 - val_accuracy: 0.9519\n",
      "Epoch 62/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0754 - accuracy: 0.96 - ETA: 0s - loss: 0.1331 - accuracy: 0.95 - ETA: 0s - loss: 0.1236 - accuracy: 0.96 - ETA: 0s - loss: 0.1271 - accuracy: 0.95 - ETA: 0s - loss: 0.1314 - accuracy: 0.95 - ETA: 0s - loss: 0.1327 - accuracy: 0.95 - ETA: 0s - loss: 0.1318 - accuracy: 0.95 - ETA: 0s - loss: 0.1312 - accuracy: 0.95 - ETA: 0s - loss: 0.1311 - accuracy: 0.95 - ETA: 0s - loss: 0.1378 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1383 - accuracy: 0.9533 - val_loss: 0.1253 - val_accuracy: 0.9542\n",
      "Epoch 63/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0791 - accuracy: 0.96 - ETA: 0s - loss: 0.1177 - accuracy: 0.96 - ETA: 0s - loss: 0.1079 - accuracy: 0.96 - ETA: 0s - loss: 0.1030 - accuracy: 0.96 - ETA: 0s - loss: 0.1103 - accuracy: 0.96 - ETA: 0s - loss: 0.1119 - accuracy: 0.96 - ETA: 0s - loss: 0.1151 - accuracy: 0.95 - ETA: 0s - loss: 0.1190 - accuracy: 0.95 - ETA: 0s - loss: 0.1230 - accuracy: 0.95 - ETA: 0s - loss: 0.1305 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1307 - accuracy: 0.9552 - val_loss: 0.1264 - val_accuracy: 0.9542\n",
      "Epoch 64/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0637 - accuracy: 0.96 - ETA: 0s - loss: 0.1203 - accuracy: 0.95 - ETA: 0s - loss: 0.1165 - accuracy: 0.95 - ETA: 0s - loss: 0.1176 - accuracy: 0.95 - ETA: 0s - loss: 0.1164 - accuracy: 0.95 - ETA: 0s - loss: 0.1218 - accuracy: 0.95 - ETA: 0s - loss: 0.1231 - accuracy: 0.95 - ETA: 0s - loss: 0.1206 - accuracy: 0.95 - ETA: 0s - loss: 0.1185 - accuracy: 0.95 - ETA: 0s - loss: 0.1263 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1281 - accuracy: 0.9561 - val_loss: 0.1271 - val_accuracy: 0.9474\n",
      "Epoch 65/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0589 - accuracy: 0.96 - ETA: 0s - loss: 0.1212 - accuracy: 0.95 - ETA: 0s - loss: 0.1341 - accuracy: 0.95 - ETA: 0s - loss: 0.1302 - accuracy: 0.95 - ETA: 0s - loss: 0.1281 - accuracy: 0.95 - ETA: 0s - loss: 0.1273 - accuracy: 0.95 - ETA: 0s - loss: 0.1272 - accuracy: 0.95 - ETA: 0s - loss: 0.1281 - accuracy: 0.95 - ETA: 0s - loss: 0.1289 - accuracy: 0.95 - ETA: 0s - loss: 0.1349 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1354 - accuracy: 0.9531 - val_loss: 0.1184 - val_accuracy: 0.9588\n",
      "Epoch 66/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0953 - accuracy: 0.93 - ETA: 0s - loss: 0.1304 - accuracy: 0.95 - ETA: 0s - loss: 0.1270 - accuracy: 0.95 - ETA: 0s - loss: 0.1176 - accuracy: 0.95 - ETA: 0s - loss: 0.1176 - accuracy: 0.95 - ETA: 0s - loss: 0.1201 - accuracy: 0.95 - ETA: 0s - loss: 0.1220 - accuracy: 0.95 - ETA: 0s - loss: 0.1194 - accuracy: 0.95 - ETA: 0s - loss: 0.1195 - accuracy: 0.95 - ETA: 0s - loss: 0.1251 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1263 - accuracy: 0.9531 - val_loss: 0.1130 - val_accuracy: 0.9600\n",
      "Epoch 67/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0601 - accuracy: 1.00 - ETA: 0s - loss: 0.1163 - accuracy: 0.96 - ETA: 0s - loss: 0.1161 - accuracy: 0.96 - ETA: 0s - loss: 0.1189 - accuracy: 0.95 - ETA: 0s - loss: 0.1220 - accuracy: 0.95 - ETA: 0s - loss: 0.1194 - accuracy: 0.95 - ETA: 0s - loss: 0.1195 - accuracy: 0.95 - ETA: 0s - loss: 0.1177 - accuracy: 0.95 - ETA: 0s - loss: 0.1164 - accuracy: 0.95 - ETA: 0s - loss: 0.1247 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1254 - accuracy: 0.9540 - val_loss: 0.1204 - val_accuracy: 0.9531\n",
      "Epoch 68/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0873 - accuracy: 0.96 - ETA: 0s - loss: 0.1186 - accuracy: 0.96 - ETA: 0s - loss: 0.1107 - accuracy: 0.96 - ETA: 0s - loss: 0.1139 - accuracy: 0.95 - ETA: 0s - loss: 0.1215 - accuracy: 0.95 - ETA: 0s - loss: 0.1199 - accuracy: 0.95 - ETA: 0s - loss: 0.1205 - accuracy: 0.95 - ETA: 0s - loss: 0.1176 - accuracy: 0.95 - ETA: 0s - loss: 0.1167 - accuracy: 0.95 - ETA: 0s - loss: 0.1256 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1257 - accuracy: 0.9556 - val_loss: 0.1197 - val_accuracy: 0.9531\n",
      "Epoch 69/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0973 - accuracy: 0.96 - ETA: 0s - loss: 0.1289 - accuracy: 0.95 - ETA: 0s - loss: 0.1204 - accuracy: 0.96 - ETA: 0s - loss: 0.1194 - accuracy: 0.95 - ETA: 0s - loss: 0.1141 - accuracy: 0.95 - ETA: 0s - loss: 0.1196 - accuracy: 0.95 - ETA: 0s - loss: 0.1203 - accuracy: 0.95 - ETA: 0s - loss: 0.1209 - accuracy: 0.95 - ETA: 0s - loss: 0.1232 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1248 - accuracy: 0.9545 - val_loss: 0.1222 - val_accuracy: 0.9531\n",
      "Epoch 70/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0837 - accuracy: 0.93 - ETA: 0s - loss: 0.1130 - accuracy: 0.96 - ETA: 0s - loss: 0.1072 - accuracy: 0.96 - ETA: 0s - loss: 0.1081 - accuracy: 0.96 - ETA: 0s - loss: 0.1076 - accuracy: 0.96 - ETA: 0s - loss: 0.1100 - accuracy: 0.95 - ETA: 0s - loss: 0.1120 - accuracy: 0.95 - ETA: 0s - loss: 0.1149 - accuracy: 0.95 - ETA: 0s - loss: 0.1164 - accuracy: 0.95 - ETA: 0s - loss: 0.1207 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1212 - accuracy: 0.9575 - val_loss: 0.1058 - val_accuracy: 0.9611\n",
      "Epoch 71/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 5s - loss: 0.0486 - accuracy: 1.00 - ETA: 0s - loss: 0.1230 - accuracy: 0.95 - ETA: 0s - loss: 0.1188 - accuracy: 0.95 - ETA: 0s - loss: 0.1080 - accuracy: 0.96 - ETA: 0s - loss: 0.1060 - accuracy: 0.96 - ETA: 0s - loss: 0.1067 - accuracy: 0.96 - ETA: 0s - loss: 0.1100 - accuracy: 0.96 - ETA: 0s - loss: 0.1081 - accuracy: 0.96 - ETA: 0s - loss: 0.1095 - accuracy: 0.96 - ETA: 0s - loss: 0.1148 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1159 - accuracy: 0.9593 - val_loss: 0.1137 - val_accuracy: 0.9588\n",
      "Epoch 72/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0803 - accuracy: 0.93 - ETA: 0s - loss: 0.1088 - accuracy: 0.95 - ETA: 0s - loss: 0.1048 - accuracy: 0.95 - ETA: 0s - loss: 0.1126 - accuracy: 0.95 - ETA: 0s - loss: 0.1092 - accuracy: 0.96 - ETA: 0s - loss: 0.1088 - accuracy: 0.96 - ETA: 0s - loss: 0.1113 - accuracy: 0.95 - ETA: 0s - loss: 0.1114 - accuracy: 0.95 - ETA: 0s - loss: 0.1120 - accuracy: 0.96 - ETA: 0s - loss: 0.1157 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1174 - accuracy: 0.9581 - val_loss: 0.1145 - val_accuracy: 0.9542\n",
      "Epoch 73/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0797 - accuracy: 0.96 - ETA: 0s - loss: 0.1265 - accuracy: 0.95 - ETA: 0s - loss: 0.1185 - accuracy: 0.95 - ETA: 0s - loss: 0.1145 - accuracy: 0.96 - ETA: 0s - loss: 0.1154 - accuracy: 0.95 - ETA: 0s - loss: 0.1169 - accuracy: 0.95 - ETA: 0s - loss: 0.1153 - accuracy: 0.95 - ETA: 0s - loss: 0.1132 - accuracy: 0.95 - ETA: 0s - loss: 0.1113 - accuracy: 0.96 - ETA: 0s - loss: 0.1166 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1170 - accuracy: 0.9577 - val_loss: 0.1001 - val_accuracy: 0.9634\n",
      "Epoch 74/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1290 - accuracy: 0.90 - ETA: 0s - loss: 0.1190 - accuracy: 0.95 - ETA: 0s - loss: 0.1142 - accuracy: 0.95 - ETA: 0s - loss: 0.1097 - accuracy: 0.95 - ETA: 0s - loss: 0.1064 - accuracy: 0.95 - ETA: 0s - loss: 0.1060 - accuracy: 0.95 - ETA: 0s - loss: 0.1075 - accuracy: 0.95 - ETA: 0s - loss: 0.1062 - accuracy: 0.95 - ETA: 0s - loss: 0.1052 - accuracy: 0.95 - ETA: 0s - loss: 0.1124 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1136 - accuracy: 0.9561 - val_loss: 0.1076 - val_accuracy: 0.9600\n",
      "Epoch 75/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0766 - accuracy: 0.96 - ETA: 0s - loss: 0.1132 - accuracy: 0.95 - ETA: 0s - loss: 0.1015 - accuracy: 0.96 - ETA: 0s - loss: 0.1075 - accuracy: 0.96 - ETA: 0s - loss: 0.1089 - accuracy: 0.96 - ETA: 0s - loss: 0.1091 - accuracy: 0.96 - ETA: 0s - loss: 0.1107 - accuracy: 0.96 - ETA: 0s - loss: 0.1111 - accuracy: 0.96 - ETA: 0s - loss: 0.1130 - accuracy: 0.96 - ETA: 0s - loss: 0.1163 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1161 - accuracy: 0.9593 - val_loss: 0.1077 - val_accuracy: 0.9600\n",
      "Epoch 76/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0597 - accuracy: 0.93 - ETA: 0s - loss: 0.1161 - accuracy: 0.95 - ETA: 0s - loss: 0.1056 - accuracy: 0.96 - ETA: 0s - loss: 0.0982 - accuracy: 0.96 - ETA: 0s - loss: 0.1012 - accuracy: 0.96 - ETA: 0s - loss: 0.1041 - accuracy: 0.96 - ETA: 0s - loss: 0.1088 - accuracy: 0.95 - ETA: 0s - loss: 0.1104 - accuracy: 0.95 - ETA: 0s - loss: 0.1124 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1159 - accuracy: 0.9570 - val_loss: 0.1022 - val_accuracy: 0.9645\n",
      "Epoch 77/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1132 - accuracy: 0.93 - ETA: 0s - loss: 0.1141 - accuracy: 0.95 - ETA: 0s - loss: 0.1101 - accuracy: 0.96 - ETA: 0s - loss: 0.1050 - accuracy: 0.96 - ETA: 0s - loss: 0.1025 - accuracy: 0.96 - ETA: 0s - loss: 0.1040 - accuracy: 0.96 - ETA: 0s - loss: 0.1040 - accuracy: 0.96 - ETA: 0s - loss: 0.1025 - accuracy: 0.96 - ETA: 0s - loss: 0.1031 - accuracy: 0.96 - ETA: 0s - loss: 0.1088 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1087 - accuracy: 0.9584 - val_loss: 0.1125 - val_accuracy: 0.9565\n",
      "Epoch 78/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0900 - accuracy: 0.93 - ETA: 0s - loss: 0.1133 - accuracy: 0.95 - ETA: 0s - loss: 0.1164 - accuracy: 0.96 - ETA: 0s - loss: 0.1145 - accuracy: 0.95 - ETA: 0s - loss: 0.1111 - accuracy: 0.96 - ETA: 0s - loss: 0.1096 - accuracy: 0.96 - ETA: 0s - loss: 0.1088 - accuracy: 0.96 - ETA: 0s - loss: 0.1078 - accuracy: 0.96 - ETA: 0s - loss: 0.1102 - accuracy: 0.96 - ETA: 0s - loss: 0.1146 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1145 - accuracy: 0.9591 - val_loss: 0.1079 - val_accuracy: 0.9588\n",
      "Epoch 79/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0988 - accuracy: 0.93 - ETA: 0s - loss: 0.1130 - accuracy: 0.96 - ETA: 0s - loss: 0.1035 - accuracy: 0.96 - ETA: 0s - loss: 0.0961 - accuracy: 0.96 - ETA: 0s - loss: 0.0987 - accuracy: 0.96 - ETA: 0s - loss: 0.1036 - accuracy: 0.96 - ETA: 0s - loss: 0.1075 - accuracy: 0.96 - ETA: 0s - loss: 0.1097 - accuracy: 0.96 - ETA: 0s - loss: 0.1123 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1152 - accuracy: 0.9586 - val_loss: 0.0987 - val_accuracy: 0.9657\n",
      "Epoch 80/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1113 - accuracy: 0.93 - ETA: 0s - loss: 0.1072 - accuracy: 0.96 - ETA: 0s - loss: 0.1076 - accuracy: 0.96 - ETA: 0s - loss: 0.1050 - accuracy: 0.95 - ETA: 0s - loss: 0.1016 - accuracy: 0.96 - ETA: 0s - loss: 0.1016 - accuracy: 0.96 - ETA: 0s - loss: 0.1033 - accuracy: 0.96 - ETA: 0s - loss: 0.0991 - accuracy: 0.96 - ETA: 0s - loss: 0.0988 - accuracy: 0.96 - ETA: 0s - loss: 0.1063 - accuracy: 0.96 - 1s 4ms/step - loss: 0.1072 - accuracy: 0.9613 - val_loss: 0.0976 - val_accuracy: 0.9611\n",
      "Epoch 81/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0746 - accuracy: 0.96 - ETA: 0s - loss: 0.0933 - accuracy: 0.96 - ETA: 0s - loss: 0.0980 - accuracy: 0.96 - ETA: 0s - loss: 0.0909 - accuracy: 0.96 - ETA: 0s - loss: 0.0951 - accuracy: 0.96 - ETA: 0s - loss: 0.1046 - accuracy: 0.95 - ETA: 0s - loss: 0.1028 - accuracy: 0.95 - ETA: 0s - loss: 0.0994 - accuracy: 0.96 - ETA: 0s - loss: 0.1010 - accuracy: 0.96 - ETA: 0s - loss: 0.1066 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1076 - accuracy: 0.9586 - val_loss: 0.0957 - val_accuracy: 0.9714\n",
      "Epoch 82/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.1480 - accuracy: 0.93 - ETA: 0s - loss: 0.1077 - accuracy: 0.96 - ETA: 0s - loss: 0.0984 - accuracy: 0.96 - ETA: 0s - loss: 0.0938 - accuracy: 0.96 - ETA: 0s - loss: 0.0979 - accuracy: 0.96 - ETA: 0s - loss: 0.1108 - accuracy: 0.95 - ETA: 0s - loss: 0.1122 - accuracy: 0.95 - ETA: 0s - loss: 0.1100 - accuracy: 0.96 - ETA: 0s - loss: 0.1086 - accuracy: 0.96 - ETA: 0s - loss: 0.1099 - accuracy: 0.96 - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9611 - val_loss: 0.1003 - val_accuracy: 0.9577\n",
      "Epoch 83/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0350 - accuracy: 1.00 - ETA: 0s - loss: 0.0902 - accuracy: 0.97 - ETA: 0s - loss: 0.0970 - accuracy: 0.96 - ETA: 0s - loss: 0.0997 - accuracy: 0.96 - ETA: 0s - loss: 0.0979 - accuracy: 0.96 - ETA: 0s - loss: 0.0979 - accuracy: 0.96 - ETA: 0s - loss: 0.0990 - accuracy: 0.96 - ETA: 0s - loss: 0.0979 - accuracy: 0.96 - ETA: 0s - loss: 0.0969 - accuracy: 0.96 - ETA: 0s - loss: 0.1024 - accuracy: 0.96 - 1s 4ms/step - loss: 0.1026 - accuracy: 0.9611 - val_loss: 0.0948 - val_accuracy: 0.9588\n",
      "Epoch 84/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0759 - accuracy: 0.96 - ETA: 0s - loss: 0.0962 - accuracy: 0.96 - ETA: 0s - loss: 0.0975 - accuracy: 0.96 - ETA: 0s - loss: 0.0985 - accuracy: 0.96 - ETA: 0s - loss: 0.0968 - accuracy: 0.96 - ETA: 0s - loss: 0.1009 - accuracy: 0.96 - ETA: 0s - loss: 0.1022 - accuracy: 0.96 - ETA: 0s - loss: 0.1026 - accuracy: 0.96 - ETA: 0s - loss: 0.1025 - accuracy: 0.96 - ETA: 0s - loss: 0.1062 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1075 - accuracy: 0.9586 - val_loss: 0.0907 - val_accuracy: 0.9691\n",
      "Epoch 85/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 6s - loss: 0.0500 - accuracy: 1.00 - ETA: 0s - loss: 0.0873 - accuracy: 0.97 - ETA: 0s - loss: 0.0904 - accuracy: 0.97 - ETA: 0s - loss: 0.0925 - accuracy: 0.97 - ETA: 0s - loss: 0.0904 - accuracy: 0.96 - ETA: 0s - loss: 0.0930 - accuracy: 0.96 - ETA: 0s - loss: 0.0932 - accuracy: 0.96 - ETA: 0s - loss: 0.0910 - accuracy: 0.96 - ETA: 0s - loss: 0.0901 - accuracy: 0.96 - ETA: 0s - loss: 0.0957 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0963 - accuracy: 0.9650 - val_loss: 0.0989 - val_accuracy: 0.9600\n",
      "Epoch 86/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0368 - accuracy: 1.00 - ETA: 0s - loss: 0.1026 - accuracy: 0.96 - ETA: 0s - loss: 0.1011 - accuracy: 0.96 - ETA: 0s - loss: 0.0971 - accuracy: 0.96 - ETA: 0s - loss: 0.1021 - accuracy: 0.96 - ETA: 0s - loss: 0.1068 - accuracy: 0.96 - ETA: 0s - loss: 0.1090 - accuracy: 0.96 - ETA: 0s - loss: 0.1062 - accuracy: 0.96 - ETA: 0s - loss: 0.1078 - accuracy: 0.96 - ETA: 0s - loss: 0.1117 - accuracy: 0.95 - 1s 4ms/step - loss: 0.1114 - accuracy: 0.9591 - val_loss: 0.0931 - val_accuracy: 0.9622\n",
      "Epoch 87/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0404 - accuracy: 1.00 - ETA: 0s - loss: 0.0938 - accuracy: 0.96 - ETA: 0s - loss: 0.0911 - accuracy: 0.96 - ETA: 0s - loss: 0.1021 - accuracy: 0.96 - ETA: 0s - loss: 0.0967 - accuracy: 0.96 - ETA: 0s - loss: 0.0972 - accuracy: 0.96 - ETA: 0s - loss: 0.0938 - accuracy: 0.96 - ETA: 0s - loss: 0.0933 - accuracy: 0.96 - ETA: 0s - loss: 0.0938 - accuracy: 0.96 - ETA: 0s - loss: 0.1017 - accuracy: 0.96 - 1s 4ms/step - loss: 0.1023 - accuracy: 0.9616 - val_loss: 0.0879 - val_accuracy: 0.9680\n",
      "Epoch 88/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0595 - accuracy: 1.00 - ETA: 0s - loss: 0.0972 - accuracy: 0.96 - ETA: 0s - loss: 0.0932 - accuracy: 0.96 - ETA: 0s - loss: 0.0914 - accuracy: 0.96 - ETA: 0s - loss: 0.0894 - accuracy: 0.96 - ETA: 0s - loss: 0.0913 - accuracy: 0.96 - ETA: 0s - loss: 0.0924 - accuracy: 0.96 - ETA: 0s - loss: 0.0931 - accuracy: 0.96 - ETA: 0s - loss: 0.0929 - accuracy: 0.96 - ETA: 0s - loss: 0.0986 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0995 - accuracy: 0.9639 - val_loss: 0.0873 - val_accuracy: 0.9622\n",
      "Epoch 89/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0904 - accuracy: 0.93 - ETA: 0s - loss: 0.0952 - accuracy: 0.96 - ETA: 0s - loss: 0.0882 - accuracy: 0.96 - ETA: 0s - loss: 0.0888 - accuracy: 0.96 - ETA: 0s - loss: 0.0899 - accuracy: 0.96 - ETA: 0s - loss: 0.0928 - accuracy: 0.96 - ETA: 0s - loss: 0.0947 - accuracy: 0.96 - ETA: 0s - loss: 0.0946 - accuracy: 0.96 - ETA: 0s - loss: 0.0954 - accuracy: 0.96 - ETA: 0s - loss: 0.1016 - accuracy: 0.96 - 1s 4ms/step - loss: 0.1016 - accuracy: 0.9616 - val_loss: 0.0807 - val_accuracy: 0.9691\n",
      "Epoch 90/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0491 - accuracy: 1.00 - ETA: 0s - loss: 0.0791 - accuracy: 0.97 - ETA: 0s - loss: 0.0822 - accuracy: 0.97 - ETA: 0s - loss: 0.0858 - accuracy: 0.96 - ETA: 0s - loss: 0.0840 - accuracy: 0.97 - ETA: 0s - loss: 0.0863 - accuracy: 0.96 - ETA: 0s - loss: 0.0846 - accuracy: 0.97 - ETA: 0s - loss: 0.0849 - accuracy: 0.97 - ETA: 0s - loss: 0.0865 - accuracy: 0.96 - ETA: 0s - loss: 0.0921 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0931 - accuracy: 0.9661 - val_loss: 0.0988 - val_accuracy: 0.9565\n",
      "Epoch 91/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0628 - accuracy: 0.96 - ETA: 0s - loss: 0.1019 - accuracy: 0.96 - ETA: 0s - loss: 0.0977 - accuracy: 0.96 - ETA: 0s - loss: 0.0918 - accuracy: 0.96 - ETA: 0s - loss: 0.0947 - accuracy: 0.96 - ETA: 0s - loss: 0.0950 - accuracy: 0.96 - ETA: 0s - loss: 0.0954 - accuracy: 0.96 - ETA: 0s - loss: 0.0938 - accuracy: 0.96 - ETA: 0s - loss: 0.0940 - accuracy: 0.96 - ETA: 0s - loss: 0.0991 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0997 - accuracy: 0.9611 - val_loss: 0.0806 - val_accuracy: 0.9680\n",
      "Epoch 92/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0975 - accuracy: 0.96 - ETA: 0s - loss: 0.1010 - accuracy: 0.96 - ETA: 0s - loss: 0.0865 - accuracy: 0.97 - ETA: 0s - loss: 0.0841 - accuracy: 0.97 - ETA: 0s - loss: 0.0863 - accuracy: 0.96 - ETA: 0s - loss: 0.0870 - accuracy: 0.96 - ETA: 0s - loss: 0.0859 - accuracy: 0.97 - ETA: 0s - loss: 0.0856 - accuracy: 0.97 - ETA: 0s - loss: 0.0903 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0940 - accuracy: 0.9671 - val_loss: 0.0888 - val_accuracy: 0.9645\n",
      "Epoch 93/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0369 - accuracy: 0.96 - ETA: 0s - loss: 0.1078 - accuracy: 0.95 - ETA: 0s - loss: 0.0920 - accuracy: 0.96 - ETA: 0s - loss: 0.0881 - accuracy: 0.96 - ETA: 0s - loss: 0.0829 - accuracy: 0.97 - ETA: 0s - loss: 0.0798 - accuracy: 0.97 - ETA: 0s - loss: 0.0780 - accuracy: 0.97 - ETA: 0s - loss: 0.0802 - accuracy: 0.97 - ETA: 0s - loss: 0.0844 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0885 - accuracy: 0.9691 - val_loss: 0.0961 - val_accuracy: 0.9588\n",
      "Epoch 94/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0367 - accuracy: 1.00 - ETA: 0s - loss: 0.1108 - accuracy: 0.97 - ETA: 0s - loss: 0.1122 - accuracy: 0.97 - ETA: 0s - loss: 0.1059 - accuracy: 0.96 - ETA: 0s - loss: 0.1007 - accuracy: 0.97 - ETA: 0s - loss: 0.1037 - accuracy: 0.96 - ETA: 0s - loss: 0.1022 - accuracy: 0.96 - ETA: 0s - loss: 0.0997 - accuracy: 0.96 - ETA: 0s - loss: 0.1005 - accuracy: 0.96 - ETA: 0s - loss: 0.1041 - accuracy: 0.96 - 1s 5ms/step - loss: 0.1045 - accuracy: 0.9634 - val_loss: 0.0805 - val_accuracy: 0.9634\n",
      "Epoch 95/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0332 - accuracy: 1.00 - ETA: 0s - loss: 0.0958 - accuracy: 0.96 - ETA: 0s - loss: 0.0776 - accuracy: 0.97 - ETA: 0s - loss: 0.0816 - accuracy: 0.96 - ETA: 0s - loss: 0.0805 - accuracy: 0.96 - ETA: 0s - loss: 0.0826 - accuracy: 0.96 - ETA: 0s - loss: 0.0832 - accuracy: 0.96 - ETA: 0s - loss: 0.0832 - accuracy: 0.96 - ETA: 0s - loss: 0.0870 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0898 - accuracy: 0.9643 - val_loss: 0.0782 - val_accuracy: 0.9725\n",
      "Epoch 96/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0371 - accuracy: 1.00 - ETA: 0s - loss: 0.0750 - accuracy: 0.96 - ETA: 0s - loss: 0.0794 - accuracy: 0.96 - ETA: 0s - loss: 0.0861 - accuracy: 0.96 - ETA: 0s - loss: 0.0809 - accuracy: 0.96 - ETA: 0s - loss: 0.0813 - accuracy: 0.97 - ETA: 0s - loss: 0.0813 - accuracy: 0.97 - ETA: 0s - loss: 0.0813 - accuracy: 0.97 - ETA: 0s - loss: 0.0840 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0849 - accuracy: 0.9700 - val_loss: 0.0735 - val_accuracy: 0.9714\n",
      "Epoch 97/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0426 - accuracy: 1.00 - ETA: 0s - loss: 0.1032 - accuracy: 0.96 - ETA: 0s - loss: 0.0915 - accuracy: 0.97 - ETA: 0s - loss: 0.0925 - accuracy: 0.97 - ETA: 0s - loss: 0.0874 - accuracy: 0.97 - ETA: 0s - loss: 0.0855 - accuracy: 0.97 - ETA: 0s - loss: 0.0861 - accuracy: 0.97 - ETA: 0s - loss: 0.0840 - accuracy: 0.97 - ETA: 0s - loss: 0.0841 - accuracy: 0.96 - ETA: 0s - loss: 0.0923 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0934 - accuracy: 0.9668 - val_loss: 0.0866 - val_accuracy: 0.9668\n",
      "Epoch 98/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0691 - accuracy: 0.96 - ETA: 0s - loss: 0.1103 - accuracy: 0.95 - ETA: 0s - loss: 0.1021 - accuracy: 0.96 - ETA: 0s - loss: 0.0977 - accuracy: 0.96 - ETA: 0s - loss: 0.0947 - accuracy: 0.96 - ETA: 0s - loss: 0.0940 - accuracy: 0.96 - ETA: 0s - loss: 0.0914 - accuracy: 0.96 - ETA: 0s - loss: 0.0903 - accuracy: 0.96 - ETA: 0s - loss: 0.0898 - accuracy: 0.96 - ETA: 0s - loss: 0.0966 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0964 - accuracy: 0.9639 - val_loss: 0.0897 - val_accuracy: 0.9611\n",
      "Epoch 99/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 5s - loss: 0.0323 - accuracy: 1.00 - ETA: 0s - loss: 0.0949 - accuracy: 0.96 - ETA: 0s - loss: 0.0846 - accuracy: 0.97 - ETA: 0s - loss: 0.0784 - accuracy: 0.97 - ETA: 0s - loss: 0.0834 - accuracy: 0.97 - ETA: 0s - loss: 0.0821 - accuracy: 0.97 - ETA: 0s - loss: 0.0853 - accuracy: 0.97 - ETA: 0s - loss: 0.0819 - accuracy: 0.97 - ETA: 0s - loss: 0.0824 - accuracy: 0.97 - ETA: 0s - loss: 0.0859 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0862 - accuracy: 0.9680 - val_loss: 0.0728 - val_accuracy: 0.9714\n",
      "Epoch 100/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0312 - accuracy: 1.00 - ETA: 0s - loss: 0.0722 - accuracy: 0.97 - ETA: 0s - loss: 0.0791 - accuracy: 0.97 - ETA: 0s - loss: 0.0847 - accuracy: 0.96 - ETA: 0s - loss: 0.0792 - accuracy: 0.97 - ETA: 0s - loss: 0.0799 - accuracy: 0.97 - ETA: 0s - loss: 0.0807 - accuracy: 0.97 - ETA: 0s - loss: 0.0820 - accuracy: 0.97 - ETA: 0s - loss: 0.0837 - accuracy: 0.96 - ETA: 0s - loss: 0.0878 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0874 - accuracy: 0.9675 - val_loss: 0.0739 - val_accuracy: 0.9703\n",
      "Epoch 101/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0274 - accuracy: 1.00 - ETA: 0s - loss: 0.1055 - accuracy: 0.95 - ETA: 0s - loss: 0.0929 - accuracy: 0.96 - ETA: 0s - loss: 0.0810 - accuracy: 0.97 - ETA: 0s - loss: 0.0796 - accuracy: 0.97 - ETA: 0s - loss: 0.0858 - accuracy: 0.97 - ETA: 0s - loss: 0.0841 - accuracy: 0.97 - ETA: 0s - loss: 0.0864 - accuracy: 0.97 - ETA: 0s - loss: 0.0904 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0916 - accuracy: 0.9691 - val_loss: 0.0683 - val_accuracy: 0.9783\n",
      "Epoch 102/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0308 - accuracy: 1.00 - ETA: 0s - loss: 0.0821 - accuracy: 0.97 - ETA: 0s - loss: 0.0775 - accuracy: 0.97 - ETA: 0s - loss: 0.0792 - accuracy: 0.97 - ETA: 0s - loss: 0.0776 - accuracy: 0.97 - ETA: 0s - loss: 0.0823 - accuracy: 0.97 - ETA: 0s - loss: 0.0795 - accuracy: 0.97 - ETA: 0s - loss: 0.0785 - accuracy: 0.97 - ETA: 0s - loss: 0.0810 - accuracy: 0.97 - ETA: 0s - loss: 0.0851 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0867 - accuracy: 0.9680 - val_loss: 0.0716 - val_accuracy: 0.9725\n",
      "Epoch 103/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0308 - accuracy: 1.00 - ETA: 0s - loss: 0.0873 - accuracy: 0.96 - ETA: 0s - loss: 0.0760 - accuracy: 0.97 - ETA: 0s - loss: 0.0808 - accuracy: 0.96 - ETA: 0s - loss: 0.0780 - accuracy: 0.97 - ETA: 0s - loss: 0.0755 - accuracy: 0.97 - ETA: 0s - loss: 0.0773 - accuracy: 0.97 - ETA: 0s - loss: 0.0785 - accuracy: 0.97 - ETA: 0s - loss: 0.0814 - accuracy: 0.97 - ETA: 0s - loss: 0.0858 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0859 - accuracy: 0.9677 - val_loss: 0.0822 - val_accuracy: 0.9657\n",
      "Epoch 104/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0450 - accuracy: 0.96 - ETA: 0s - loss: 0.0816 - accuracy: 0.97 - ETA: 0s - loss: 0.0738 - accuracy: 0.97 - ETA: 0s - loss: 0.0725 - accuracy: 0.97 - ETA: 0s - loss: 0.0775 - accuracy: 0.97 - ETA: 0s - loss: 0.0779 - accuracy: 0.97 - ETA: 0s - loss: 0.0796 - accuracy: 0.97 - ETA: 0s - loss: 0.0857 - accuracy: 0.97 - ETA: 0s - loss: 0.0874 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0933 - accuracy: 0.9691 - val_loss: 0.0786 - val_accuracy: 0.9748\n",
      "Epoch 105/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0531 - accuracy: 0.96 - ETA: 0s - loss: 0.0876 - accuracy: 0.96 - ETA: 0s - loss: 0.0839 - accuracy: 0.96 - ETA: 0s - loss: 0.0873 - accuracy: 0.96 - ETA: 0s - loss: 0.0872 - accuracy: 0.96 - ETA: 0s - loss: 0.0869 - accuracy: 0.96 - ETA: 0s - loss: 0.0861 - accuracy: 0.96 - ETA: 0s - loss: 0.0822 - accuracy: 0.96 - ETA: 0s - loss: 0.0817 - accuracy: 0.96 - ETA: 0s - loss: 0.0871 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0886 - accuracy: 0.9673 - val_loss: 0.0748 - val_accuracy: 0.9748\n",
      "Epoch 106/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0659 - accuracy: 0.96 - ETA: 0s - loss: 0.0860 - accuracy: 0.97 - ETA: 0s - loss: 0.1082 - accuracy: 0.96 - ETA: 0s - loss: 0.0983 - accuracy: 0.96 - ETA: 0s - loss: 0.0920 - accuracy: 0.97 - ETA: 0s - loss: 0.0895 - accuracy: 0.97 - ETA: 0s - loss: 0.0913 - accuracy: 0.97 - ETA: 0s - loss: 0.0925 - accuracy: 0.96 - ETA: 0s - loss: 0.0908 - accuracy: 0.96 - ETA: 0s - loss: 0.0962 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0961 - accuracy: 0.9668 - val_loss: 0.0799 - val_accuracy: 0.9714\n",
      "Epoch 107/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0349 - accuracy: 1.00 - ETA: 0s - loss: 0.0933 - accuracy: 0.96 - ETA: 0s - loss: 0.0792 - accuracy: 0.96 - ETA: 0s - loss: 0.0729 - accuracy: 0.97 - ETA: 0s - loss: 0.0748 - accuracy: 0.97 - ETA: 0s - loss: 0.0776 - accuracy: 0.97 - ETA: 0s - loss: 0.0780 - accuracy: 0.97 - ETA: 0s - loss: 0.0819 - accuracy: 0.97 - ETA: 0s - loss: 0.0854 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0870 - accuracy: 0.9677 - val_loss: 0.0768 - val_accuracy: 0.9657\n",
      "Epoch 108/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0695 - accuracy: 0.96 - ETA: 0s - loss: 0.0718 - accuracy: 0.96 - ETA: 0s - loss: 0.0665 - accuracy: 0.97 - ETA: 0s - loss: 0.0639 - accuracy: 0.97 - ETA: 0s - loss: 0.0688 - accuracy: 0.97 - ETA: 0s - loss: 0.0707 - accuracy: 0.97 - ETA: 0s - loss: 0.0726 - accuracy: 0.97 - ETA: 0s - loss: 0.0711 - accuracy: 0.97 - ETA: 0s - loss: 0.0717 - accuracy: 0.97 - ETA: 0s - loss: 0.0764 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0776 - accuracy: 0.9698 - val_loss: 0.0665 - val_accuracy: 0.9737\n",
      "Epoch 109/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0369 - accuracy: 1.00 - ETA: 0s - loss: 0.0910 - accuracy: 0.97 - ETA: 0s - loss: 0.0869 - accuracy: 0.97 - ETA: 0s - loss: 0.0777 - accuracy: 0.97 - ETA: 0s - loss: 0.0735 - accuracy: 0.97 - ETA: 0s - loss: 0.0703 - accuracy: 0.97 - ETA: 0s - loss: 0.0732 - accuracy: 0.97 - ETA: 0s - loss: 0.0752 - accuracy: 0.97 - ETA: 0s - loss: 0.0739 - accuracy: 0.97 - ETA: 0s - loss: 0.0789 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0794 - accuracy: 0.9712 - val_loss: 0.0732 - val_accuracy: 0.9737\n",
      "Epoch 110/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0347 - accuracy: 1.00 - ETA: 0s - loss: 0.0756 - accuracy: 0.96 - ETA: 0s - loss: 0.0756 - accuracy: 0.97 - ETA: 0s - loss: 0.0704 - accuracy: 0.97 - ETA: 0s - loss: 0.0765 - accuracy: 0.97 - ETA: 0s - loss: 0.0737 - accuracy: 0.97 - ETA: 0s - loss: 0.0766 - accuracy: 0.97 - ETA: 0s - loss: 0.0793 - accuracy: 0.96 - ETA: 0s - loss: 0.0809 - accuracy: 0.96 - ETA: 0s - loss: 0.0855 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0852 - accuracy: 0.9664 - val_loss: 0.0724 - val_accuracy: 0.9737\n",
      "Epoch 111/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0497 - accuracy: 1.00 - ETA: 0s - loss: 0.0822 - accuracy: 0.96 - ETA: 0s - loss: 0.0674 - accuracy: 0.97 - ETA: 0s - loss: 0.0661 - accuracy: 0.97 - ETA: 0s - loss: 0.0818 - accuracy: 0.97 - ETA: 0s - loss: 0.0796 - accuracy: 0.97 - ETA: 0s - loss: 0.0786 - accuracy: 0.97 - ETA: 0s - loss: 0.0766 - accuracy: 0.97 - ETA: 0s - loss: 0.0769 - accuracy: 0.97 - ETA: 0s - loss: 0.0820 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0819 - accuracy: 0.9714 - val_loss: 0.0629 - val_accuracy: 0.9805\n",
      "Epoch 112/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0503 - accuracy: 0.96 - ETA: 0s - loss: 0.0603 - accuracy: 0.98 - ETA: 0s - loss: 0.0715 - accuracy: 0.97 - ETA: 0s - loss: 0.0737 - accuracy: 0.97 - ETA: 0s - loss: 0.0770 - accuracy: 0.97 - ETA: 0s - loss: 0.0756 - accuracy: 0.97 - ETA: 0s - loss: 0.0743 - accuracy: 0.97 - ETA: 0s - loss: 0.0733 - accuracy: 0.97 - ETA: 0s - loss: 0.0765 - accuracy: 0.97 - ETA: 0s - loss: 0.0846 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0851 - accuracy: 0.9684 - val_loss: 0.0708 - val_accuracy: 0.9703\n",
      "Epoch 113/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 5s - loss: 0.0327 - accuracy: 1.00 - ETA: 0s - loss: 0.0685 - accuracy: 0.97 - ETA: 0s - loss: 0.0680 - accuracy: 0.97 - ETA: 0s - loss: 0.0673 - accuracy: 0.97 - ETA: 0s - loss: 0.0669 - accuracy: 0.97 - ETA: 0s - loss: 0.0680 - accuracy: 0.97 - ETA: 0s - loss: 0.0666 - accuracy: 0.97 - ETA: 0s - loss: 0.0684 - accuracy: 0.97 - ETA: 0s - loss: 0.0673 - accuracy: 0.97 - ETA: 0s - loss: 0.0745 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0749 - accuracy: 0.9714 - val_loss: 0.0693 - val_accuracy: 0.9680\n",
      "Epoch 114/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0475 - accuracy: 0.96 - ETA: 0s - loss: 0.1239 - accuracy: 0.95 - ETA: 0s - loss: 0.0940 - accuracy: 0.97 - ETA: 0s - loss: 0.0843 - accuracy: 0.97 - ETA: 0s - loss: 0.0846 - accuracy: 0.97 - ETA: 0s - loss: 0.0851 - accuracy: 0.97 - ETA: 0s - loss: 0.0830 - accuracy: 0.97 - ETA: 0s - loss: 0.0823 - accuracy: 0.97 - ETA: 0s - loss: 0.0823 - accuracy: 0.97 - ETA: 0s - loss: 0.0884 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0882 - accuracy: 0.9682 - val_loss: 0.0744 - val_accuracy: 0.9691\n",
      "Epoch 115/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0250 - accuracy: 1.00 - ETA: 0s - loss: 0.0840 - accuracy: 0.97 - ETA: 0s - loss: 0.0830 - accuracy: 0.97 - ETA: 0s - loss: 0.0750 - accuracy: 0.97 - ETA: 0s - loss: 0.0769 - accuracy: 0.97 - ETA: 0s - loss: 0.0768 - accuracy: 0.97 - ETA: 0s - loss: 0.0761 - accuracy: 0.97 - ETA: 0s - loss: 0.0754 - accuracy: 0.97 - ETA: 0s - loss: 0.0737 - accuracy: 0.97 - ETA: 0s - loss: 0.0801 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0811 - accuracy: 0.9716 - val_loss: 0.0711 - val_accuracy: 0.9714\n",
      "Epoch 116/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0446 - accuracy: 1.00 - ETA: 0s - loss: 0.0749 - accuracy: 0.97 - ETA: 0s - loss: 0.0636 - accuracy: 0.97 - ETA: 0s - loss: 0.0683 - accuracy: 0.97 - ETA: 0s - loss: 0.0693 - accuracy: 0.97 - ETA: 0s - loss: 0.0742 - accuracy: 0.97 - ETA: 0s - loss: 0.0722 - accuracy: 0.97 - ETA: 0s - loss: 0.0711 - accuracy: 0.97 - ETA: 0s - loss: 0.0694 - accuracy: 0.97 - ETA: 0s - loss: 0.0733 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0732 - accuracy: 0.9723 - val_loss: 0.0703 - val_accuracy: 0.9771\n",
      "Epoch 117/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0444 - accuracy: 1.00 - ETA: 0s - loss: 0.0704 - accuracy: 0.97 - ETA: 0s - loss: 0.0623 - accuracy: 0.98 - ETA: 0s - loss: 0.0583 - accuracy: 0.98 - ETA: 0s - loss: 0.0585 - accuracy: 0.97 - ETA: 0s - loss: 0.0597 - accuracy: 0.97 - ETA: 0s - loss: 0.0594 - accuracy: 0.97 - ETA: 0s - loss: 0.0571 - accuracy: 0.97 - ETA: 0s - loss: 0.0566 - accuracy: 0.97 - ETA: 0s - loss: 0.0636 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0647 - accuracy: 0.9762 - val_loss: 0.0659 - val_accuracy: 0.9748\n",
      "Epoch 118/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0551 - accuracy: 1.00 - ETA: 0s - loss: 0.0681 - accuracy: 0.97 - ETA: 0s - loss: 0.0600 - accuracy: 0.97 - ETA: 0s - loss: 0.0624 - accuracy: 0.97 - ETA: 0s - loss: 0.0660 - accuracy: 0.97 - ETA: 0s - loss: 0.0735 - accuracy: 0.97 - ETA: 0s - loss: 0.0721 - accuracy: 0.97 - ETA: 0s - loss: 0.0706 - accuracy: 0.97 - ETA: 0s - loss: 0.0723 - accuracy: 0.97 - ETA: 0s - loss: 0.0766 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0777 - accuracy: 0.9714 - val_loss: 0.0676 - val_accuracy: 0.9771\n",
      "Epoch 119/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0440 - accuracy: 1.00 - ETA: 0s - loss: 0.0822 - accuracy: 0.96 - ETA: 0s - loss: 0.0798 - accuracy: 0.96 - ETA: 0s - loss: 0.0877 - accuracy: 0.96 - ETA: 0s - loss: 0.0793 - accuracy: 0.96 - ETA: 0s - loss: 0.0779 - accuracy: 0.96 - ETA: 0s - loss: 0.0824 - accuracy: 0.96 - ETA: 0s - loss: 0.0787 - accuracy: 0.96 - ETA: 0s - loss: 0.0756 - accuracy: 0.96 - ETA: 0s - loss: 0.0785 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0791 - accuracy: 0.9682 - val_loss: 0.0817 - val_accuracy: 0.9588\n",
      "Epoch 120/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0778 - accuracy: 0.93 - ETA: 0s - loss: 0.0787 - accuracy: 0.96 - ETA: 0s - loss: 0.0742 - accuracy: 0.97 - ETA: 0s - loss: 0.0731 - accuracy: 0.97 - ETA: 0s - loss: 0.0667 - accuracy: 0.97 - ETA: 0s - loss: 0.0633 - accuracy: 0.97 - ETA: 0s - loss: 0.0644 - accuracy: 0.97 - ETA: 0s - loss: 0.0629 - accuracy: 0.97 - ETA: 0s - loss: 0.0650 - accuracy: 0.97 - ETA: 0s - loss: 0.0712 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0686 - val_accuracy: 0.9748\n",
      "Epoch 121/1000\n",
      "138/138 [==============================] - ETA: 5s - loss: 0.0332 - accuracy: 1.00 - ETA: 0s - loss: 0.0704 - accuracy: 0.97 - ETA: 0s - loss: 0.0594 - accuracy: 0.98 - ETA: 0s - loss: 0.0610 - accuracy: 0.98 - ETA: 0s - loss: 0.0590 - accuracy: 0.98 - ETA: 0s - loss: 0.0648 - accuracy: 0.97 - ETA: 0s - loss: 0.0664 - accuracy: 0.97 - ETA: 0s - loss: 0.0668 - accuracy: 0.97 - ETA: 0s - loss: 0.0695 - accuracy: 0.97 - ETA: 0s - loss: 0.0746 - accuracy: 0.97 - 1s 4ms/step - loss: 0.0743 - accuracy: 0.9739 - val_loss: 0.0660 - val_accuracy: 0.9760\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "59/59 [==============================] - ETA: 4s - loss: 0.3549 - accuracy: 0.84 - ETA: 0s - loss: 0.3200 - accuracy: 0.92 - 0s 3ms/step - loss: 0.2804 - accuracy: 0.9242\n",
      "[0.280415896263163, 0.9242262]\n"
     ]
    }
   ],
   "source": [
    "import autokeras as ak\n",
    "\n",
    "# Initialize the image classifier.\n",
    "clf = ak.ImageClassifier()\n",
    "# Feed the image classifier with training data.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict with the best model.\n",
    "predicted_y = clf.predict(X_test)\n",
    "print(predicted_y)\n",
    "\n",
    "# Evaluate the best model with testing data.\n",
    "print(clf.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "model = clf.export_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17523671351803084613\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6574135706\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13676257606802177618\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.config' has no attribute 'experimental_list_devices'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-3d00d838479b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_available_gpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_available_gpus\u001b[1;34m()\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdevices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m             \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_list_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'device:gpu'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.config' has no attribute 'experimental_list_devices'"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
